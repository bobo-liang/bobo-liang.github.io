---
layout: post
title: 'Swin Transformer: Hierarchical Vision Transformer using ShiftedWindows'
date: 2021-08-30
author: Poley
cover: '/assets/img/20210830/Swin.png'
tags: 论文阅读
---

本文提出一种层次化的Transformer，通过滑动窗口(shifted windows)。此模型可以灵活的建模多重尺度，并且相对于图像尺寸具有线性复杂度。

![](/assets/img/20210830/SwinF1.png)

Swin和ViT的区别如上图所示，Swin使用层次化的feature map，通过合并image patches的方式。并且由于只计算每个Local window的self-attention，因此复杂度和图像输入尺寸是线性的。而ViT整个网络都被分为16个patch，一直处于较低的分辨率下；并且由于需要计算全局的self-attention，引起复杂度是$O(n^2)$。

Transformer在NLP领域已经基本成为主流，但是在CV领域还远远没有。作者认为这主要是由于两个模态之间的巨大差距导致的：

+ **Scale**:不像NLP中语言Transformer的基本处理单元word token，visual element是可以在scale上剧烈变化的。
+ **Resolution**:图像的分辨率远高于文本，尤其是语义分割任务。大分辨率对于Transformer的$O(n^2)$复杂度很不友好。

Swin也使用了**shifted window**代替了传统的**slide window**，提高了算法的效率，降低了延迟，同时也可以有效的捕捉不同windows之间的关系。示意图如下所示

![](/assets/img/20210830/SwinF2.png)

Swin-T的结构如下(tiny版本)
![](/assets/img/20210830/SwinF3.png)

这个结构类似于VGG和resnet，通过层次化的处理逐渐减小分辨率，增大特征维度。这里的Transformer使用一个基于**shifted windows**的模块代替了标准的multi-head self attention，其他的并无区别。

标准的Transformer以及其在图像分类上的衍生网络都使用了Global self-attention，计算一个token和其他所有token之间的关系。这并不适合一些需要高分辨率或者密集预测的视觉任务，因为复杂度太高。

这里作者提出使用**Self-attention in non-overlapped windows**，将图像均匀分为若干个不重叠的windows，并在**每个windows中划分出$M /times M$个patch**，在每个windows内做self-attention。对于一个具有$H \times W$个patch的image来说，这样和传统方法的复杂度区别如下

$$ 
\begin{equation}
\begin{aligned}
&\Omega(\mathrm{MSA})=4 h w C^{2}+2(h w)^{2} C \\
&\Omega(\mathrm{W}-\mathrm{MSA})=4 h w C^{2}+2 M^{2} h w C,
\end{aligned}
\end{equation}
$$

可以看到，这样导致了线性复杂度，使得网络可以承受更大的$hw$。

但是单纯的上述的局部self-attention会没有办法引入**cross-window connections**，因此这里使用了shifted windows，通过更改window的划分方式来引入不同windows之间的联系。具体方法是在两个连续的Transformer中使用不同的windows划分。这样一整个模块的数学表达可以如下

$$
\begin{equation}
\begin{aligned}
&\hat{\mathbf{z}}^{l}=\mathrm{W}-\mathrm{MSA}\left(\mathrm{LN}\left(\mathbf{z}^{l-1}\right)\right)+\mathbf{z}^{l-1} \\
&\mathbf{z}^{l}=\mathrm{MLP}\left(\mathrm{LN}\left(\hat{\mathbf{z}}^{l}\right)\right)+\hat{\mathbf{z}}^{l} \\
&\hat{\mathbf{z}}^{l+1}=\mathrm{SW} \text {-MSA }\left(\mathrm{LN}\left(\mathbf{z}^{l}\right)\right)+\mathbf{z}^{l} \\
&\mathbf{z}^{l+1}=\mathrm{MLP}\left(\mathrm{LN}\left(\hat{\mathbf{z}}^{l+1}\right)\right)+\hat{\mathbf{z}}^{l+1}
\end{aligned}
\end{equation}
$$

具体的划分方式如图2所示，对于window size $M$,在相邻的第二层对windows进行一个$\left(\left\lfloor\frac{M}{2}\right\rfloor,\left\lfloor\frac{M}{2}\right\rfloor\right)$的偏移，得到图二右边的划分

其一种高效的计算方式如下所示
![](/assets/img/20210830/SwinF4.png)

和其他视觉Transformer一样，本文也加入了位置编码，如下
$$
\begin{equation}
\text { Attention }(Q, K, V)=\operatorname{SoftMax}\left(Q K^{T} / \sqrt{d}+B\right) V
\end{equation}
$$

但是实际效果并不好，因此最终在实现中没有使用位置编码（相对或者绝对）。

和RESNET一样，也使用了多重结构变体，具有不同的模型大小和性能，如下

![](/assets/img/20210830/SwinV1.png)

从实验结果来说，在检测上确实优于传统的CNN不少

![](/assets/img/20210830/SwinT2.png)