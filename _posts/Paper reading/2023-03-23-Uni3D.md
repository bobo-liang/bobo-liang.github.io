---
layout: post
title: 'Uni3D: A Unified Baseline for Multi-dataset 3D Object Detection'
date: 2023-3-14
author: Poley
cover: '/assets/img/20230311/Uni3D.jpg'
tags: 论文阅读  
---


现有UDA方法本质上还是一个source-to-target的过程，而不是一个多数据集的双向泛化过程。即只能通过UDA方法完成对target性能的提升，而不能同时保证两个域的性能。为了获得泛化的3D检测性能，自然的想法是混合多数据集一起训练。但是结果不令人满意。这里作者首先分析了多数据集之前的差异，提出Uni3D方法，通过引入Statistic
level Alignment 和semantic-level Coupling-and-Recoupling module两个方法，实现了模型在多个数据集上的泛化能力。


简单的混合多个数据集的数据一起训练，相比单独在每个数据集上训练，出现了显著的性能损失。因为不同数据集间巨大的分布差异会影响模型对于特征的学习。各数据集的差异可以通过统计特性以及设备参数看出，如下所示

![](/assets/img/20230311/Uni3DF2.jpg)

![](/assets/img/20230311/Uni3DT1.jpg)

作者发现，点密度的差异是数据间的主要差异，其导致了相同物体在不同数据中具有不同的感受野大小。因此，首先需要进行Pointcloud range的对齐。其效果如下所示

![](/assets/img/20230311/Uni3DT2.jpg)

其次，是类别的差异。不同数据集对类别的划分并不相同。这里按泛用类别做了简单的映射，将其都映射到waymo的三个类别当中。

上述问题是做cross domian的基础工作。在模型的泛化上，作者提出了两个方法

## Data-level Correction Operation

首先进行统计level的对齐。一般网络使用BN等统计特性来归一化网络，可能会影响模型的性能在MDF中。（不同数据集统计特性差距大）。解决方案。每个domain维护一个自己的BN参数。然后再用一个dataset-shared的归一化参数来调整最终的模型输出。

即，来自不同的特征使用自己数据集的BN参数进行归一化如下

$$
\begin{equation}
\hat{x}_t^j=\frac{x_t^j-\mu_t^j}{\sqrt{\sigma_t^j+\xi}}
\end{equation}
$$

之后，使用可学习的参数，对上述分别归一化的特征进行统一调整如下。这个方法在streamPERT和ConvNextV2中都被使用
$$
\begin{equation}
\hat{y}_t^j=\gamma^j \hat{x}_t^j+\beta^j
\end{equation}
$$

## Semantic-level Feature Coupling-and-Recoupling Module
整体结构如下
特征层面，使用一个dataset-level的attention实现对不同数据集bev的重组，从而使得重组后的特征是dataset-agnostic的。具体如下,成为coupling
$$
\begin{equation}
\begin{aligned}
& f_{\text {cat }}^{b e v}=\left[f_i^{b e v}, \ldots, f_j^{b e v}\right] \\
& \hat{f}_{\text {shared }}^{\text {bev }}=\left[M_{\text {shared }} \odot \phi_d\left(\operatorname{Conv}\left(f_{\text {cat }}^{b e v}\right)\right)\right] f_{\text {cat }}^{b e v}
\end{aligned}
\end{equation}
$$
将重组的data-agnostic特征再加入到原有的dataset-specific特征中，如下，成为recoupling。个人认为这里的主要目的是让head在学习的时候关注到多个数据集的特征

$$
\begin{equation}
\begin{aligned}
\hat{f}_i^{b e v} & =S E_i\left(\hat{f}_{\text {shared }}^{b e v}\right)+f_i^{b e v} \\
\hat{f}_j^{b e v} & =S E_j\left(\hat{f}_{\text {shared }}^{b e v}\right)+f_j^{b e v}
\end{aligned}
\end{equation}
$$

在测试时，由于只有单数据集的特征，因此将BEV特征复制，一保持原有的网络结构。

![](/assets/img/20230311/Uni3DF4.jpg)

## Dataset-specific Detection Heads
和上述用不同BN保持各数据集统计特性类似，这里也为每个数据集保留了一个单独的HEAD，每个HEAD分别计算Loss，这类似于检测任务中常用的Multi-task方法，实现起来非常简单。可以表示如下

$$
\begin{equation}
L_{\text {det }}^{\text {overall }}=\sum_{\boldsymbol{k}} L_{\text {det }}^k\left(H_k\left(\hat{f}_k^{b e v}\right)\right)
\end{equation}
$$

最后，从作者给出的实验结果来看，看起来似乎还是特征自身的统计特性影响更大，但是对BEV的CR也可以显著的提升性能。

![](/assets/img/20230311/Uni3DT3.jpg)