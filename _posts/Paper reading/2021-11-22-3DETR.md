---
layout: post
title: 'An End-to-End Transformer Model for 3D Object Detection'
date: 2021-11-22
author: Poley
cover: '/assets/img/20211122/3DETR.png'
tags: 论文阅读
---

提出使用端到端的Transformers实现3D点云检测，3DETR只需要在vanilla Transformer block做出很小的修改即可实现，并且在室内场景中体现出了不错的性能。

本文可以算作在VoteNet上的改进，总体结构与VoteNet一致，将点云检测问题视为一个set-to-set问题，即输入一个无序集合（点云），输出一个无序集合（Bounding boxes）。主要结构包括一个编码器和一个解码器，Votenet结构如下所示。
![](/assets/img/20211122/VoteNet.png)

本文做出的主要改变
+ 编码器上，使用Transformer代替了PointNet++
+ 解码器上，参考了DETR，做出了适应3D检测的两个重要改变，non-parametric query embeddings and Fourier positional embeddings。

VoteNet虽然是一种非常经典的3D目标检测pipeline。但是为了达到最好的效果，其超参数需要人为的精调。据原文作者所述，VoteNet中的multiple hand-encoded radii used in encoder, decoder, and the loss function都对于检测器的性能有非常重要的影响，并且都被carefully tuned。本文提出的3DTR也立志于解决这个问题，即省去复杂的超参数调整。
3DETR的结构如下所示
![](/assets/img/20211122/3DETRF2.png)

## 3DETR

首先，使用PointNet++来对点云进行下采样，得到$N'$个特征。PointNet++其中的MLP包含两个隐藏层，64128个节点，最终输出$d=256$的特征。之后送入Encoder中，这里的Encoder使用的就是标准的自注意力机制，同时忽略了位置编码，因为点云中本身是无序的并且已经包含了位置信息。

解码器上，和DETR一样，将目标检测视为一个集合预测问题。因此，网络同时预测一组Box而不区分特定的顺序。这可以通过一个由Transformer blocks组成的parallel decoder实现。和DETR一样，解码器使用点特征一组$B$个query embeddings来作为输入，产生一组$B$个特征用于产生3D-bounding box。这里的query embedding主要表示3D空间中的位置。对于解码器的两个输入，都使用了positional embeddings，因为这些输入没有与坐标的直接关联。

### Non-parametric query embeddings
受到 VoteNet和BoxNet的启发，这里使用seed点来实现无参数的query embedding。通过随机采样$B$个 query points来作为种子。这里通过最远点采样FPS实现。将其和query embedding串联起来一起作为 query的输入。

### 3DETR-m: Inductive biases into 3DETR
这部分基于上述的3D3TR做了一下小改动，引入了归纳偏置。一个简单的归纳偏置就是PointNet++中提出的，局部特征aggregation的效果好于global aggregation。这里的额改动主要针对encoder，即特征提取部分。给每一层self-attention添加了mask，用于筛选每个点的邻域点特征。同时将encoder拓展为3层，逐级拓展邻域半径，减少采样点数量，来融合到更加丰富的特征。

### Bounding box parameterization and prediction
需要回归的量和对应的编码方式如下：
+ 位置：回归实际位置和query 坐标的残差
+ 尺寸：直接回归
+ 方向：圆周分为12Bin分类+残差
+ 类别：one-hot，并添加背景类。

## Set Matching and Loss function
为了找到$B$个预测框和真值框直接的对印关系，需要进行matchting。VoteNet使用手动定义的半径来进行匹配，这里直接和DETR中一样，使用二分图的匈牙利算法来进行更简单的搜索。即通过设定一个loss，然后寻找$B$个预测框和$B$个真值框（包括空集填充）的一个最小loss匹配（通过改变一方的排序即可实现）。这里使用的Loss如下
$$
\begin{equation}
\begin{aligned}
C_{\text {match }}(\hat{\mathbf{b}}, \mathbf{b}) &=\underbrace{-\lambda_{1} \mathrm{GIoU}(\hat{\mathbf{b}}, \mathbf{b})+\lambda_{2}\|\hat{\mathbf{c}}-\mathbf{c}\|_{1}}_{\text {geometric }} \\
&-\underbrace{\lambda_{3} \hat{\mathbf{S}}\left[s_{\mathrm{gt}}\right]+\lambda_{4}\left(1-\hat{\mathbf{s}}\left[s_{\mathrm{bg}}\right]\right)}_{\text {semantic }}
\end{aligned}
\end{equation}
$$

总体loss如下
$$
\begin{equation}
\begin{array}{r}
\mathcal{L}_{3 \mathrm{DETR}}=\lambda_{c}\|\hat{\mathbf{c}}-\mathbf{c}\|_{1}+\lambda_{d}\|\hat{\mathbf{d}}-\mathbf{d}\|_{1}+\lambda_{a r}\left\|\hat{\mathbf{a}}_{r}-\mathbf{a}_{r}\right\|_{\text {huber }} \\
-\lambda_{a c} \mathbf{a}_{c}^{\top} \log \hat{\mathbf{a}}_{c}-\lambda_{s} \mathbf{s}_{c}^{\top} \log \hat{\mathbf{s}}_{c}
\end{array}
\end{equation}
$$
## Experiments
![](/assets/img/20211122/3DETRT1.png)

![](/assets/img/20211122/3DETRF3.png)

![](/assets/img/20211122/3DETRT4.png)

