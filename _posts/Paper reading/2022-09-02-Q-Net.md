---
layout: post
title: 'A Unified Query-based Paradigm for Point Cloud Understanding'
date: 2022-09-03
author: Poley
cover: '/assets/img/20220903/QNet.jpg'
tags: 论文阅读
---

本文来自于CVPR2022，其一种新的点云特征学习范式，鉴于特征的embeeding(比如backbone)和task-specific的head之间。个人感觉是一个比较有意思的思路。

主流的点云表征方法分为两类，VOXEL和POINT。但是其都包含一个下采样过程，通过采样/STRIDE实现。再通过一个上采样，实现下游任务或者MASK。称为Encoder-Decoder范式（ED）。由于升降采样的设计，ED范式在降采样过程中你那个提取固定位置的特征。

这里作者提出一种全新的Embedding-Querying范式（EQ），可以在3D场景的任意部分生成特征，相当于泛化版的ED范式。包含3部分，Embedding,Querying和task-specific head。基本流程：通过任意特征提取网络，得到固定位置的特征，即支持特征。之后根据任意位置，进行query，基于支持特征得到中间表示，即Q表征。本文的主要创新在于提出一种有效提取Q表征的网络QNet。整体概念如下图所示。本方法的特点在于其灵活性可以使其轻松的部署到多种下游任务上。

![](/assets/img/20220903/QNetF1.jpg)

## EQ-Paradigm
本方法主要流程为，对输入编码得到支持特征和支持位置。对任意位置进行索引，产生特征。用索引特征完成下游任务。如下所示

$$
\begin{equation}
\begin{aligned}
F_S, S &=\operatorname{Embedding}(I), \\
F_Q &=\operatorname{Querying}\left(F_S, S, Q\right), \\
O &=\operatorname{Head}\left(F_Q, Q\right),
\end{aligned}
\end{equation}
$$

### Embedding Stage
这部分可以使用任意类型的点云特征提取主干，支持向量的选择，对于点方法，使用降采样后的点特征。对体素方法，则使用体素中心和特征。

### Querying Stage
Querying的关键在于根据不同的任务来选择合适的query position。例如
+ 对于检测任务：对SSD head来说，query位置是BEV map下目标的中心位置，对于Point-based head来说，query位置是一组降采样的点位置；
+ 对于分割任务：query位置是每一个输入点云的位置，因为要对其产生一个Point-level的分类结果；
+ 对于分类任务：query位置可以是物体中心或者均匀分布的位置。

灵活的query位置是本方法可以应用于多种下游任务的关键，如图所示。

![](/assets/img/20220903/QNetF2.jpg)

### Q-Net
这部分是本文的主要创新，即如何产生在任意位置索引的特征。本文作者提出了一个基于Transformer的结构如下

![](/assets/img/20220903/QNetF3.jpg)

其中的子模块是Q-Block，分别由Q-Encoder Layer和Q-Decoder Layer构成。这里简单的采用标准的T编码器和解码器作为需要的编码器和解码器。

对于编码器，目的在于逐级对支持特征进行学习，并引入了残差结构，可以表示为
$$
\begin{equation}
\begin{aligned}
&\hat{F}_S^l=\operatorname{Attention}\left(S, F_S^{l-1}, S, F_S^{l-1}\right)+F_S^{l-1} \\
&F_S^l=\operatorname{FFN}\left(\hat{F}_S^l\right)+\hat{F}_S^l
\end{aligned}
\end{equation}
$$

对于解码结构，和Transformer中一样，通过索引和残差链接实现
$$
\begin{equation}
\begin{aligned}
&\hat{F}_Q^l=\operatorname{Attention}\left(Q, F_Q^{l-1}, S, F_S^{l-1}\right)+F_Q^{l-1}, \\
&F_Q^l=\operatorname{FFN}\left(\hat{F}_Q^l\right)+\hat{F}_Q^l,
\end{aligned}
\end{equation}
$$

需要注意的是，不对query采用自注意力，直接使用cross-attention。目的，这里不需要保留query之间的相关性，而不像语言模型中，每个位置的词依旧具有相关性。

位置编码在这里非常关键。由于query feature的初始特征是0，因此位置编码是产生query features的唯一依据。但是，只根据相对位置编码是不合适的，因为这样会得到相同的权重，对于同样相对距离但是不同尺度和形状的点。因此，这里选用contextual relative positional encoding。relative positional encoding根据其邻域内的K和V生成位置嵌入特征。具体可以参考相关的文献，这里没有展开描述。

最后，针对计算复杂度的问题，这里作者提出了Local Attention的方法，使用KNN来避免全局的Attention权重计算（否则需要计算上万甚至上十万的权重矩阵，不可接受）。作者将这个网络进一步用于诸多下游任务重，但是，并没有给出运行时间方面的评估（比如在检测应用上的时间），以笔者经验来说，这样的Attention可能计算负担较大。当然，其在性能上确实也很高。

检测任务上的性能如下所示
![](/assets/img/20220903/QNetT3.jpg)