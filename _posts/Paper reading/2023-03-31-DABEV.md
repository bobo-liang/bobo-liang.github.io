---
layout: post
title: 'DA-BEV Depth Aware BEV Transformer for 3D Object Detection'
date: 2023-3-28
author: Poley
cover: '/assets/img/20230328/DABEV.jpg'
tags: 论文阅读  
---
传统基于深度估计产生位点云进行点云检测的方法，非常依赖深度估计，并且进而收到复合误差的影响。作者认为，目前的Camera-based DETR方法也存在深度估计的问题，因为其自然地继承自2D检测，而没有考虑到3D空间的内部特性。即用query检索2D图像特征的时候没有考虑深度信息。

作者认为，类似于bevformer这样的方法将3的ref point投影到2d来query信息，忽略了depth影响。具体表现为，在一条射线上的query都会get到图像中相同位置的信息。这可能导致bev上此射线上的特征趋同，从而导致一条射线上的重复预测。如下图所示

![](/assets/img/20230328/DABEVF1.jpg)

一个解决这个问题的直观方法就是，为图像引入depth信息，并与ref points的depth对比。从而使得同一个射线上的query获得的特征有所区别。因此，提出DA-SCA来使得query和img feature同时获得深度信息。并引入对比学习来boost对于深度的学习。本文的整体结构如下所示

![](/assets/img/20230328/DABEVF2.jpg)


BEVFormer根据3DPE来预测offset。这使得不同相机内的采样点相同，是一个不合理的假设。（但是，deformable的offset使用query or query+pe完成的啊？）这里提出分别在query和key上加入深度信息，如下所示：
query因为自带3d ref points，因此其直接具有深度信息，只需要将其投影到cam坐标系即可

$$
\begin{equation}
\begin{aligned}
u, v, d & =\operatorname{Ego} 2 \operatorname{cam}(x, y, z) \\
Q_p & =\operatorname{SinePE}(u, v, d),
\end{aligned}
\end{equation}
$$

图像则使用预测的depth信息，如下

$$
\begin{equation}
V_p=\operatorname{SinePE}\left(u_c, v_c, \mathbf{d}\left[u_c, v_c\right]\right)
\end{equation}
$$
query自带深度信息，作者这里的意思好像是是通过残差接，query自带的深度信息得以保留，同时又aggregation了图像的depth特征？如下所示
$$
\begin{equation}
\begin{aligned}
F= & \operatorname{DeformAttn}(Q, r, V) \\
= & \operatorname{DeformAttn}\left(Q_c+Q_p, r, V_c+V_p\right) \\
= & \operatorname{DeformAttn}\left(Q_c+\operatorname{SinePE}(u, v, d),(u, v),\right. \\
& \left.V_c+\operatorname{SinePE}\left(u_c, v_c, \mathbf{d}\left[u_c, v_c\right]\right)\right)
\end{aligned}
\end{equation}
$$
这部分的cross attention可以表示为下图
![](/assets/img/20230328/DABEVF3.jpg)

作者为了进一步提高网络的深度信息的判别能力，进一步提出深度对比学习。思想：在射线上取若干个特征，深度上靠近gt的为正样本，否则为负样本。并进行对比。但是这里直接用gt进行监督，而不是在特征层面上的。这样不会干扰最终的检测结果么？相当于把若干个bevfeature都往正样本做回归。不会产生冗余框么？或者因为和最终Head的assign不一致而影响性能？不过从实验中没有看到这种现象。

上述取到的正负样本分别按正常的检测计算分类和回归损失，作为网络的辅助Loss出现，如下所示
$$
\begin{equation}
\begin{aligned}
L_{D C L} & =\sum_{i \in I_{p o s}} L_{b o x}\left(H_{b o x}\left(f_i\right), b\right)+L_{c l s}\left(H_{c l s}\left(f_i\right), c\right) \\
& +\sum_{i \in I_{\text {neg }}} L_{c l s}\left(H_{c l s}\left(f_i\right), c_{n o}\right)
\end{aligned}
\end{equation}
$$

最终性能上具有一定的优势
![](/assets/img/20230328/DABEVT1.jpg)

![](/assets/img/20230328/DABEVT4.jpg)

作者也进一步吧这个方法拓展到了PETR和DETR3D上，这里原本使用BEV feature map来从object ray上得到对应的特征并用于对比。PETR方法没有bev，因此这里直接构建了ray上的ref point用于对比，同时保留了query的zeros初始化。但也同时在上面加入了深度信息，即query位置在相机坐标系中带深度的位置编码。本文提出的方法也同样在这两个模型上获得了不错的收益。
![](/assets/img/20230328/DABEVT5.jpg)