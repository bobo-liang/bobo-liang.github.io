---
layout: post
title: 'VPFNet: Improving 3D Object Detection with Virtual Point based LiDAR and Stereo Data Fusion'
date: 2022-10-09
author: Poley
cover: '/assets/img/20221008/VPFNet.jpg'
tags: 论文阅读
---

图像和点云的融合主要的问题是两者分辨率的不匹配。用图像给点云上色会极大浪费点云的信息。如何高效的align两者是融合方法的关键，包括SFD也是解决的这个问题。另一个关键问题是使用数据增广。数据增广对模型训练具有显著的影响，但是单模态的增广往往不能用于直接应用于多模态。

常见的点云图像融合方法有两种：
object level fusion 一般直接融合RoI特征。其属于融合高等级、粗粒度的特征。点/体素level的融合相当于细粒度，低等级的融合。但是存在分辨率不匹配的问题。



本文通过建立virtual point的方式来作为点云和图像之间的桥梁。原因如下图所示

![](/assets/img/20221008/VPFNetF1.jpg)

对于图中黑车，其颜色使得激光反射率较低，呈现出较低的点数。如果直接通过点云反投影来获取对应位置的图像特征的话，相当于极低的采样率。图中黑车点云可以获得的图像信息非常有限。从全局上来说，以KITTI数据集为例，其一般具有20000个点云，而像素约为300000，对应的采样率为6.7%。相当于浪费了海量的信息。另一个可行的方法是将图像中所有的像素都进行深度预测形成伪点云，但是由于像素密度很高，这样会带来很大的计算负担，同样不太可行。

传统的融合方法如下图所示
![](/assets/img/20221008/VPFNetF2.jpg)

为了解决分辨率不匹配的问题，一种方法是将2D提升至3D空间。但是对所有的2D像素都跟整个3D空间建立association关系计算负担太大。因此通常需要对聚合特征进行降采样，比如在BEV平面上。这样丢失了3D信息。本文通过部署virtual point在3D空间的形式来作为多模态数据的聚合点。其密度介于点云和像素之间。比如比点云稠密20倍，以建立点云和像素之间的bridge。virtual point更密集，投影到图像相当于对图像以更高的比例采样。virtual point跟点云的关系则通过K近邻聚合来建立。

VPF的设计思路和目的：1、建立点云和图像分辨率上的bridge 2、平衡图像采样率所带来的accuracy和efficiency。

首先确定，是将2D特征融合到3D中，以保留尽可能多的点信息。其次需要确定在3D空间中聚合信息的位置。这里使用Virtual Point。Virtual point密度介于pixel和point之间。通过采样获得pixel的信息，通过邻域聚合将自己的信息进一步聚合到更稀疏的点云（其实是grid points）上。

本质上来说，本方法这相当于也是继承Voxel-RCNN的方法。Virtual point就是proposal里的grid point（更密集），只是进一步又用这些point去索引图像特征而已。

Virtual的具体产生方式：
1. 通过3D Backbone产生Proposal；
2. 在Proposal里按grid points的方式产生比较密集的virtual point。
3. 使用virtual point去索引图像特征。其中图像特征通过一个简单的4层卷积产生。之后对这些virtual point在经过一个简单的3D稀疏卷积，产生3Dfeature。
4. 同样在proposal里产生传统的6*6*6的grid point。从多个特征图中聚合特征。和Voxelnet不同的是，此时多了一个由virtual points构成的基于图像特征的3d feature，以同样的方式聚合到gridpoint上，实现了对图像和点云信息的聚合（实际上也起到了一个align的作用，即点云物体的part的特征和图像物体的part的特征是对应的，是fine grained的，而不是传统的object level融合直接将两个roi的整体特征串联，coarse grained。）
5. 为了防止出现single-modality
dominance and over-fitting，单独对图像模态的信息增加一个辅助分支，确保其信息的获取。
6. 多模态数据增广，在训练中起到至关重要的作用。

其整体结构如下

![](/assets/img/20221008/VPFNetF3.jpg)

![](/assets/img/20221008/VPFNetF4.jpg)

最后，在数据增广上，这点笔者没有太看懂。但是大概是首先通过预训练的图像分割模型将图像中的实例（前景点）分割出来，并和点云的前景点（通过BBox直接获得）一起操作。基于cut-n-paste方法，将图像、mask和点云一起粘贴到新的场景中。由于这个粘贴同时在两个模态上进行，因此为了保持对应关系，这里作者只在参数矩阵完全相同的场景之间进行这样的操作（Only those scenes that have exactly the same calibration information as the current scene will be selected）。

目前这几个融合方法确实在数据集上也展现出了非常出色的性能，图像信息对于远距离小目标的识别是非常关键的，这也体现在性能评估结果上（在mod和hard上的巨大优势）

![](/assets/img/20221008/VPFNetT2.jpg)