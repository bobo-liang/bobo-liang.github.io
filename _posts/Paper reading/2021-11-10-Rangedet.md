---
layout: post
title: 'RangeDet: In Defense of Range View for LiDAR-based 3D Object Detection'
date: 2021-11-10
author: Poley
cover: '/assets/img/20211110/RangeDet.png'
tags: 论文阅读
---

提出一种基于 range view representation的 anchor-free signle-stage LiDAR-based 3D object detector。

作者认为基于 range image的方法效果不好是因为忽略了两个原因：
+ range image中存在的scale variation
+ range image用于提取特征的坐标系（球坐标系）和最终回归目标的坐标系（笛卡尔坐标系）不一致

为了解决这几个问题，作者分别提出以下的创新：
+ Range Conditioned Pyramid 来解决 scale variation;
+ Meta-Kernel 来从2D range image中捕获3D几何信息;
+ Weighted NMS来高效的利用range image的dense feature。

## Review of Range View Representation

Range image 大小为 $m\times n$，其中$m$由激光雷达的垂直分辨率（线束）决定，$n$由激光雷达的水平角分辨率决定。两个轴分别代表inclination 和azimuth，每个像素点上的值为range，即点云距离传感器的距离。如下图所示

![](/assets/img/20211110/RangeDetF2.png)

## Methodology

### Range Conditioned Pyramid
类似于2D物体检测，使用一个FPN来应对不同scale的目标。但这里有一个问题，就是如何将目标分配给FPN的不同层，因为这些3D目标本质上是差不多大小的，这和2D目标不同。

传统方法通过在range image上的大小来分配，和2D方法一样。但是这样可能会忽略2D和3D的区别，即不同大小的目标可能拥有类似的area。因此，这里通过距离来划分，让相似距离的目标在同一个FPN层上被提取。成为Range Conditioned Pyramid(RCP).

### Meta-Kernel Convolution
![](/assets/img/20211110/RangeDetF3.png)
作者将卷积分为四个部分：sampling, weight acquisition, multiplication and aggregation.

+ Sampleing: 即选择要被加权求和的数据，比如
$$
\begin{equation}
\mathcal{G}=\{(-1,-1),(-1,0), \ldots,(1,0),(1,1)\}
\end{equation}
$$

+ Weight acquisition: 获得加权求和的权重。在卷积中，这部分具有权重共享的特性。
+ Multipilication：加权
$$
  \begin{equation}
\mathbf{o}_{\mathbf{p}_{0}}\left(\mathbf{p}_{n}\right)=\mathbf{W}\left(\mathbf{p}_{n}\right) \cdot \mathbf{F}\left(\mathbf{p}_{0}+\mathbf{p}_{n}\right)
\end{equation}
$$


+ Aggregation: 求和 
$$
\begin{equation}
\mathbf{z}\left(\mathbf{p}_{0}\right)=\sum_{\mathbf{p}_{n} \in \mathcal{G}} \mathbf{o}_{\mathbf{p}_{0}}\left(\mathbf{p}_{n}\right)
\end{equation}
$$

在range image中，我们希望卷积更多的获得局部3D信息，因此作者对上述4个部分中的3个部分进行了修改，通过meta-learning approach。

+ Weight acquisition: 获得两个像素之间的 relationship vector $\textbf{h}(\textbf{p}_0,\textbf{p}_n)$，一般包括相对的笛卡尔坐标，range等等。之后自适应的产生一个权重，通过MLP实现

$$
\begin{equation}
\mathbf{W}_{\mathbf{p}_{0}}\left(\mathbf{p}_{n}\right)=\operatorname{MLP}\left(\mathbf{h}\left(\mathbf{p}_{0}, \mathbf{p}_{n}\right)\right)
\end{equation}
$$

+ Multiplication： 同样，加权。
$$
\begin{equation}
\mathbf{o}_{\mathbf{p}_{0}}\left(\mathbf{p}_{n}\right)=\mathbf{W}_{\mathbf{p}_{0}}\left(\mathbf{p}_{n}\right) \odot \mathbf{F}\left(\mathbf{p}_{0}+\mathbf{p}_{n}\right)
\end{equation}
$$

**个人认为这部分其实就是一个Attention机制。**

这里没有使用矩阵乘法，因为失去了参数共享，权重矩阵会变得非常大。收到 depth-wise 卷积的启发，这里使用element-wise product来消除了权重的$C_{out}$维度，降低了内存需求。（笔者注：但是depth convolusion会增加IO负担，使得计算能力的瓶颈从显存变到IO，推理速度不一定加快）

+ Aggregaion： 取代了之前的求和操作，使用一个FC来自适应的进行 aggregation。
$$
\begin{equation}
\mathbf{z}\left(\mathbf{p}_{0}\right)=\mathcal{A}\left(\mathbf{W}_{\mathbf{p}_{0}}\left(\mathbf{p}_{n}\right) \odot \mathbf{F}\left(\mathbf{p}_{0}+\mathbf{p}_{n}\right)\right), \quad \forall \mathbf{p}_{n} \in \mathcal{G}
\end{equation} 
$$

本方法和其他的点云卷积方法对比如下
![](/assets/img/20211110/RangeDetT1.png)
## Weight NMS

来自ICCV 2015，Object detection via
a multi-region semantic segmentation-aware CNN model。
$$
\begin{equation}
\widehat{\mathbf{b}}_{\mathbf{0}}=\frac{\sum_{k} \mathbb{I}\left(\operatorname{IoU}\left(\mathbf{b}_{\mathbf{0}}, \mathbf{b}_{\mathbf{k}}\right)>t\right) s_{k} \mathbf{b}_{\mathbf{k}}}{\sum_{k} \mathbb{I}\left(\operatorname{IoU}\left(\mathbf{b}_{\mathbf{0}}, \mathbf{b}_{\mathbf{k}}\right)>t\right) s_{k}}
\end{equation}
$$

## Data Augmentation in Range View

将常用的3D数据增强方法搬到2D range image上，主要包括 Random global rotaion, Random global flip, Copy-Paste。

+ Random global rotaion: 相当于Range image中沿 azimuth方向的平移;
+ Random global flip： 相当于Range image中绕某一条垂直线反转整个图像；
上述两种方法增强完了需要重新计算笛卡尔坐标，以和range image一致。
+ Copy-Paste：保持采样样本的垂直坐标不变，直接贴到range image中。但是相同的垂直坐标（激光线）上的距离可能远近不同，因此作者还额外引入了一个range test来防止一个很远的目标贴到了很近的背景之前。

## Overall architecture 
![](/assets/img/20211110/RangeDetF4.png)

对于所有对应与gt 3d bounding box中的点的range image上的点，都视为正样本。每个点的分类损失通过varifocal loss得到，如下
$$
\begin{equation}
\operatorname{VFL}(p, q)=\left\{\begin{aligned}
-q(q \log (p)+(1-q) \log (1-p)), q &>0 \\
-\alpha p^{\gamma} \log (1-p), & q=0
\end{aligned}\right.
\end{equation}
$$

其中$p$是预测得分，$q$是预测框和gt bounding box的IOU，$\alpha and \gamma$和focal loss一样。

### Regression head
这里的回归和LaserNet中一样，将点的azimuth direction看做其local x-axis。其笛卡尔坐标下的中心偏差会被做如下的转换
$$
\begin{equation}
\begin{gathered}
\alpha_{i}=\arctan 2\left(y_{i}, x_{i}\right), \\
\boldsymbol{R}_{\boldsymbol{i}}=\left[\begin{array}{ccc}
\cos \alpha_{i} & \sin \alpha_{i} & 0 \\
-\sin \alpha_{i} & \cos \alpha_{i} & 0 \\
0 & 0 & 1
\end{array}\right], \\
\phi_{i}^{g}=\theta_{i}^{g}-\alpha_{i}, \quad\left[\Omega x_{i}, \Omega y_{i}, \Omega z_{i}\right]=\boldsymbol{R}_{\boldsymbol{i}}\left[\Delta x_{i}, \Delta y_{i}, \Delta z_{i}\right]^{\top}
\end{gathered}
\end{equation}
$$

这是由于**物体的表现并不随着物体在固定距离的情况下沿着azimuth移动**，因此回归的过程应该与azimuth信息无关。因此在朝向和偏移上都删去了和azimuth有关的部分。如下图所示

![](/assets/img/20211110/RangeDetF5.png)

最后的回归向量如下

$$
\begin{equation}
\mathcal{Q}_i=\left\{\Omega x_{i}^{g}, \Omega y_{i}^{g}, \Omega z_{i}^{g}, \log l_{i}^{g}, \log w_{i}^{g}, \log h_{i}^{g}, \cos \phi_{i}^{g}, \sin \phi_{i}^{g}\right\}
\end{equation}
$$
$$
\begin{equation}
L_{\mathrm{reg}}=\frac{1}{N} \sum_{i}\left(\frac{1}{n_{i}} \sum_{q_{i} \in \mathcal{Q}_{i}} \operatorname{SmoothL} 1\left(q_{i}-p_{i}\right)\right)
\end{equation}
$$

## Experiments
![](/assets/img/20211110/RangeDetT2.png)

![](/assets/img/20211110/RangeDetT7.png)
