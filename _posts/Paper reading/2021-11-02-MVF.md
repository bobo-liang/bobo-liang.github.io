---
layout: post
title: 'End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds'
date: 2021-11-02
author: Poley
cover: '/assets/img/20211101/MVF.png'
tags: 论文阅读
---

本文主要提出dynamic voxelization。用于解决体素化中的信息损失问题。

这是一篇比较早期的文章，发表于CVPR2019，主要使用的是2D的卷积网络和多投影信息融合方法。笔者阅读这篇文章的目的主要是了解一下dynamic voxelization的原理。其实现已经被集成到开源3D目标检测框架 mmdetection3d中。

Dynamic voxelization的优点主要在：
+ 不需要预定义voxel容纳的点大小，使得每个点都被模型利用，最小化信息损失
+ 不需要扩充体素到预定义的大小(补0)
+ 克服了随机dropout点/体素的问题，得到了判别性的体素embedding，得到更稳定的检测输出。


![](/assets/img/20211101/MVFF1.png)

传统的体素化方法这里作者称为*hard voxelization*，其有两个事先设定的限制：
+ 体素数：非空体素的最多数量，如果超过了这个数量，则会有非空体素被随机丢弃；
+ 单体素内最多点数： 一个体素内可以容纳的最多点数，如果超过了这个数量，体素内的点将会被随机丢弃；如果一个体素内的点少于这个数量，则需要补0。

用$F_V(\mathbf{p}_i)$来表示将点分配到对应的体素的映射，用$F_P(\mathbf{v}_j)$表示体素到其包含的点的映射，则其可以被表示为如下形式：
$$
\begin{equation}
\begin{aligned}
&F_{V}\left(\mathbf{p}_{i}\right)= \begin{cases}\emptyset & \mathbf{p}_{i} \text { or } \mathbf{v}_{j} \text { is dropped out } \\
\mathbf{v}_{j} & \text { otherwise }\end{cases} \\
&F_{P}\left(\mathbf{v}_{j}\right)= \begin{cases}\emptyset & \mathbf{v}_{j} \text { is dropped out } \\
\left\{\mathbf{p}_{i}\right. & \left.\mid \forall \mathbf{p}_{i} \in \mathbf{v}_{j}\right\} \quad \text { otherwise }\end{cases}
\end{aligned}
\end{equation}
$$

而在dynamic voxelization中，上述两个限制都不存在，而是都变为动态的，取决于特定的映射函数。可以表示如下：
$$
\begin{equation}
\begin{aligned}
&F_{V}\left(\mathbf{p}_{i}\right)=\mathbf{v}_{j}, \forall i \\
&F_{P}\left(\mathbf{v}_{j}\right)=\left\{\mathbf{p}_{i} \mid \forall \mathbf{p}_{i} \in \mathbf{v}_{j}\right\}, \forall j
\end{aligned}
\end{equation}
$$

这样，所有的点和体素之间都建立了bi=directional的关系，形成了多视角特征融合的天然基础。

总的来说，上述的思路很简单。但是体素化的过程之所以要设计成具有上述两个限制条件，主要还是由于程序实现上的限制。因此，dynamic voxelization的思想很简单，但是实现上会比普通的体素化复杂不少，这部分可以参考mmdet3d的代码。

在mmdet3d中，与Hard VFE直接分配一个指定大小的python数组进行处理不同，dynamic VFE需要先建立点和体素之间的双向映射关系。由于没有现成的数组形式，因此每个体素的输出特征(通过体素内的点均值或者max)需要根据其映射到的点来决定。这个过程虽然不难但是需要通过循环等方法实现。mmdet3d中，这部分代码通过cuda实现，从而保证了运行速度。原理并不复杂，但是笔者看不懂CUDA……还需要后续进一步学习。