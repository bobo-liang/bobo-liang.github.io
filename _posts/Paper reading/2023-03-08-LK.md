---
layout: post
title: 'Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs'
date: 2023-3-7
author: Poley
cover: '/assets/img/20230227/LK.jpg'
tags: 论文阅读  
---

本文提出使用少量的大卷积核代替众多的小卷积核，得到更强的卷积backbone，获得可以匹敌vit的性能。

已经有很多工作分析了Vit为什么有效，比如其更灵活，没有归纳偏置，容量更大等。但是本文主要关注与感受野。transformer-based的方法一般都是全局感受野，或者是较大Kernal的局部感受野。而卷积一般则很少用大感受野的核。自然的，作者提出，如果使用少量的大卷积核代替大量的小卷积核会如何？是否可以缩短CNN和VIT的性能差距？

作者总结的5点：1、非常大的kernel是有效的；2、短路链接非常必要；3、重参数化可以帮助优化；4、大卷积核对下游任务的帮助超过ImageNet；5、大卷积核对小feature map也很有用。

接下来将对这5点分别进行分析。

## Guideline 1: large depth-wise convolutions can be efficient in practice.

理论上大卷积核更有助于提高dw卷积的效率，因为小卷积的depth-wise核计算太少，比如3*3的卷积和仅仅进行9次乘法，此时memory access成本占比高。但是作者也注意到pytorch对于depthwise卷积的支持不是很好，效率不高。因此作者选择使用block-wise(inverse) implicit gemm来作为底层算法执行depth wise卷积，这部分代码开源在了megengine中。其性能对比如下图所示

![](/assets/img/20230227/LKT1.jpg)

## Guideline 2: identity shortcut is vital especially for networks with very large kernels.

相关研究表明，transformer在丢失shoutcut之后，性能掉的很厉害。同样，本文也发现大卷积核没有shotcut后，很难去学习local details。实验如下
![](/assets/img/20230227/LKT2.jpg)

## Guideline 3: re-parameterizing with small kernels helps to make up the optimization issue.

重参数化，大致就是在原有基础上加入一些shortcut来优化训练，在inference时再将这些额外的shortcut合并回与其平行的3*3卷积，从而不改变原有模型的结构。示意图如下所示
![](/assets/img/20230227/LKF2.jpg)

这里重参数化的作用主要是引入一些local的归纳偏置。这在解决vit在小数据集上的训练时也经常使用。同样，在大数据训练的情况下，可以不使用重参数化，也不会带来性能退化。对应的消融实验结果如下

![](/assets/img/20230227/LKT3.jpg)

## Guideline 4: large convolutions boost downstream tasks much more than ImageNet classification

这里大感受野对下游任务提升更大的原因是有效感受野更大，更有利于下游任务。
分类任务可以从纹理和形状来分别，但是下游任务中，shape cue是更加重要的。大卷积核带来的有效感受野的扩大的示意图如下所示
![](/assets/img/20230227/LKF1.jpg)

## Guideline 5: large kernel (e.g., 13×13) is useful even on small feature maps (e.g., 7×7).

如下图所示，大卷积核在小featmap上实际上已经不能严格保持平移不变性了。
![](/assets/img/20230227/LKF3.jpg)

但是本文进行的消融实验在已经具有较大感受野的Mobilenet的最后一个stage采用大卷积核，仍然实现了显著的性能提升，体现了大卷积核的有效性
![](/assets/img/20230227/LKT4.jpg)