---
layout: post
title: 'DINO: DETR WITH IMPROVED DENOISING ANCHOR
BOXES FOR END-TO-END OBJECT DETECTION'
date: 2023-3-28
author: Poley
cover: '/assets/img/20230328/DINO.jpg'
tags: 论文阅读  
---

本文是在DN-DETR和DAB-DETR上的进一步改进，性能曲线上可以看到较大的优势，如下所示
![](/assets/img/20230328/DINOF1.jpg)

网络的主要设计：
1、继承Dynamic DETR的动态anchor boxes设计
2、继承DN的denoising设计
3、提出对比去噪训练，在DN的基础上加入负样本噪声
4、提出look forward twice策略来利用tranformer下一层的梯度纠正当前层的梯度
5、提出混合query选择方法

整体结构如下所示
![](/assets/img/20230328/DINOF2.jpg)

作者认为，DN-DETR只提供正样本去噪，而不能促进网络判断背景。这在DETR里很重要，因为后者可以帮助减少冗余框。这里加入离GT较近的负样本，相当于rejecting hard negative example。
为了更好的训练hard negative,这里一般选择较小的阈值来让negative距离gt更近。
对DN的另一个改进是使用动态自适应的denoising组数，其根据图像中的gt数量决定。其划分方法如下所示
![](/assets/img/20230328/DINOF3.jpg)

DETR通过self-attention实现冗余框的抑制，一般是降低得分或者Pushing away。但是这些框一般都不是hard negative。因此，DETR并不能完全避免冗余框，尤其是对于低置信度的。

一般DETR的回归方法是每个层对gt分别进行独立的回归。作者指出，DETR原有的这种回归方式实际上是一种贪心方法，每一层的decoder都在独立的估计gt box。这样的方法稳定了训练并加快了训练早期的收敛速度，但是可能导致次优的结果。如果梯度传递全开可能会导致网络收敛更困难。因此，这里只开了下一层的梯度。如下所示

![](/assets/img/20230328/DINOF4.jpg)

从公式表达上，原版的detr预测框可以表示为
$$
\begin{equation}
b_i^{\prime}=\sigma\left(\sigma^{-1}\left(b_{i-1}\right)+\Delta b_i\right)
\end{equation}
$$

本文的方法引入两帧的回归，可以表示为
$$
\begin{equation}
b_{i+1}^{(\text {pred })}=\sigma\left(\sigma^{-1}\left(b_i^{\prime}\right)+\Delta b_{i+1}\right)=\sigma\left(\sigma^{-1}\left(b_{i-1}^{\prime}\right)+\Delta b_i+\Delta b_{i+1}\right)
\end{equation}
$$

这样相当于前层网络同时考虑到后层网络的回归结果，从各自独立回归变成联合回归，这使得浅层网络的回归性能更差，但是经过多层网络后，总的回归性能得到提升。

最后，是对query初始化的调整。如下图所示

![](/assets/img/20230328/DINOF5.jpg)

这里调整query的初始化方式主要是为了和CDN保持一致。因为CND和DN确定ref point位置简单，但是不好确定content。因此这里全部使用可学习的content，以及选择的Position。可学习的content应该指0初始化。


![](/assets/img/20230328/DINOF6.jpg)
![](/assets/img/20230328/DINOT5.jpg)