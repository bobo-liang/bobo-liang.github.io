---
layout: post
title: 'Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D'
date: 2022-10-25
author: Poley
cover: '/assets/img/20221025/LSS.jpg'
tags: 论文阅读
---

本文发表在ECCV2020上，其提出一种图像向BEV特征转换的方式，广泛的应用于现在基于BEV特征的camera-lidar特征融合上。其标题的含义为
+ lift,提升（将2D提升到3D）
+ splat,泼(将3D特征拍落在平面上)
+ shoot,将运动规划照射在bev cost map上（运动规划相关，这里笔者不太关心）


一种简单的融合多个camera的方法是分别检测，根据内外参数将检测的object映射到同一坐标系中。
其具有三个有价值的对称性：
1. 平移不变
2. 顺序不变
3. Ego-frame等距等价

缺点：每个camera独立进行的Post-processing是不可导的，导致不能学习data-driben的融合信息的方法。而本文中实现的将Image feature在BEV空间下的融合，除了实现多传感器的特征融合以外，其分割结果也可以快速投影回相机图像中，形成图像中分割结果的可视化，实际上还可以产生类似地图的效果，如下图所示

![](/assets/img/20221025/LSSF1.jpg)

image level的分割和bev level的分割的差异如下图所示

![](/assets/img/20221025/LSSF2.jpg)

## Lift: Latent Depth Distribution
这里lift的方法实际上已经在相关的几篇笔记中记录了，这里就不过多赘述。总的来说，这里相当于对之前集中基于图像的3D检测方法的深度预测进行了折中。
当这种深度预测是one-hot时，这就退化成了pesudo-lidar，每个图像点可以唯一的投影成空间中的一个点云。当权重在射线上均匀分配时，就退化成OFT，即将图像的feature均匀的分配在其所属的射线的空间上。

这里则是在射线上预定义一系列距离，预测pixel属于这些距离的权重。从而将pixel的feature有权重的分配到这些对应的空间位置上，即lift。总的来说，这相当于把camera的特征映射到了空间中的一个furstum上。
![](/assets/img/20221025/LSSF3.jpg)

## Splat: Pillar Pooling
这一步的主要工作是将上述空间中若干的点全部scatter到对应的bev feature上。将这么多点scatter到bev map上是很费时间的。因此本文提出一个cumsum trick的方法来加速这一过程。采用逐个Bin累加的方式，每个bin结束的时候，当前累加值减去上一个Bin的累加值即可得到当前Bin的sum。这一部分在后续的bevfusion中进一步呗改进，实现了更高的性能。

## Shoot: Motion Planning
上述通过得到BEV特征之后很自然的可以得到BEV下的分割结果，而轨迹规划任务一般也是在BEV下的， 那么关于轨迹规划的cost可以很轻松从分割结果中计算得到。由于笔者不搞这个，这部分就不展开了。


## Experiment
作者也在实验中对camera的可插拔特性进行了探究，得到了缺失不同相机对分割结果的影响，如下图所示

![](/assets/img/20221025/LSSF7.jpg)

对每个相机的重要性进行了消融。和直观印象类似，front和back的最重要。back影响更大，因为其具有最大的视野（丢失后损失的性能最多。）