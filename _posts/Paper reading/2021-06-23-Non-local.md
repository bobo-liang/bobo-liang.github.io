---
layout: post
title: 'Non-local Neural Networks'
date: 2021-06-23
author: Poley
cover: '/assets/img/20210607/NL.png'
tags: 论文阅读
---

> 论文链接 ： https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf

目的：**捕捉Long-range dependencies，时间上的或者空间上的**。

# Inreoduction

传统的Recurrent或者Convlution网络都是捕捉局部信息的，为了捕捉到更大范围的信息，只能不断的重复这种网路哦结构，带来的大的计算量，以及BP时候的困难。因此提出了本网络，这种**non-local  operation是经典non-local mean operation在CV上的泛化。这种操作是通过甲酸feature map上的所有点的加权求和来计算一个点的输出响应**。

# Non-local Neural Networks

## Formulation

Non-local Neural Networks的定义如下
$$
\begin{equation}
\mathbf{y}_{i}=\frac{1}{\mathcal{C}(\mathbf{x})} \sum_{\forall j} f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) g\left(\mathbf{x}_{j}\right)
\end{equation}
$$

上述操作的特点是所有位置的信息都被包含进来，这点是它和卷积的主要区别。和fc不同的是，这里计算点对函数，而不是直接学习权值。

## Instantiations
一些点对函数的例子。

**Gaussion**
$$
\begin{equation}
f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=e^{\mathbf{x}_{i}^{T} \mathbf{x}_{j}}
\end{equation}
$$
**Embedded Gaussion**
$$
\begin{equation}
f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=e^{\theta\left(\mathbf{x}_{i}\right)^{T} \phi\left(\mathbf{x}_{j}\right)}
\end{equation}
$$
self-attention可以视为Embedded Gaussion的一种特殊情况。
**Dot product**
$$
\begin{equation}
f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\theta\left(\mathbf{x}_{i}\right)^{T} \phi\left(\mathbf{x}_{j}\right)
\end{equation}
$$
**Concatenation**
$$
\begin{equation}
f\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\operatorname{ReLU}\left(\mathbf{w}_{f}^{T}\left[\theta\left(\mathbf{x}_{i}\right), \phi\left(\mathbf{x}_{j}\right)\right]\right)
\end{equation}
$$

## Non-local Block
$$
\begin{equation}
\mathbf{z}_{i}=W_{z} \mathbf{y}_{i}+\mathbf{x}_{i}
\end{equation}
$$

如上，以一种残差模块的形式组成网络。
