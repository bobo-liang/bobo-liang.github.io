---
layout: post
title: 'A ConvNet for the 2020s'
date: 2022-12-17
author: Poley
cover: '/assets/img/20221205/ConvNeXt.jpg'
tags: 论文阅读  
---

本文主要是提出一种全新整合的卷积神经网络设计，吸纳了诸多的先进方法，并对标Transformer，通过现代化改进使得卷积网络具有和Transformer（主要是Swin）类似的结构、设计和特性，从而大幅提高了卷积网络的性能上限，同时也无需引入任何额外的特殊模块。其在多个尺度上的性能都实现了对Swin的超越，同时在下游任务和模型的部署难度上也都具有优势。其具体性能如下图所示

![](/assets/img/20221205/ConvNeXtF1.jpg)


Transformer优秀的scaling特性是其性能优于卷积的关键，其中multi-head self-attention是最关键的。原生的Transformer使用Global attention，但是其并不完全适合于CV任务，直到Swin Transformer的出现。Swin同样将整体划分为若干窗口，这使得Swin和Conv具有类似的归纳偏置，但是在训练和宏观微观结构上又差异很大。本文则基于Resnet，对标Swin的结构设计，对卷积网络其进行现代化改进。

这里主要分为三部分：
## 训练方法

Transformer不仅带来了一系列新模块和结构设计，同样也带来了不同的训练技巧，比如adamW，以及更多现代化的数据增广方法，比如：Mixup Cutmix RandAugment Rand Erasing，以及比较现代化的正则方式，Stochastic Depth， Label Smoothing。通过用这些方法重新训练ResNet50，大幅提升了baseline的性能。

## 宏观设计 Macro Design

首先，在计算比例的分配上，也跟swin对齐为1:1:3:1，实现了性能的提高（实验结论）？在宏观设计上，首先是stem设计。传统卷积用大卷积核+stride实现下采样。Transformer更简单也更激进，直接划分为无重叠的大patch(即大卷积核)来对图像进行下采样。这里使用了Swin的stem，即4*4，s=4的无重叠卷积。对性能提升不明显。

在卷积的选择上，参考ResNetXt中分组卷积的成功（其类似于MultiHead）,而depthwise卷积可以视为grouped卷积的特殊情况。大卷积核的depthwise指融合空间上的信息，而1*1卷积则只融合channel上的信息。这类似于transformer的self-attention和ffn的关系。在ResNext架构下，将通道数调整为和Swin一致，得到的性能的提升。

Transformer中另一个关键设计是反向bottolneck即FFN的中间层有channel*4的过程。这里也将其copy过来到cnn的block中。由于这样实际上外部的通道数是减少的，因此减小了Flop，并轻微提升了性能。

同样，为了模拟Transformer中的large kernal，这里把depthwise卷积提前了。（因为MSA模块在FFN之前。）这样实现了Flop的进一步减小，虽然带来了暂时的性能衰退，但是后来被大卷积核（7*7）所弥补。注意到，这里的7*7卷积也是跟Swin的7*7 local attention对齐的。实验证明，更大的卷积核并不能带来额外的收益。如下图所示

![](/assets/img/20221205/ConvNeXtF3.jpg)

## 微观设计 Micro Design
这里主要关注于激活函数、归一化层的类型和数量。简单的说，在BN的设置上也和Swin对齐，使用尽可能少的激活函数和BN。这里归一化层甚至使用的比Swin更少。在归一化层上使用了LN，激活函数上使用了更新的GELU方法。其中，RELU和GELU的替换对性能没有影响，而更少的激活层和归一化层则对性能又显著的提升。在下采样的设计上，同样采取了和Swin对齐的方式。并且实验表明，降采样过程中，normalization layer有助于维持稳定的训练。


最终，在性能上，其实现了优于Swin的性能。
![](/assets/img/20221205/ConvNeXtT1.jpg)

总的来说，笔者个人认为本文章通过大量的实验验证了目前网络设计中很多选择的有效性。同时提出的纯卷积ConvNeXt在部署上也和ResNet没有任何区别，可能可以在自己所在的下游任务中实现无痛提点的作用。
