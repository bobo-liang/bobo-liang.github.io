---
layout: post
title: 'PointAugmenting: Cross-Modal Augmentation for 3D Object Detection'
date: 2022-10-10
author: Poley
cover: '/assets/img/20221008/PA.jpg'
tags: 论文阅读
---

参考博客: https://blog.csdn.net/Rolandxxx/article/details/123335497

## 摘要+intro
作者认为fusion的核心问题在于将BEV视角的点云信息和camrea信息融合。在分割分数的帮助下，PointPainting已经作为一种流行的baseline，PointPainting模型是用图像级的分割分数来分割点云，通过转换矩阵得到lidar2image的检索信息，根据将每个点在图像上得到的语义分割分数融合到lidar的表示上。1）尽管PointPainting有了令人印象深刻的改进，但在覆盖图像中的颜色和纹理方面，分割分数是次优的。直观地说，图像的高维CNN特征比分割分数包含更丰富的外观线索和更大的接受场，因此更有利于与点云的融合。本文提出了一种新的跨模态三维目标检测算法，称为PointAugmenting。PointAugmenting利用预先训练的二维检测模型提取的相应的点状CNN特征来修饰点云，然后对降维后的点云进行三维目标检测。与用高度抽象的语义分割分数来修饰点云相比，使得检测网络的CNN特征适应了目标的外观变化，取得了显著的改善。2）现有的仅限LiDAR的探测器广泛使用GT-Paste，直接将GT-Paste应用于跨模态，检测器会破坏LIDAR点与camera图像之间的一致性。为了解决这个问题，我们提出了一种简单而有效的跨模态增强方法，使GT-Paste既适用于点云，也适用于图像。

## 贡献点
1）从二维目标检测网络中挖掘出有效的CNN特征作为图像表示，与LiDAR点融合用于三维目标检测。
2）考虑到保持摄像机和激光雷达之间的模态一致性，我们提出了一种简单而有效的训练三维目标检测器的跨模态数据增强方法

## PointAugmenting
采用CenterPoint作为仅限LiDAR的baseline，并通过跨模式融合机制和有效的数据增强方案对其进行扩展。整体思路大概是把每个点云数据透过相机内外参数投影到图像坐标中，在将图像中的CNN特征加到该点上，然后进行体素化（应该是指的体素分区），然后再将体素化的再转为BEV，进行常规的3d目标检测。
### 跨模态融合
![](/assets/img/20221008/PAF3.jpg)
**point-wise特征提取**
作者提出，相比PointPainting中使用分割分数作为点云的额外特征，
作者用CNN图像特征来装饰LiDAR点。为了提取图像的point-wise特征，我们使用现成的网络进行二维目标检测，而不是语义分割。原因是因为作者认为二维和三维目标检测是互为补充的任务，它们关注的是不同粒度的目标，它们相互受益。其次，2D检测标签很容易从3D投影中获得，而分割标签是昂贵的，并且通常是不可用的。作者这里用的centernet的DLA34输出激活作为图像特征，为了提取相应的point-wise图像特征，我们通过齐次变换将LiDAR点投影到图像平面上以建立对应关系。然后，利用提取的逐点图像特征附加LiDAR点作为网络输入进行检测。
**3D检测**
融合的LiDAR点可以用(x，y，z，r，(t)，fi)表示。其中x，y，z是位置坐标，r表示反射率，t是相对时间戳，fi为64维度的图像特征。考虑到多模态之间的差异还有激光雷达和摄像机之间不同的数据特性，不同于PointPainting所使用的point-wise concatenation，我们采用了一种跨通道的后期融合机制。在体素特征编码之后，我们使用两个独立的3D稀疏卷积分支来处理LiDAR和相机特征。然后，我们将两个下采样的3D特征体展平为2DBev图，每个图的通道数为256。然后这2个BEV图按照通道concatenate，被送到四个二维卷积块中进行特征聚合。最后在聚集的特征与先前的camera和LiDAR的Bev特征之间添加跳层连接，最后被送到RPN中。

## 跨模态数据增强
GT-Paste会导致3D和2D间的不一致性，那么如何缓解这种不一致性呢？最简单的思路就是，对2D和3D同时做增广，那么作者也确实是这么做的。
Augmentation for LiDAR Points
我们将LI-DAR点(x，y，z)变换为LiDAR球坐标系为(r，θ，φ)。也就是说如果随机添加点云数据，可能其在2D图像上是被遮挡的，也因此会导致mismatching problem。做法也很直接，其实就是去除遮挡的点就可以了。去除的原则总结起来只有一条，就是"留近去远"，保留近处points，去除远处occluded points。
Augmentation for Camera Images
为了匹配LiDAR和摄像机之间的一致性，对于粘贴到LiDAR场景中的每个虚拟对象，我们将其在2D边界框内的对应patch附加到图像上。二维包围盒是从三维地面真实投影中获得的，为了确定粘贴位置，我们注意到虽然虚拟patch被粘贴在LiDAR场景中的原始位置，但由于摄像机外部参数的变动，虚拟patch贴到这个图像上时并不位于它原始图像平面的位置。我们需要通过当前的摄像机外定标重新计算二维包围盒的位置，然后对原始patch进行平移和缩放变换。才能保证LIDAR和image的位置相对应。

之所以要保持点云和图像的一致性，是因为如果点云图像不一致的话，在点云反投影图像获取图像feature的时候会产生mismatching。这就是为何点云和图像需要同时具备合理的遮挡关系。这里作者的方法是，对于图像，将所有的目标都抠出来（包括真实的和粘贴的），从远到近依次贴到图像上，这样近处的patch自然的覆盖了远处的。对于点云则复杂一点，对于真实物体，只需要遮挡其背后的被粘贴物体（从透视图上可以判断什么物体被其遮挡，perspective view），由此实现在2D和3D上一致的增广。保持点云和图像在投影的时候可以获得相匹配的特征。总体效果如下图所示。
![](/assets/img/20221008/PAF4.jpg)