---
layout: post
title: 'PETRv2: A Unified Framework for 3D Perception
from Multi-Camera Images'
date: 2022-12-26
author: Poley
cover: '/assets/img/20221226/PETRV2.jpg'
tags: 论文阅读  
---

PETRV2是基于PETR的拓展，在PETR的基础上进一步进入了时序，并将分割任务统一到框架中来。对于分割任务，PETRV2基于这样一个基本思路：如果基于query生成的bev featuremap不适合密集预测，可以将seg也变成基于query的形式？

首先，PETRV2关注于时序信息的引入。由于PETR隐式的将3D位置编码到2D特征里，因此不好进行显式的特征转换（对齐）

为了完成分割任务（稠密预测），BEV Feature必不可少。目前最新的方法BEVFormer中，密集的BEV QUERY可以实现Feature map的效果。但是其query数量太大，不适合做global attention。这里参考目前一些先进的分割方法，引入若干seg query，负责patch的分割，并结合为整体图像的分割。

针对PETR的PIPELINE，本文在3DPE的基础上加入fuature-guided position encoder。即通过feature预测一个权重为3DPE加权。

本文的整体结构如下，笔者这里主要还是关注其对于时序信息的引入

![](/assets/img/20221226/PETRV2F1.jpg)

这里使用的相当于是隐式的方法。原版的3DPE中，为图像建立一个Frustrum,其中每个图像点包含若干个深度（一般为64）。之后全部投影到3D空间中，相当于将图像特征隐式投影到3D空间中进行处理。因此很自然的，对于另一时间的图像，同样将其投影到3D空间，然后做自车补偿即可。这样上一帧图像的就也都可以得到其在本帧坐标系中合理的3D坐标，从而进行和PETR相同的编码。具体如下所示

本帧的图像对应的3D点可以从2D点（with depth）中通过相机内参和外参得到。
$$
\begin{equation}
P_i^{l(t)}(t)=T_{c_i(t)}^{l(t)} K_i^{-1} P^m(t)
\end{equation}
$$

自然的，前一帧的也可以这样做
$$
\begin{equation}
P_i^{l(t)}(t-1)=T_{l(t-1)}^{l(t)} P_i^{l(t-1)}(t-1)
\end{equation}
$$

综上，分别将图像转换成在各帧中的3D点，事情就变得很简单了，通过自车运动补偿即可以对齐两者。

$$
\begin{equation}
T_{l(t-1)}^{l(t)}=T_{e(t)}^{l(t)} T_g^{e(t)} T_g^{e(t-1)^{-1}} T_{e(t-1)}^{l(t-1)^{-1}}
\end{equation}
$$

同时，v2版本相比PETR，增加了图像特征对position embedding的加权，这相当于隐式的加入了一个分割任务，降低了背景的权重。
$$
\begin{equation}
P E_i^{3 d}(t)=\xi\left(F_i(t)\right) * \psi\left(P_i^{l(t)}(t)\right)
\end{equation}
$$

上述时序和PE相关的操作消融实验如下
![](/assets/img/20221226/PETRV2FT3.jpg)

可以看到，在不加入CA和FPE的情况下，引入时序对PETR对NDS的提升相当显著，对MAP则略有提升，而CA对引入时序非常重要。FPE则是对时序和非时序模型都具有显著的性能提升。