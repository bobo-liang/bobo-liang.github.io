---
layout: post
title: 'Voxel Transformer for 3D Object Detection'
date: 2021-12-28
author: Poley
cover: '/assets/img/20211228/VoTr.png'
tags: 论文阅读
---

作者提出 Voxel Transformer方法，主要是将Transformer使用到体素的Backbone上，并解决了一些相关的问题。传统卷积的缺点主要在于感受野小，需要很深的网络才能达到大的感受野，而self attention机制可以很轻松的引入更大的感受野。和卷积类似，本文同样引入sparse voxel module和submanifold voxel module来作为Backbone的组成模块。参考Multihead机制，这里提出两种Attention的选择方法，Local Attention 和 Dilated Attention。在实现方面，高效的索引稀疏的非空体素，作者通过CUDA实现了 Fast Voxel Query。

CNN的感受野受到深度的限制，3D目标检测常用的8x下采样Backbone来说，对于常用的体素大小$(0.05m,0.05m,0.1m)$，最终的感受野大小只有$(3.65m,3.65m,7.3m)$。相比之下，Transformer可以轻易达到更大的感受野。

但是Transformer并不能直接利用到体素上，像图像一样。因为体素是稀疏的，非空体素只占总体素数量的约0.1%。因此不能像图像那样直接分割Patch。同时，及时比例很小，非空体素的数量也达到了约$90k$（上述数字均指Waymo数据集）。直接对这么多体素做全连接和self-attention民新概念是不现实的。因此本文提出了两种不同的索引 attend voxel的形式， Local Attention 和 Dilated Attention。

VoTr的整体结构如下所示，其作为一个Backbone，可以轻易替换到一些Voxel-based的目标检测算法中，比如SECOND和PV-RCNN。

![](/assets/img/20211228/VoTrF2.png)

首先，在注意力的机制上，和传统的略有不同，这里加入了一个相对位置，个人认为其实就相当于做了位置编码了。如下所示

$$
\begin{equation}
Q_{i}=f_{i} W_{q}, K_{j}=f_{j} W_{k}+E_{\text {pos }}, V_{j}=f_{j} W_{v}+E_{\text {pos }}
\end{equation}
$$
$$
\begin{equation}
E_{\text {pos }}=\left(p_{i}-p_{j}\right) W_{\text {pos }}
\end{equation}
$$
$$
\begin{equation}
f_{i}^{\text {attend }}=\sum_{j \in \Omega(i)} \sigma\left(\frac{Q_{i} K_{j}}{\sqrt{d}}\right) \cdot V_{j}
\end{equation}
$$
这里同样分为 Submanifold Voxel module 和 Sparse Voxel module。主要区别和卷积中的一致，但是对于Sparse Voxel module，由于空体素没有体素特征，因此没法做Query。这里使用插值或者pooling的方法从其邻域体素内得到空提速的特征，进而进入self-attention的流程中。

由于体素数量过多，需要先进行筛选，再进行self-attention。这个过程类似于卷积，这里推出两种搜索策略，分别是Local 和 Dilated，原理如下图所示。
![](/assets/img/20211228/VoTrF3.png)

其实个人理解就是和卷积一样的搜索范围，只不过将里面的非空体素拿出来做self-attention,而不是直接做卷积的加权求和。其邻域包含的位置$\Omega_{local}, \Omega_{dilated}$可以表示如下

$$
\begin{equation}
\Omega_{\text {local }}(i)=\varnothing\left(v_{i}-R_{\text {local }}, v_{i}+R_{\text {local }},(1,1,1)\right)
\end{equation}
$$

同理，Dilated方法使用步进的stride来搜索更大范围上的非空体素，其表达式如下
$$
\begin{equation}
\begin{array}{r}
\Omega_{\text {dilated }}(i)=\bigcup_{m=1}^{M} \varnothing\left(v_{i}-R_{\text {end }}^{(m)}, v_{i}+R_{\text {end }}^{(m)}, R_{\text {stride }}^{(m)}\right) \backslash \\
\varnothing\left(v_{i}-R_{\text {start }}^{(m)}, v_{i}+R_{\text {start }}^{(m)}, R_{\text {stride }}^{(m)}\right)
\end{array}
\end{equation}
$$

上述方案的主要实现难度在于对每个体素邻域的快速索引，因为非空体素的数量较多（约90k），逐个搜索的速度和成本是不可接受的。同时由于体素稀疏，因此也不能像图像那样直接索引附近的位置。这里提出了Fast Voxel Query方法，通过建立hash表和CUDA并行来快速的对所有体素进行索引。由于这里的非空体素都是无需排列的，并和自己的索引相互对应。因此每一个空间3D坐标都要所以到非空体素Array的一个index。这里通过对所有非空体素建立hash表，以3D坐标为key，对应得voxel array的index为value。attending index 的计算非常简单，之后直接在hash表中进行搜并返回对应的坐标，放弃空体素索引。这样对于具有$\Omega(i)$个邻域的体素，其搜索成本为$O(N_\Omega)$，大大降低了搜索成本。同时上述过程可以在CUDA中并行，实现很高的效率。
![](/assets/img/20211228/VoTrF4.png)

## Experiments

可以看出，由于更大的感受野，网络可以在Hard或者远距离目标上具有更好的性能。这在Waymo和KITTI都有所体现
![](/assets/img/20211228/VoTrT1.png)

![](/assets/img/20211228/VoTrT2.png)