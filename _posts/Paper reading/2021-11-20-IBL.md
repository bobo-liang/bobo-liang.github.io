---
layout: post
title: 'Influence-Balanced Loss for Imbalanced Visual Classification'
date: 2021-11-18
author: Poley
cover: '/assets/img/20211116/IBL.png'
tags: 论文阅读
---
作者提出一种re-weight的loss来解决训练中类别不平衡的问题。与之前的众多基于loss的方法不同，这种方法通过样本对于决策边界的影响来决定per-sample weight。

目前不平衡学习主要可以分为三种方法：
+ data-level approach: re-sampling 或者生成合成样本。
+ cost-sensitive re-weighting;approach: 通过新设计的Loss来考虑不同样本的重要性; 
+ meta-learning approach: 通过元学习，来藏羌上述两种方法的性能。

其中data-level approach会地阿莱很大的额外计算负担，而元学习方法需要额外的无偏数据或者meta-sampler，同样有很大的计算开销。因此，最实用的方法是通过新设计的loss来解决不平衡问题。

Re-weighting方法主要关注与如何分配class penalties来移动决策边界。之前很多方法根据类别出现的频率来决定类别中的样本权重，这些方法对于同一类别中的数据均使用相同的权重。但是同一类别中的样本对于模型参数的影响是不一样的。这里作者认为应该根据每个样本对于模型的影响能力来进行re-weighting，即 sample-wise loss function。

但不平衡训练中，大多数的hard samples都来自于major class，网络对于这些hard samples的拟合会将决策边界进一步的向minor class靠近。这里作者使用Influence function来得到一个共识，来展示每个样本对于决策边界的复杂性和bias的影响能力，并以此作为依据来为样本分配不同的权重。

本方法分为两个阶段，standard training 和 fine-tuning for influence balancing。


## Method

众所周知DNN具有足够的网络容量来记住所有的样本，因此在决策边界上很容易发生过拟合。尤其是当类别不平衡时，大量的major class samples 穿插在稀疏的 minor class samples 中，形成了过渡区域的主流，从而导致决策边界大大的向minor class 靠近。如下图所示。
![](/assets/img/20211116/IBLF1.png)

上图中的黑色X就是具有高inflence的样本，即导致complex和bias边界的样本。通过降低这些样本的权重，可以得到更加平滑的边界，如上图$(b)$所示，即更合理的边界。

### Influence Function
给定经验风险
$$
\begin{equation}
R(w)=\frac{1}{n} \sum_{i=1}^{n} L\left(y_{i}, f\left(x_{i}, w\right)\right)
\end{equation}
$$
网络的最优参数可以表示为
$$
\begin{equation}
w^{*} \stackrel{\operatorname{def}}{=} \operatorname{argmin}_{w} R(w)
\end{equation}
$$
Influence function定义如下
$$
\begin{equation}
\mathcal{I}(x ; w)=-H^{-1} \nabla_{w} L(y, f(x, w))
\end{equation}
$$
where $H \stackrel{\text { def }}{=} \frac{1}{n} \sum_{i=1}^{n} \nabla_{w}^{2} L\left(y_{i}, f\left(x_{i}, w\right)\right)$ ，这里海森矩阵是正定的，基于损失函数$L$在最优点$w^*$附近是严格局部凸的假设。

### Influence-balanced weighting factor

上述Influence Function由于要计算海森矩阵，基本不可行。但是这里我们并不需要绝对的Influence Function数据，而只需要相对值。因此可以忽略海森矩阵的影响，得到 IB weighting factor如下
$$
\begin{equation}
\mathcal{I} \mathcal{B}(x ; w)=\left\|\nabla_{w} L(y, f(x, w))\right\|_{1}
\end{equation}
$$

### Influence-Balanced Loss
交叉熵损失可以表示为
$$
\begin{equation}
L(y, f(x, w))=-\sum_{k}^{K} y_{k} \log f_{k}
\end{equation}
$$
由于这里主要关心决策边界的判定，因此关注的对象是处于网络最后端的全连接层。网络提取的特征为$h=\left[h_{1}, \cdots, h_{L}\right]^{T}$，通过全连接层之后的输出表示为$f(x, w)=\left[f_{1}, \cdots, f_{K}\right]^{T}$，其中$f_{k}:=\sigma\left(w_{k}^{T} h\right)$。可以得到
$$
\begin{equation}
\frac{\partial}{\partial w_{k l}} L(y, f(x, w))=\left(f_{k}-y_{k}\right) h_{l}
\end{equation}
$$
进而
$$
\begin{equation}
\begin{aligned}
\mathcal{I} \mathcal{B}(x ; w) &=\sum_{k}^{K} \sum_{l}^{L}\left|\left(f_{k}-y_{k}\right) h_{l}\right| \\
&=\sum_{k}^{K}\left|\left(f_{k}-y_{k}\right)\right| \sum_{l}^{L}\left|h_{l}\right| \\
&=\|f(x, w)-y\|_{1} \cdot\|h\|_{1},
\end{aligned}
\end{equation}
$$
表示了当前sample对于决策边界的影响力。因此，其倒数可以作为权重来对每个样本进行动态加权，如下

$$
\begin{equation}
L_{I B}\left(y, f(x, w)\right)=\frac{L(y, f(x, w))}{\|f(x, w)-y\|_{1} \cdot\|h\|_{1}}
\end{equation}
$$

最后，加入一个类别平衡因子$\lambda_{k}=\alpha n_{k}^{-1} / \sum_{k^{\prime}=1}^{K} n_{k^{\prime}}^{-1}$，得到最终的损失为
$$
\begin{equation}
L_{I B}(w)=\frac{1}{m} \sum_{(x, y) \in D_{m}} \lambda_{k} \frac{L(y, f(x, w))}{\|f(x, w)-y\|_{1} \cdot\|h\|_{1}},
\end{equation}
$$

加入类别平衡因子是因为，由于major class 和 minor class数量的差异，决策边界更倾向于会收到major class更多的影响。为了平衡这方面的差异，因此降低了major class的权重。

总体流程如下所示
![](/assets/img/20211116/IBLA1.png)

## Experiment 
![](/assets/img/20211116/IBLF2.png)
![](/assets/img/20211116/IBLF3.png)
![](/assets/img/20211116/IBLT3.png)