---
layout: post
title: 'Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection'
date: 2021-10-13
author: Poley
cover: '/assets/img/20211012/GFLV1.png'
tags: 论文阅读
---

> 论文链接：https://arxiv.org/abs/2006.04388
> 参考博客: https://zhuanlan.zhihu.com/p/147691786

 一阶段检测器通常通过密集分类和定位了实现目标检测。分类通常使用Focal loss，box location通常在Dirac delta distribution下学习。最近的趋势中，单阶段检测器通常印度一个独立的预测分支来估计定位的quality（比如IoU预测和Centerness）。 本文发现了上述机制的两个问题：

1. 目标框质量估计和分类在训练和预测中的不一致性：两者在训练中是独立的，在test中又被合并使用；
2. Dirac delta distribution 对于定位来说具有不灵活性，当场景复杂是可能效果不好。


# 问题

## 目标定位质量估计和分类得分在训练和推理中的不一致性

+ 两者独立训练，并结合起来用于推理，比如相乘（IoU score * class confidence）。
+ 定位质量估计只对正样本有监督，这可能使得负样本具有不可控的高质量预测。这种Gap可能导致检测性能的下降。比如具有高质量得分的负样本可能在NMS中排在具有低质量得分的正样本之前。如下图所示

![](/assets/img/20211012/GFLV1F2.png)

## Bounding box表征的不灵活性
 + 广泛使用的Box representation可以被视为Dirac delta distribution，但是其忽略了数据集中可能存在的**模糊性和不确定性**。如下图所示。一些工作将其改变为一个高斯分布，但是这种假设过于简单，实际上的分布应该更任意更灵活，并不一定是想高斯分布一样对称的。

![](/assets/img/20211012/GFLV1F3.png)

# 解决方案

## 定位质量表征

本文为了解决其和分类得分不一致的问题，将两者合并为一个表示。同样使用一个分类向量，**在代表其GT的Index上的数值对应其localization** quality，本文中是IoU。如此可以实现训练和测试的end-to-end。这样可以消除训练测试的不一致性，允许定位质量和分类之间更强的相关性。这样，负样本一定会以0定位质量被监督，也消除了上述提到的不可控负样本定位质量的问题。

## 标注框表征

提出使用任意分布（这里成为General distruibution）来直接学习一个离散的分布，在其连续空间上，而不需要更强的先验，比如高斯假设。

## Focal Loss ，QFL和DFL

上述提到，使用了IoU和类别的联合分类向量，因此这个时候得到的IoU标签 $[0,1]$，但是原始的Focal Loss只支持离散的类别标签${0,1}$。因此本文提出拓展Focal Loss到连续域，称为**Generalized Focal Loss**。同时GFL还可以特化为 Quality Focal Loss 和 Distribution Focal Loss。

### Focal loss
$$
\begin{equation}
\mathbf{F L}(p)=-\left(1-p_{t}\right)^{\gamma} \log \left(p_{t}\right), p_{t}=\left\{\begin{aligned}
p, & \text { when } y=1 \\
1-p, & \text { when } y=0
\end{aligned}\right.
\end{equation}
$$

### Quality Focal Loss

![](/assets/img/20211012/GFLV1F4.png)

主要变化有两个
+ 将交叉熵部分拓展为完整的交叉熵形式
+ 修改了Focal Loss的权重部分，改成了绝对值方式
  
$$
\begin{equation}
\mathbf{Q F L}(\sigma)=-|y-\sigma|^{\beta}((1-y) \log (1-\sigma)+y \log (\sigma))
\end{equation}
$$

### Disruibution Focal Loss
本文同样使用点到bounding box四边的距离作为回归目标。传统的方法将回归目标视为狄拉克分布，真值y可以通过对输出的平均（积分）得到，如下
$$
\begin{equation}
y=\int_{-\infty}^{+\infty} \delta(x-y) x \mathrm{~d} x .
\end{equation}
$$

如之前提到的，作者希望用一个General ditribution来代替狄拉克分布或者高斯分布，通过积分得到真值标签$y$的估计$\hat{y}$,这个估计值呗限制在$[y_0,y_n]$中。

$$
\begin{equation}
\hat{y}=\int_{-\infty}^{+\infty} P(x) x \mathrm{~d} x=\int_{y_{0}}^{y_{n}} P(x) x \mathrm{~d} x
\end{equation}
$$

为了方便实现，这里将$\hat{y}$的值域离散化，如下$\left\{y_{0}, y_{1}, \ldots, y_{i}, y_{i+1}, \ldots, y_{n-1}, y_{n}\right\}$，之后以求和的方式代替积分，达到相同的效果
$$
\begin{equation}
\hat{y}=\sum_{i=0}^{n} P\left(y_{i}\right) y_{i}
\end{equation}
$$

其中，概率$P(y_i)$直接通过softmax就可以获得。

![](/assets/img/20211012/GFLV1F5.png)

如上图B所示，不同的分布表示了对于Bounding box不同的置信度。其中(3)说明对其的预测更加准确，对$P(x)$的优化会鼓励高置信度的值尽可能的靠近真值$\hat{y}$。于是其会使得网络的预测概率迅速向真值两侧的值靠拢。由于回归只对正样本进行，因此无需考虑样本平衡的问题。将DFL带入QFL的交叉熵部分得到
$$
\begin{equation}
\operatorname{DFL}\left(\mathcal{S}_{i}, \mathcal{S}_{i+1}\right)=-\left(\left(y_{i+1}-y\right) \log \left(\mathcal{S}_{i}\right)+\left(y-y_{i}\right) \log \left(\mathcal{S}_{i+1}\right)\right)
\end{equation}
$$

上述优化函数使得其最大化真值附近的若干个概率，比如$y_i,y_{i+1}$,i.e, $\mathcal{S}_{i}=\frac{y_{i+1}-y}{y_{i+1}-y_{i}}, \mathcal{S}_{i+1}=\frac{y-y_{i}}{y_{i+1}-y_{i}}$

### Generalized Focal Loss(GFL)

结合上述两者，得到
$$
\begin{equation}
\mathbf{G F L}\left(p_{y_{l}}, p_{y_{r}}\right)=-\left|y-\left(y_{l} p_{y_{l}}+y_{r} p_{y_{r}}\right)\right|^{\beta}\left(\left(y_{r}-y\right) \log \left(p_{y_{l}}\right)+\left(y-y_{l}\right) \log \left(p_{y_{r}}\right)\right)
\end{equation}
$$

因为临近的两个概率都是通过softmax获得的，因此实际上也抑制了分布中的其他部分。

最终训练Loss如下 
$$
\begin{equation}
\mathcal{L}=\frac{1}{N_{\text {pos }}} \sum_{z} \mathcal{L}_{\mathcal{Q}}+\frac{1}{N_{\text {pos }}} \sum_{z} \mathbf{1}_{\left\{c_{z}^{*}>0\right\}}\left(\lambda_{0} \mathcal{L}_{\mathcal{B}}+\lambda_{1} \mathcal{L}_{\mathcal{D}}\right)
\end{equation}
$$

作者在附录中还对其他内容进行了一些讨论

### 对于文中提出Loss的极小值点的推导

这部分比较冗长，略。
### IoU 和 centerness
作者发现IoU作为质量得分的结果比centerness好，经过分析，发现centerness 经常会出现一些非常小的值，尤其是对小目标，如下所示

![](/assets/img/20211012/GFLV1F11.png)

这些太小的centerness很明显不利于网络的学习，可能使得正样本*很小的值，排在负样本之后。两种标签的分布如下，可以看到IoU通常具有更大的值。

![](/assets/img/20211012/GFLV1F12.png)

### Distributed Bounding Boxes

如下图所示，对于一些具有模糊和不确定性的边框，可以看到网络预测到的某些边实际上具有更平摊的概率分布预测，甚至是双峰的情况。
![](/assets/img/20211012/GFLV1F13.png)