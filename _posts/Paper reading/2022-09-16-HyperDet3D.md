---
layout: post
title: 'HyperDet3D: Learning a Scene-conditioned 3D Object Detector'
date: 2022-09-13
author: Poley
cover: '/assets/img/20220913/HyperDet3D.jpg'
tags: 论文阅读
---

![](/assets/img/20220913/HyperDet3DF1.jpg)

本文发表于CVPR2022，主要关注将场景与检测物体的关系引入3D检测中，形成对物体的一个先验判断。室内场景中的物体相似度较高，容易产生错误判断。但是根据物体所在的不同场景，实际上可以提供对于物体类别的很强的先验信息，即Scene-conditioned。如图所示，在洗衣房里出现Bed和Cabinet显然是不合理的，如果能结合场景信息就会更容易判断出其是洗衣机。这种信息对于3D物体是有益的，因为点云中并没有纹理信息的存在，室内场景中，具有类似的属性的物体是很容易ambiguous的（点云数据也没有颜色）。本文提出使用场景知识来消除。

本文工作主要使用的方法是超网络(hypernetworks),作者是David Ha, Andrew Dai, Quoc V. Le，此为2017年的ICLR论文

这项工作探索了超网络：一种使用一个网络（也称为超网络）为另一个网络生成权重的方法。

超网络提供了一种与自然界相似的抽象：基因型（超网络）与表型（主网络）之间的关系。 这项工作的重点是使超网络对深度卷积网络和长循环网络有效。在这些网络中，超网络可以看作是跨层共享权的轻松形式。 主要结果是超网络可以为LSTM生成非共享的权重，并在各种序列建模任务（包括字符级语言建模，手写体生成和神经机器翻译）上获得近乎SOTA的结果。

应用于卷积网络的超网络仍然可以在图像识别任务上获得可观的结果，同时所需的可学习参数更少。

具体如下图所示,即对每一层的参数，使用一个因变量，通过共享的超网络映射成主网络中的参数，实现学习：
![](/assets/img/20220913/HyperDet3DA1.jpg)


本文中，分为了两个超网络，分别是
Scene-Agnostic HyperNetwork和Scene-Specific HyperNetwork。前者学习跨场景的泛化信息，而后者学习场景相关的特化信息。

对Scene-Agnostic HyperNetwork来说，其维持一个隐变量集合
$$
\begin{equation}
\boldsymbol{Z}^a=\left\{z_j^a \in \mathbb{R}^{C_a}\right\}_{j=1}^n
\end{equation}
$$

之后通过超网络映射成主网络对应权重的维度
$$
\begin{equation}
\boldsymbol{W}^a:=\left\{w_j^a \in \mathbb{R}^{C_{u i}}\right\}_{j=1}^n, w_j^a=\boldsymbol{h}_\theta^a\left(z_j^a\right)
\end{equation}
$$

其中
$$
\begin{equation}
\bmod \left(C_{\text {out }}, n\right) \equiv 0, \quad \bmod \left(C_{\text {in }}, C_{\text {ui }}\right) \equiv 0
\end{equation}
$$
即主网络的参数尺寸需要被超网络生成的尺寸整除，以方便拓展生成参数尺寸，减小计算量。

同理，对于Scene-Specific HyperNetwork，只是在隐变量中额外加入了一个输入点云场景的特征，

$$
\begin{equation}
\begin{aligned}
\boldsymbol{W}^s &:=\left\{w_k^s \in \mathbb{R}^{C_{u i}}\right\}_{k=1}^n \\
w_k^s &=\boldsymbol{h}_\theta^s\left(z_k^s, \mathrm{P}_d^i\right)=W_f\left(z_k^s \| W_p \mathrm{P}_d^i\right)
\end{aligned}
\end{equation}
$$

最终的主网络权重又两者结合而成，如下

$$
\begin{equation}
\boldsymbol{W}^u=\boldsymbol{W}^s \odot \boldsymbol{W}^a
\end{equation}
$$

进一步的，为了捕捉到更多子空间内的信息，本文模仿多头注意力机制，提出了Multi-Head Scene-Conditioned Attention如下图所示

![](/assets/img/20220913/HyperDet3DF3.jpg)

但与多头注意力不同，多头注意力是一组输入，使用多组不同的Atten模块，而这里是多组输入，使用同一组共享网络。对同一层的参数W，使用多组隐变量Z（不同初始化）生成W的多个部分，然后串联成为最后主网络的参数，如下

$$
\begin{equation}
\boldsymbol{W}=\operatorname{Concat}\left(\boldsymbol{W}_{(1)}^u, \boldsymbol{W}_{(2)}^u, \ldots, \boldsymbol{W}_{\left(\frac{C_{\text {out }}}{n} \times \frac{C_{\text {in }}}{C_{\text {ui }}}\right)}^u\right)
\end{equation}
$$

最后，本文也提出了一个Disentangled Detection Head，个人认为和Canonical Voting一样，即以局部坐标的回归代替全局offset。再通过旋转和缩放获得最后的结果。

本文的原理虽然简单，但是实际效果却很好，实现了室内场景3D检测显著的性能提升。

![](/assets/img/20220913/HyperDet3DT2.jpg)