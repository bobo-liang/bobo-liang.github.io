---
layout: post
title: '3D-MAN: 3D Multi-frame Attention Network for Object Detection'
date: 2021-11-01
author: Poley
cover: '/assets/img/20211101/3DMAN.png'
tags: 论文阅读
---

本文是一个3D multi-frame attention network 用于有效的聚合来自于多个感知的特征并且达到了waymo上的SOTA性能。


本文首先使用一个快速的单帧检测器来得到proposals，并将其与其对应的特征一起保存到memory bank中。通过本文提出的基于attention的多视角对齐和聚合模型，可以有效地聚合来自不同场景视角的特征，并且提升检测器的性能。


## Introduction

激光雷达在一帧中注定只可以看到物体的部分视角，因此，检测可能会变成一个病态问题，因为有多种预测都可能是合理的，也就是框的不唯一性。如下图所示

![](/assets/img/20211101/3DMANF1.png)

自动驾驶场景中，传感器可以在运动过程中获得3D空间的多个视角，来提供关于一个实例的更全面信息来避免上述的问题。但是这需要提供数据序列的数据集，最近新发布的NuScenes和Waymo Open Dataset 都可以实现这一目的。

![](/assets/img/20211101/3DMANT2.png)

最简单的方法是把相邻帧的点云串联起来，这样对于静止或者低速情况下效果很好。但是如上表所示，在高速情况下效果不佳，这是由于LiDAR点在这种情况下没有对齐导致的。

另一种方法是对他们的feature map进行串联，但这一样会产生同样的misalignment的问题。

本工作的主要贡献如下：

+ 在waymo上达到sota
+ 引入一种全新的训练策略，用于训练fast signle frame detector，使用max-pooling和NMS,以及一个匈牙利算法的变体来匹配计算检测损失。
+ 设计一种多视角对其和融合模块，用来从memory bank中提取多帧的特征。


## 3D-MAN Framework
![](/assets/img/20211101/3DMANF2.png)

### Fast Single-frame Detector

#### Anchor-free Point Pillars
基于 PointPillar 和 dynamic voxelization 的单阶段检测器。回归设置中适应了Bin来应对方向预测。最终输出需要使用NMS，但是在处理大量框的时候，NMS的速度慢，作者提出了改进的MaxpoolingNMS，即在局部内寻找置信度最高（局部峰值）的作为局部的输出，并抑制其他的输出。这样速度6倍快于传统的NMS，当有200k proposal的时候。

#### Hungarian Matching
上述MaxpoolingNMS通过分类得分来选择最优的框，然而分类得分实际上是一个代理度量(proxy metric)，本质上是想要得到最好的定位框。这要求理想的score map具有一个单峰。因此作者使用匈牙利匹配算法(Hungarian matching algorithm)来产生郑一个score map。即形成一对一的匹配，一个gt box只对应一个Proposal，而其他的全部为false。

这样做还有三个问题：

+ 匈牙利算法的复杂度$O(n^3)$，对于大量框时速度很慢。这里loss只对MaxPoolNMS之后的框计算。
+ 可能导致bad local minima，当剩余的预测框距离真值很远，而匈牙利算法又一定要配对一组gt和prediction时。即gt和prediction没有overlap的情况。因此，对于没有overlap的这种情况，将gt重新分配到离其最近的pillar上(可能没有被MaxPoolNMS保留)，来避免学习到无用的信息。

### Memory bank

维护一个memory，来存储过去n帧的proposal和feature map，采取队列形式，先进先出。

本文作者发现使用memory中所有的proposal来在当前帧里提取特征具有很好的效果，而不需要考虑proposal来自于哪一帧。因为单个帧的proposal可能会由于遮挡或者部分可见性而丢失proposal，上述这么做可以有效的提高召回率。

提取特征使用的类似于grid pooling，只不过不是聚合点云/体素特征，而是直接在feature map上插值。**这样的好处是不需要对整个点云进行运动校正**。之后提取到的特征会作为query，而Memory中的特征会被作为keys和values。

### Multi-view Alignment and Aggregation

#### Multi-view Alignment
其原理如下图所示。这个网络是针对当前帧与所存储的每一帧处理的。本质就是通过一个Attention机制。其主要部分是两帧所有Proposal feature之间的attention，只不过其中进一步加入了proposal 的 box residuals以及帧差异$t-s$来做一个额外的编码，为attention加入更多的信息。最终得到proposal的特征输出。注意，这里$s$是存储帧，$t$是target。即用当前帧来校正存储帧。**其目的是从存储帧中提取出和当前帧中proposal最相关的特征**。
![](/assets/img/20211101/3DMANF4.png)

为了鼓励提取到有用的特征，在此阶段加入了一个Cross-view Loss。对上述提取到的特征使用一个单独的head来做预测和分类，并计算loss。

#### Multi-view Aggregation

同上，只不过这回进行的是proposal之间的attention，而不是frame之间的。先对之前两两alignment得到的proposal特征进行重新排列，进行proposal之间的attention。实现对不同帧特征的聚合。

## Experiments

![](/assets/img/20211101/3DMANT3.png)