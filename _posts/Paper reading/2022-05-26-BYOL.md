---
layout: post
title: 'Bootstrap Your Own Latent A New Approach to Self-Supervised Learning'
date: 2022-5-26
author: Poley
cover: '/assets/img/20220523/BYOL.png'
tags: 论文阅读
---
本文是2020年无监督（自监督）的SOTA方法，其主要解决了传统自监督方法中需要依赖大量负样本来得到非平凡解的问题，今儿允许算法在训练过程中对于Batchsize和数据增广方法更加宽容。

目前的自监督方法主要有如下集中

生成方法通过生成样本并和时机数据进行判别来探索数据的latent embedding并以此作为图像的特征编码。但是计算复杂度高，并且图像生成也不一定是表征学习所必须的。

对比学习使用同一图像的不同增强来构建正样本对，不同图像的构建负样本对。其中负样本对的选择非常关键。

deepcluster使用上一个representation进行聚类，作为下一次学习的target。避免了负样本，但是聚类过程同样成本高，并且也需要特殊的预警来避免平凡解。

本文则采取了一个类似于Mean-teacher模型的方法。Mean-teacher验证了一致性损失的有效性，其双网络结构和EMA参数传递方式与本文类似。但是缺失分类损失时（只有一致性损失），即在无监督场景下，模型同样会崩塌（平凡解）。因此本文加入一个predictor来避免这个问题。MOCO通过EMA维护一个memory bank来包含负样本对，而这里使用一个EMA的Network来直接产生预测的target，相比MOCO可以得到更加稳定的结果。


目前对比学习，主要基于cross-view prediction，即直接在特征空间中，从不同的view预测同一个图像，使其产生同样的结果。这样在不加约束的情况下，网络会生成平凡解，即对所有样本都输出相同的向量，这样可以保证永远对一组不同增广的输入图片得到相同的表征，但是这不是我们所期待的，因此其需要引入负样本对，来学习具有判别性的特征。

而本文发现，对于给定的representation，通过预测上述给定的representation的方式，可以训练一个新的，潜在的增强的representation。BYOL通过迭代上述形式，来获得一个不断强化的representation。这就是BYOL的基本思路，其结构如下所示。

![](/assets/img/20220523/BYOLF2.png)

其损失如下
$$
\begin{equation}
\mathcal{L}_{\theta, \xi} \triangleq\left\|\overline{q_{\theta}}\left(z_{\theta}\right)-\bar{z}_{\xi}^{\prime}\right\|_{2}^{2}=2-2 \cdot \frac{\left\langle q_{\theta}\left(z_{\theta}\right), z_{\xi}^{\prime}\right\rangle}{\left\|q_{\theta}\left(z_{\theta}\right)\right\|_{2} \cdot\left\|z_{\xi}^{\prime}\right\|_{2}} .
\end{equation}
$$

即online和target网络具有相同的结构，但online最后多一个prediction用于预测target的表征。两者得到的representation向量在归一化后通过L2距离衡量损失，即相当于一个没有负样本的对比损失。同时，对于输入的两组图像（原图像和增广图像），本方法将两者调换再计算一次损失，并共同用于online网络的参数更新。同时，使用EMA更新target网络。

那么问题在于，为何这样不会使得模型坍塌成平凡解？本文对其进行了分析。

本文的结构类似于gan，这里没有一个全局最小化的loss，同时关于target和online network。因此损失不会收敛到局部极小值，也就是平凡解。从理论上分析，可以假设一个理想的q，对任意z都有最小的损失，则其输出就是z的条件均值。即如下
$$
\begin{equation}
q_{\theta}=q^{\star} \text { with } q^{\star} \triangleq \underset{q}{\arg \min } \mathbb{E}\left[\left\|q\left(z_{\theta}\right)-z_{\xi}^{\prime}\right\|_{2}^{2}\right], \quad \text { where } \quad q^{\star}\left(z_{\theta}\right)=\mathbb{E}\left[z_{\xi}^{\prime} \mid z_{\theta}\right]
\end{equation}
$$

那么对其求梯度，可以得到
$$
\begin{equation}
\nabla_{\theta} \mathbb{E}\left[\left\|q^{\star}\left(z_{\theta}\right)-z_{\xi}^{\prime}\right\|_{2}^{2}\right]=\nabla_{\theta} \mathbb{E}\left[\left\|\mathbb{E}\left[z_{\xi}^{\prime} \mid z_{\theta}\right]-z_{\xi}^{\prime}\right\|_{2}^{2}\right]=\nabla_{\theta} \mathbb{E}\left[\sum_{i} \operatorname{Var}\left(z_{\xi, i}^{\prime} \mid z_{\theta}\right)\right]
\end{equation}
$$

这里会发现模型的梯度和target的条件方差成正比（理想分类器情况下）。但是，注意到，根据概率论，有
$$
\begin{equation}
\text { for any random variables } X, Y \text {, and } Z, \operatorname{Var}(X \mid Y, Z) \leq \operatorname{Var}(X \mid Y)
\end{equation}
$$

这里X在这里是target表征，Y是online表征，Z是网络引入的随机干扰。那么根据上式，完全丢弃Y和Z并不会使得方差减小，也就是说，这里的梯度与条件方差成正比，但单纯的丢失信息（条件）并不会使得方差减小，意味着网络不能通过丢失信息来减小梯度，实现收敛！很巧妙。

上述理论中，直接复制Online的参数到target是可以的，但是这样剧烈的变化可能会导致上述最优预测器的假设偏离，从而梯度不一定靠近于条件方差。因此使用一个移动平均的方法来使得预测期一直处于接近最优的状态，从而始终避免收敛到平凡解。

最终的实验结果也证实了本方法的优势，即在不需要负样本的情况下，对batchsize和增广方法更加宽容
![](/assets/img/20220523/BYOLF3.png)