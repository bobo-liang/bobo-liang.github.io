---
layout: post
title: 'MixMatch: A Holistic Approach to Semi-Supervised Learning'
date: 2022-5-05
author: Poley
cover: '/assets/img/20220505/MixMatch.png'
tags: 论文阅读
---

本文是一篇非常经典的半监督学习论文，其后续改进也被应用在目标检测领域，如focalmix等等。

半监督损失一般分为三个类别：熵最小化、一致性正则和通用正则。其中，熵最小化鼓励模型输出对无标签数据的可信的预测；一致性正则则鼓励模型对于被扰动的输入具有一致的输出分布；而通用正则则鼓励模型具有更好的泛化性以及避免对训练数据过拟合。而MixMatch这一种方法就融合上述三种方法的优点。


首先，本文对上述三种正则做了详细的介绍。

### 一致性正则

一致性正则使用数据增强来对输入数据进行扰动，并要求分类器对于被扰动的数据和原始数据具有相同的输出分布。因此，其损失函数可以表示为：
$$
\begin{equation}
\left\|\mathrm{p}_{\text {model }}(y \mid \operatorname{Augment}(x) ; \theta)-\mathrm{p}_{\text {model }}(y \mid \operatorname{Augment}(x) ; \theta)\right\|_{2}^{2}
\end{equation}
$$

这类方法的一个典型的代表是Mean-Teacher模型。MixMatch智力同样通过使用标准数据增广方法来实现这种一致性损失。

### 熵最小化
许多半监督学习方法都基于一个假设，即分类器的决策边界不应该穿越边界数据分布的高密度区域。强制满足这个假设的方法是使得分类器输出低熵预测（高置信度）。在VAT中也使用熵最小化来得到更强的结果。伪标签方法使用上最小化，从高置信度预测中构建1-hot,即硬标签，并使用标准的交叉熵损失来进行训练。

### 传统正则化

如weight decay和L2正则化等，略。

## MixMatch
MixMatch使用一份有标注数据和一份无标注数据来作为输入。其中未标注数据的标签是通过K个增强的预测结果平均“猜测”而来，表达式如下所示。
$$
\begin{equation}
\begin{aligned}
\mathcal{X}^{\prime}, \mathcal{U}^{\prime} &=\operatorname{MixMatch}(\mathcal{X}, \mathcal{U}, T, K, \alpha) \\
\mathcal{L}_{\mathcal{X}} &=\frac{1}{\left|\mathcal{X}^{\prime}\right|} \sum_{x, p \in \mathcal{X}^{\prime}} \mathrm{H}(p, \text { pmodel }(y \mid x ; \theta)) \\
\mathcal{L}_{\mathcal{U}} &=\frac{1}{L\left|\mathcal{U}^{\prime}\right|} \sum_{u, q \in \mathcal{U}^{\prime}}\left\|q-\operatorname{p}_{\text {model }}(y \mid u ; \theta)\right\|_{2}^{2} \\
\mathcal{L} &=\mathcal{L}_{\mathcal{X}}+\lambda_{\mathcal{U} \mathcal{L} \mathcal{U}}
\end{aligned}
\end{equation}
$$

注意到这里对伪标签使用的是L2损失，而没有交叉熵损失。L2损失在分类上存在的问题是当其预测接近0时，不产生损失。这意味本方法只对伪标签的正样本进行学习？

### 数据增强
对于每个标注样本，产生一个增广样本。对于每个未标注数据，产生K个增广样本，并通过预测，平均其输出作为未标注数据的标签。如下所示
$$
\begin{equation}
\bar{q}_{b}=\frac{1}{K} \sum_{k=1}^{K} \operatorname{p}_{\text {model }}\left(y \mid \hat{u}_{b, k} ; \theta\right)
\end{equation}
$$
之后，为了进一步输出低熵模型，这里对输出进行了锐化，来减小预测标签的熵。锐化的程度通过温度$T$控制。
$$
\begin{equation}
\operatorname{Sharpen}(p, T)_{i}:=p_{i}^{\frac{1}{T}} / \sum_{j=1}^{L} p_{j}^{\frac{1}{T}}
\end{equation}
$$

### Mixup
这里采用了Mixup方式来同时对无标签（伪标签）和有标签数据进行数据增强。如下所示
$$
\begin{equation}
\begin{aligned}
\lambda & \sim \operatorname{Beta}(\alpha, \alpha) \\
\lambda^{\prime} &=\max (\lambda, 1-\lambda) \\
x^{\prime} &=\lambda^{\prime} x_{1}+\left(1-\lambda^{\prime}\right) x_{2} \\
p^{\prime} &=\lambda^{\prime} p_{1}+\left(1-\lambda^{\prime}\right) p_{2}
\end{aligned}
\end{equation}
$$

这里通过max操作，确保合成后的样本更加接近$x_1$而不是$x_2$。这样做的原因是因为MixMatch将真实标注数据和伪标注数据混合在一个batch中进行训练，因此在增强过程中需要保留他们的顺序，以确保分别使用不同的Loss进行计算。显然，这需要确保原位置上混合的样本不强于其本身。


整体产生伪标签的方法如下图所示。

![](/assets/img/20220505/MixMatchF1.png)

整体算法流程如下所示。

![](/assets/img/20220505/MixMatchA1.png)