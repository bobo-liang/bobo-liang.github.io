---
layout: post
title: 'MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of
Hierarchical Vision Transformers'
date: 2023-5-31
author: Poley
cover: '/assets/img/20230530/MixMAE.jpg'
tags: 论文阅读  
---

## 背景
MIM方法使用mask代替部分输入，导致了训练的低效和预训练-微调的不一致（由于预训练遮挡率较高）。MAE没有这个问题，但是不适用于hierarchical VIT（比如swin，因为稀疏所以不好下采样）, 本文提出的方法主要是为了同时满足上述三个良好的性质。

预训练-微调不一致只Mask token只出现在预训练中；浪费计算量指对无信息的mask token进行计算。典型是SimMIM，表现为其需要较长的训练时间800e，以及有限的下游任务性能。MAE则没有上述问题，但因为随机变长的token seq length，swin等结构就无法处理。
本文提出MixMAE,使用两张图片互为遮挡，并使得输入图像分别还原出两张图像。也可以理解为，将mask token换为另一个图像的可见token

## 方法
本文的整体结构如下所示
![](/assets/img/20230530/MixMAEF1.jpg)


虽然上述mix的方法有很多好处，但是会大大增加网络收敛的难度，这里主要采取两个操作使得其更接近于单独进行SIMMIM的操作：1、加入emb，区分两个img；2、为sa打mask，使token只和同图像的交互。其中2可以加快收敛速度，其对大规模的pretrained非常重要。


本方法在多个下游任务上相比MAE和SIMMIM取得了性能优势

![](/assets/img/20230530/MixMAEF2.jpg)

同时，在收敛性和计算量上均有一些优势。

![](/assets/img/20230530/MixMAET4.jpg)