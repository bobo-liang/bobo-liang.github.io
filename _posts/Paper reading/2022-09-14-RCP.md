---
layout: post
title: 'RCP: Recurrent Closest Point for Scene Flow Estimation on 3D Point Clouds'
date: 2022-09-13
author: Poley
cover: '/assets/img/20220913/RCP.jpg'
tags: 论文阅读
---

本文发表在CVPR2022上，以一种point-wise优化的方式解决了点云光流估计和配准问题。

在3D场景下使用点云进行光流估计，相比基于2D图像的光流估计，具有时延低（2D首先需要深度补全），带宽占用小的特点（点云数据传输量小于图像）。但是，点云光流估计的难点在于，由于不规则，难以设定搜索窗口，即cost-volume（视差的密集搜索网格）。

本文方法的目的是对两幅点云之间的光流或者6-DOF刚性变换进行估计，光流估计是对单个点来说的，每个点可以有不同的运动特性，适用于动态场景。6自由度的旋转平移一般对刚性场景。首先，使用PointNet++对两幅点云进行点特征的提取，基于点特征匹配和传统匹配方法sinkhorn，可以得到光流估计的初始值。一般来说，光流估计的损失为特征距离和正则项的和，如下所示
$$
\begin{equation}
E(\mathcal{X})=D_{\mathcal{P}, \mathcal{Q}}(\mathcal{X})+R(\mathcal{X})
\end{equation}
$$

但上述优化的难点在于，由于点云的不规则化，点云+光流的位置处不一定正好对应另一幅点云中的某个点，使得特征距离无从计算，而且特征距离关于位移X也不可导。

因此，本文提出使用一个中间变量Z来decouple数据项和正则项，同时起到一个可导的特性。依次更新中心变量和光流估计，如下

$$
\begin{equation}
\left\{\begin{array}{l}
\mathcal{Z}^k=\underset{\mathcal{Z}}{\arg \min } D_{\mathcal{P}, \mathcal{Q}}(\mathcal{Z})+\left\|\mathcal{Z}-\mathcal{X}^{k-1}\right\|_2^2 \\
\mathcal{X}^k=\underset{\mathcal{X}}{\arg \min }\left\|\mathcal{Z}^k-\mathcal{X}\right\|_2^2+R(\mathcal{X})
\end{array}\right.
\end{equation}
$$

综上，本方法一共分为三部分：特征提取、Point-wise Optimization和Recurrent Regularization

## 特征提取
使用PointNet++提取点特征，略。
## Point-wise Optimization
根据上式，可以得到这一步的优化目标位
$$
\begin{equation}
\boldsymbol{z}_m^k=\underset{\boldsymbol{z}}{\arg \min }\left\|\boldsymbol{f}_{\mathcal{P}}\left(\boldsymbol{p}_m\right)-\boldsymbol{f}_{\mathcal{Q}}\left(\boldsymbol{p}_m+\boldsymbol{z}\right)\right\|+\left\|\boldsymbol{z}-\boldsymbol{x}_m^{k-1}\right\|
\end{equation}
$$
这是一个winer-take-all的目标函数，并不可导，因此对其进行软化。使用点$p_m$和其在另一幅点云中的的邻域点$q_n$之前的相对距离$u_n$作为即，通过加权来获得$z$，从而实现可导的目的。总体来说，Z相当于多个方向的加权，权重取决于对应的方向和光流估计的差距以及特征差距。如下
$$
\begin{equation}
\boldsymbol{z}_m^k=\frac{1}{W} \sum_{\boldsymbol{q}_n \in \Omega} \exp \left(-\frac{\left\|\boldsymbol{f}_{\mathcal{P}}\left(\boldsymbol{p}_m\right)-\boldsymbol{f}_{\mathcal{Q}}\left(\boldsymbol{q}_n\right)\right\|}{\sigma_{\boldsymbol{f}}}-\frac{\left\|\boldsymbol{u}_n-\boldsymbol{x}_m^{k-1}\right\|}{\sigma_u}\right) \boldsymbol{u}_n
\end{equation}
$$

进一步的，为了简化计算，将特征和位置串联，并使用内积作为相似度度量，得到如下
$$
\begin{equation}
\boldsymbol{z}_m^k=\sum_{\boldsymbol{q}_n \in \Omega} \operatorname{softmax}\left(\boldsymbol{g}_m^{\top} \boldsymbol{g}_n\right) \boldsymbol{u}_n
\end{equation}
$$
网络结构如图所示
![](/assets/img/20220913/RCPF4.jpg)

对于配准任务，需要一些额外步骤。上式中的光流，则光流可以通过前一步的旋转平移矩阵获得，迭代后的旋转平移矩阵可以通过对应点光流的PNP计算得到。

## Recurrent Regularization

这部分比较简单，即通过一个GRU网络来预测得到上述中间变量和光流X之间的残差，并用于更新X。首先，以当前的特征距离（通过插值获得）和中间变量作为输入，即$\boldsymbol{v}_k=\left[\boldsymbol{f}_{\mathcal{P}}\left(\boldsymbol{p}_m\right)-\boldsymbol{f}_{\mathcal{Q}}\left(\boldsymbol{p}_m+\boldsymbol{x}_m^{k-1}\right), \boldsymbol{z}_m^k\right]$，根据GRU的结构，可以得到更新的隐变量$h$为
![](/assets/img/20220913/RCPE8.jpg)

之后根据隐状态预测残差即可。对于光流估计，残差是光流向量，对于点云配准，残差是刚性变换矩阵。

# Training Loss
在训练损失上，分为两种
## 有监督损失
这部分很简单，在给出完整光流GT的情况下，简单的通过L1损失即可实现训练
$$
\begin{equation}
\mathcal{L}_{\text {flow }}=\frac{1}{M} \sum_i\left\|\hat{\boldsymbol{x}}_{\boldsymbol{p}_i}-\boldsymbol{x}_{\boldsymbol{p}_i}^*\right\|
\end{equation}
$$

或是配准损失

$$
\begin{equation}
\mathcal{L}_{\text {register }}=\frac{1}{M} \sum_i\left\|\left(\hat{\boldsymbol{Q}} \cdot \boldsymbol{p}_i+\hat{\boldsymbol{T}}\right)-\left(\boldsymbol{Q}^* \cdot \boldsymbol{p}_i+\boldsymbol{T}^*\right)\right\|
\end{equation}
$$

## 无监督（自监督）损失

光流GT的标注成本很高，大多数时候可能并不存在GT标注，因此本文也提出了对应的自监督损失。无监督情况下分为三部分损失：1、chamfer loss，确保配准的局部最优 2、平滑正则，局部内的光流应该近似 3、拉普拉斯正则，配准后的两幅点云局部结构应该近似。分别如下

Chanfer Loss
$$
\begin{equation}
\mathcal{L}_C=\sum_{\boldsymbol{p}_i \in \mathcal{P}^{\prime}} \min _{\boldsymbol{q}_j \in \mathcal{Q}}\left\|\boldsymbol{p}_i-\boldsymbol{q}_j\right\|_2^2+\sum_{\boldsymbol{q}_j \in \mathcal{Q}} \min _{i \in \mathcal{P}^{\prime}}\left\|\boldsymbol{q}_j-\boldsymbol{p}_i\right\|_2^2
\end{equation}
$$

Smoothness Term
$$
\begin{equation}
\mathcal{L}_S=\sum_i \frac{1}{\left|L\left(\boldsymbol{p}_i\right)\right|} \sum_{\boldsymbol{p}_j \in L\left(\boldsymbol{p}_i\right)}\left\|\hat{\boldsymbol{x}}_{\boldsymbol{p}_j}-\hat{\boldsymbol{x}}_{\boldsymbol{p}_i}\right\|_2^2,
\end{equation}
$$
Laplacian Term
$$
\begin{equation}
\delta_i=\sum_i \frac{1}{\left|L\left(\boldsymbol{p}_i\right)\right|} \sum_{\boldsymbol{p}_j \in L\left(\boldsymbol{p}_i\right)}\left(\boldsymbol{p}_j-\boldsymbol{p}_i\right)
\end{equation}
$$
$$
\begin{equation}
\mathcal{L}_R=\sum_{\boldsymbol{p}_i^{\prime} \in \mathcal{P}^{\prime}}\left\|\delta\left(\boldsymbol{p}_i^{\prime}\right)-\delta\left(\boldsymbol{q}_{i n t e r}\right)\right\|_2^2
\end{equation}
$$

最终损失为

$$
\begin{equation}
\mathcal{L}_{\text {selflow }}=\alpha_1 \mathcal{L}_C+\alpha_2 \mathcal{L}_S+\alpha_3 \mathcal{L}_R
\end{equation}
$$


