---
layout: post
title: 'Towards Domain Generalization for Multi-view 3D Object Detection in Bird-Eye-View'
date: 2023-3-11
author: Poley
cover: '/assets/img/20230311/DGBEV.jpg'
tags: 论文阅读  
---


本文关注于MV3D的跨数据表现。大多数在跨域检测的时候都会有显著的性能损失。本文对其进行了分析，认为带来domain gap的主要原因是深BEV的特征分布，主要取决于深度估计和2D图像特征表达。为了获得鲁棒的深度估计，提出将深度估计从内参中解耦出来(通过转换depth的metric为scale-invariant depth)，以及对perspective的动态争光来增加外参的diversity。同时，手动调整focal length来创建pseudo-domains，和对抗损失一起，促进特征表达的domain-agnostic。

本文以bevdepth作为baseline，如下图所示，从图里看似乎主要是径向距离的差异，也就是深度预测不准。

![](/assets/img/20230311/DGBEVF1.jpg)

由于焦距的不同，不同物体在图像中的大小差异很大。单目深度估计主要靠先验，因此内参的变化会直接影响深度估计的精度。如下图所示

![](/assets/img/20230311/DGBEVF2.jpg)

域适应的工作在2D中已经有很多，2D检测的域适应或domain generalization通常关注与光照，颜色和纹理的差异。但是其不适用于3D，3D往往更关注与准确预测物体的空间信息，在这里主要表现为对深度的估计。

## Problem Definition

这里以概率的形式定义这个问题，整个检测问题的分布可以表示为
$$
\begin{equation}
P(Y, X, K, E)=P(Y \mid X, K, E) P(X, K, E)
\end{equation}
$$
其中Y是gt box，X是图像，K是内参，E是外参。对于跨域检测问题，作者认为实际上主要的差异在于$P(X, K, E)$这个边缘分布上，这个分布主要描述图像特征和产生BEV Feature map的过程，而$P(Y \mid X, K, E)$主要是head部分，在跨域检测中并不是主要矛盾。

## DGBEV
为了减小上述边缘分布的差异，作者这里提出三个创新：
+ 与内参解耦的深度估计
+ 相机视角增广
+ domain-agnostic feature learning


### Intrinsics-Decoupled Depth Prediction
统一的内参会造成相机默认近大远小，而不考虑内参的影响。但是物体在图像中的大小与深度的关系，除了物体本身的先验信息外，主要是和相机的焦距所耦合的。因此，这里主要的目的就是将深度预测和焦距解耦。

首先，自然的想到使用不同的焦距来训练网络，防止网络产生焦距的Bias。对相机resize实际上相当于改变相机的focal length，即实现了扰动内参的目的。如下所示
$$
\begin{equation}
K=\left[\begin{array}{lll}
r_x & r_y & 1
\end{array}\right]\left[\begin{array}{ccc}
f_x & 0 & p_x \\
0 & f_y & p_y \\
0 & 0 & 1
\end{array}\right]
\end{equation}
$$

关键在于深度预测的解耦，这里作者使用一下公式获得解耦的深度
$$
\begin{equation}
\begin{gathered}
d=\frac{s}{c} \cdot d_m \\
s=\sqrt{\frac{1}{f_x^2}+\frac{1}{f_y^2}},
\end{gathered}
\end{equation}
$$
深度的scale-invatiant实际上相当于深度对focal lenght的归一化。s是世界坐标系下，像素的大小（即内参矩阵的逆得到）。c是一个参考像素大小。这样相当于将d与图像中物体的大小解耦了，使得物体的近大远小的特性和focal length无关。预测归一化的深度，再通过已知的focal length进行缩放得到metric depth。这相当于是改变真值的分布。

### Dynamic Perspective Augmentation

这部分相当于对外参的扰动。单目深度估计自然的根据相机的姿态而产生bias。因此，这里选择不断变换相机的视角来避免这种bias。但是由于pixel-wise的depth不可知，因此直接扰动是不可行的。这里使用单应性来启发式的生成多视角图像。

根据多视几何，两张图像之间的像素对应关系可以通过单应矩阵得到

$$
\begin{equation}
s p_2=H p_1,
\end{equation}
$$
单应矩阵，建立两个图像坐标之间的对应关系。其具有8个自由度，根据多视几何，其小至少4个点对来求解矩阵。

这里的扰动对象是相机相对于ego坐标系的欧拉角，分别对每个轴进行扰动如下

$$
\begin{equation}
\hat{P}_i=\left(y_i+\Delta y_i, p_i+\Delta p_i, r_i+\Delta r_i\right)
\end{equation}
$$

已知两幅图像的外参矩阵后，可以得到同一个点关键点在两幅图像中的坐标
$$
\begin{equation}
d \cdot q=K\left(\Phi(P) \cdot Q_{g t}+T\right),
\end{equation}
$$

$$
\begin{equation}
\hat{d} \cdot \hat{q}=K\left(\Phi(\hat{P}) \cdot Q_{g t}+T\right)
\end{equation}
$$
这里选择以3d gt box的角点作为关键点。手工扰动相机的外参，并根据扰动的外参得到分别在得到3d点在两个图像中的位置，从而求解单应矩阵。这里一个框取5个点。当在同一相机中保留的点超过4个时，即可求解。进而完成图像的单应变换。即
$$
\begin{equation}
\hat{q}=H q
\end{equation}
$$

### Domain-Invariant Feature Learning

MV3D中，相比一般的2D检测，特征上最重要的domain gap是由于不同内参导致的object的scale区别(来自于focal length区别)。这里选择通过对抗损失，判别特征的domain类别，从而产生domain invariant的特征。
domain的判断类别则是通过focal length分段划分得到的，而focal length可以在增广时通过resize来灵活的变换，这里成为pesudo-domain。划分方式比较简单，为常见的线性划分，如下
$$
\begin{equation}
t_i=\alpha+\frac{(\beta-\alpha) * i}{K}
\end{equation}
$$


![](/assets/img/20230311/DGBEVF4.jpg)

对于图像产生的特征，使用一个额外的辅助网络来对其进行对抗损失的训练。但是BCE损失等分类损失是不考虑类别顺序的，显然这种情况考虑顺序会更好，因此这里对BCE小做改进，使其变成一个有序的损失。如下所示
$$
\begin{equation}
\begin{gathered}
\mathcal{L}(y, l)=\sum_{k=0}^{K+1} \gamma(k, l) \log \left(P^k\right)+(1-\gamma(k, l)) \log \left(1-P^k\right), \\
\gamma(k, l)= \begin{cases}1, \quad l \leq k \\
0, \quad l>k\end{cases} \\
P^k=\frac{e^{y_{(2 k)}}}{e^{y_{(2 k)}}+e^{y_{(2 k+1)}}},
\end{gathered}
\end{equation}
$$

最终，本文的整体结构如图所示


![](/assets/img/20230311/DGBEVF3.jpg)

在性能上，其确实解决了MV3D的方法在跨域数据上基本没有性能的问题（径向偏离，如图1所示）

![](/assets/img/20230311/DGBEVT1.jpg)

![](/assets/img/20230311/DGBEVT2.jpg)