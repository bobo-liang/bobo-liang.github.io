---
layout: post
title: 'Momentum Contrast for Unsupervised Visual Representation Learning'
date: 2021-05-31
author: Poley
cover: '/assets/img/20210531/MOCO.png'
tags: 论文阅读
---

>论文链接 ： https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf



本文多次引用了参考文献[61]，应当去学习一下。
![](/assets/img/20210531/MOCOF1.png)

## Constrastive Learning as Dictionary Look-up

Contrastive loss function，设计的目的是当一个query 有唯一对应的key时，两者尽可能相似时，损失小，反之，则损失大（即和其他keys相似则视为负样本）。一种常用的是 **InfoNCE** 如下

$$
\begin{equation}
\mathcal{L}_{q}=-\log \frac{\exp \left(q \cdot k_{+} / \tau\right)}{\sum_{i=0}^{K} \exp \left(q \cdot k_{i} / \tau\right)}
\end{equation}
$$

Contrastive loss mechanisms的几种形式如下

![](/assets/img/20210531/MOCOF2.png)


## Momentum Contrast

对比学习是一种对高维连续输入建立离散字典的方法,比如图像输入。这个字典是动态的，其中的keys是随机采样的，并且keys encoder 参与学习。我们的假设是一个好的feature可以被从一个包含丰富负样本的大字典中学习到，同时这个字典keys的encoder是尽可能的保持一致性（consistent），除了evolution。基于这个冬季，提出了**Momentum Contrast as described**。

### Dictionary as a queue

本文提出的方法的核心是以**一个data samples队列来维持字典**，这使得我们可以重复利用不久前的Mini-batch中编码的keys，同时还可以将字典大小与mini-batch size解耦，变为一个超参数，可以人为控制，可以远大于典型的Mini-batch size。

字典里的Samples是逐步的呗替换的，新来的Mini-batch执行入队操作，然后最老的mini-batch对应的出队，被移除出字典。这样字典永远表示一个全部数据的采样子集，同时，维护字典所需要的额外计算量也是可以接受的。移除最老的mini-batch是合理的，因为他是最老的，所以他和最新的具有最差的一致性（所以被移除）。

### Momentum update
使用队列可以使得字典更大，但是同时它使得key enchoder不能反向传播，因为梯度应该像队列中的所有samples传播。一个简单的解决方法是copy query的encoder，而不对key encoder进行反向传播。但是这种方法的实验结果很差，**作者猜想是由于encoder的剧烈变化导致 key representations'的一致性被破坏**。因此提出了一种动量更新方法来解决这个问题。

使用query encoder的参数+动量来更新key encoder的参数，而不是直接更新，避免了参数的剧烈变化。特征表达的一致性非常重要，因此参数需要缓慢变化，较大的动量，比如0.999，相比较小的动量，比如0.9,有更好的效果。

$$
\begin{equation}
\theta_{\mathrm{k}} \leftarrow m \theta_{\mathrm{k}}+(1-m) \theta_{\mathrm{q}}
\end{equation}
$$

### Realtions to previous mechanisms

Figure2中体现了集中对比学习的对比。

1. **The end-to-end update**，使用当前mini-batch中的数据作为dictionary。但是大小收到Mini-batch size的限制，也就是GPU显存的限制。其也受到**Large mini-batch optimization**的挑战。

2. **Memory bank**,存储了数据集中所有sample的特征表示。对于每个mini-batch所用的字典是从其中随机采样的，并且没有梯度，所以可以支持很大的字典大小。但是**Memory bank**中的特征表达不能及时更新（**was updated when is was last seen**）。所以sampled keys根本上是关于过去的几个epoch的编码器，因此一致性较差。之前提出了一种动量更新方法，但是更新的是同一sample的representation,而不是encoder。

## Pretrxt Task
这里基本上使用了参考文献[61]中的设置，不再重复。

### Shuffling BN

本文作者使用的Backbone是ResNet50，其中包含了BN。作者发现BN阻止了模型学习到好的特征表示。**这可能是由于intra-batch communication among samples（caused by BN） leaks information**。

因此本文通过使用shuffling BN来解决这个问题，对于每个GPUs上的数据独立的计算BN。对于key encoder，先shuffle sample order，然后在encoder之后再变回来，**这样保证了batch的统计特性，用于计算query和positive key的来自于两个不同的子集**。这样有效的解决了BN带来的cheatting issue并且允许训练从BN中受益。

## Experiments

略