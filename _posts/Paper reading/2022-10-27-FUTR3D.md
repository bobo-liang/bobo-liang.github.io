---
layout: post
title: 'FUTR3D: A Unified Sensor Fusion Framework for 3D Detection'
date: 2022-10-27
author: Poley
cover: '/assets/img/20221025/FUTR.jpg'
tags: 论文阅读
---

本文主要是基于DETR3D做的一个多模态改进，主要创新，一个query-based Modality-Agnostic Feature Sampler。通过这个模块可以应对多种不同的传感器配置，比如camera + lidar + radar。如下图所示
![](/assets/img/20221025/FUTR3DF1.jpg)

不同级别不同任务的自动驾驶具有不同的传感器配置。需要一种unified融合框架。本方法类似于DETR3D，但是提出的query方法是可以跨模态的（DETR3D是在image中索引。）

![](/assets/img/20221025/FUTR3DF2.jpg)

本文的思路也是比较简单的，主要分4阶段：
1、单模态encoder
2、query-based modality Agnostic feature sampler 也是本文的主要创新
3、共享的transformer decoder head
4、 set to set loss

这里主要介绍一下其模态融合方法。思路基本和DETR3D一致
首先，对于query，通过MLP得到reference point
$$
\begin{equation}
c_i=\Phi_{\mathrm{ref}}\left(q_i\right)
\end{equation}
$$

对于lidar特征的提取，这里直接将reference point投影到BEV平面上的连续最表，通过lidar backbone得到的bev feature map做双线性插值得到，如下

$$
\begin{equation}
\mathcal{S F}_{\text {lid }}^i=\sum_{j=1}^M \mathcal{F}_{\text {lid }}^j\left(\mathcal{P}\left(c_i\right)\right) \cdot \sigma_{\text {lid }}^{i j}
\end{equation}  
$$

对于radar模态则更简单，radar模态一般一帧中只有两三百个点，太稀疏。这里直接加权求和reference附近的topK近邻

$$
\begin{equation}
\mathcal{S} \mathcal{F}_{\mathrm{rad}}^i=\sum_{j=1}^K \mathcal{F}_{\mathrm{rad}}^j \cdot \sigma_{\mathrm{rad}}^{i j}
\end{equation}
$$

对于camera部分，则和DETR3D一样，投影到image的feature map上做双线性插值得到
$$
\begin{equation}
\mathcal{S F}_{\mathrm{cam}}^i=\sum_{o=1}^O \sum_{j=1}^M \mathcal{F}_{\mathrm{cam}}^{o j}\left(\mathcal{T}_o\left(c_i\right)\right) \cdot \sigma_{\mathrm{cam}}^{i j o}
\end{equation}
$$

之后，加上位置编码作为query的更新量，并通过self-attention进一步更新query并回归Bounding box，如下

$$
\begin{equation}
\Delta q_i=\mathcal{S F}_{\text {fus }}^i+\operatorname{PE}\left(c_i\right)
\end{equation}
$$

$$
\begin{equation}
Q=Q+\operatorname{SelfAttn}(Q)
\end{equation}
$$

## Experiments

![](/assets/img/20221025/FUTR3DT1.jpg)