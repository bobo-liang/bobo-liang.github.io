---
layout: post
title: 'Dense Learning based Semi-Supervised Object Detection'
date: 2022-06-22
author: Poley
cover: '/assets/img/20220622/DSL.png'
tags: 论文阅读
---

本文主要针对FCOS等Anchor-free的检测器，其相比传统的Anchor-based检测器，由于需要pixel-level的label，因此对于伪标签中的噪声更加敏感，对于伪标签的质量提出了更高的需求。本文作者也主要围绕这提高pixel-level的伪标签质量来提出创新。
本文三个创新，自适应过滤策略分配多尺度，密集的像素级伪标签，这一步用于动态过滤掉部分真假难辨的伪标签；聚合Teacher来产生更稳定和准确的伪标签，这一步主要是用于筛选具有高置信度的FP伪标签；不确定性一致性正则在不同scale和shuffled patches之间来提高泛化性。

本文方法的整体结构如下
![](/assets/img/20220622/DSLF2.png)

## Adaptive Filtering Strategy

作者首先对在10%有监督训练下的FCOS的预测标签进行了分析，结果如下图所示。
![](/assets/img/20220622/DSLF3.png)

可以看到，随着分数阈值的上升，BG数量指数减少，TP-数量也大幅减少，标签质量明显上升。但是注意到，也有相当数量的TP+和TP-处于低阈值区域，和大量的BG混合在一起。如果使用太高的分数阈值对这些样本进行切割，虽然可以保证伪标签的正确率，但是也会遗漏大量的TP。FCOS通过引入像素级的label摆脱了对于anchor的依赖，但是这导致其对于伪标签的要求更高。因为不管多好的检测器都难免包含噪声，因此像素级的监督应该被谨慎对待。注意到，和Anchor-based方法不同，FCOS为代表的Anchor-free方法在训练过程中使用Pixel-level，意味着特征图上的每一个位置都直接受到监督并参与梯度传播，因此遗漏大量的TP标签或者引入大量的BG标签都会对降低检测器的性能。这里作者提出的方法是使用一个缓冲带，来忽略中间部分TP和BG混杂严重的部分。如下所示

$$
\begin{equation}
\bar{p}_{h, w}^{*}= \begin{cases}\text { Foreground }:[0, \cdots, C-1] & p_{h, w}>=\tau_{2} \\ \text { Ignorable Region }:[-1] & \tau_{1}<p_{h, w}<\tau_{2}, \\ \text { Background }:[C] & p_{h, w}<=\tau_{1}\end{cases}
\end{equation}
$$

这里，关键在于阈值的确定，这里作者使用了一个固定的下限阈值0.1和参考阈值0.35，之后根据帧对像素标签的预测情况动态调整每一类的阈值标签，如下
$$
\begin{equation}
\tau_{2}^{k}=\left(\frac{\sum_{h, w} \mathbb{1}_{\left\{\bar{p}_{h, w}^{*}==k\right\}} p_{h, w}}{N_{p o s}}\right)^{\beta} \tau
\end{equation}
$$

## MetaNet
上一步，通过将不确定度高的标签设置为ignore，减小了错误标签可能对pixel-level 监督带来的不利影响。但是注意到，仍然有一些FP是具有很高置信度的，如下所示
![](/assets/img/20220622/DSLF4.png)

这里作者提出使用一个单独的MetaNet（实际上就是一个编码器），来对unlabel instance进行编码。这个MetaNet使用labeled instance进行训练，为每个类别生成一个向量（相当于特征空间内的聚类中心），对于unlabel instance，使用相同的网络编码，并判断其和类别中心的距离，如果过大则舍弃。这类似于对比学习中的思路。通过这样可以对FP进行过滤，进一步提高标签质量。

## Aggregated Teacher
这里作者认为，虽然传统的EMA非常有用，但是其对于每一层的参数单独传播，可能会破坏层参数之间的相关性 ，因此这里作者通过类似于RNN的结构，引入层间的相关性隐状态，并同样使用EMA进行传递，来实现相关性的保留，如下图所示。
![](/assets/img/20220622/DSLF5.png)

在这种设计下，网络的输入输出和隐状态的关系如下
$$
\begin{equation}
\begin{aligned}
x_{l+1} &=\theta_{l+1}\left[x_{l}+h_{l}\right]+x_{l} \\
h_{l+1} &=g_{2}\left[g_{1}\left[\theta_{l+1}\left[x_{l}+h_{l}\right]\right]+h_{l}\right]
\end{aligned}
\end{equation}
$$

RNN的隐藏状态来自于上一个输入，而这里的隐藏状态来自于上一层的参数。这也使得其不会像RNN一样因为输入的顺序变化而产生不同的输出。

## Uncertainty Consistency
这一步的操作独立与上面两个操作，主要通过patch shuffle用于降低网络对于context信息的依赖，提高网络的鲁棒性；同时使用一个降采样的输入，来对网络不同scale上的上的得分进行一致性正则（得分即代表不确定度，即uncertainty consistency）。如下图所示。

![](/assets/img/20220622/DSLF6.png)

具体方法如下

![](/assets/img/20220622/DSLA1.png)

最终，各模块的效用如下所示
![](/assets/img/20220622/DSLT4.png)