---
layout: post
title: 'LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving'
date: 2021-11-11
author: Poley
cover: '/assets/img/20211110/Lasernet.png'
tags: 论文阅读
---

本文提出一种方法，使用全卷积网络为每个点预测一个3Dbox的多模态分布，单后高效的融合这些分布来产生一个物体的预测。

![](/assets/img/20211110/LasernetF1.png)

range image是从激光雷达视角出发得到的扫描线，最符合激光雷达的数据采集原理。一般来说，range image上得到的图像是相对dense的，因为这里的图像实际上是在球坐标系中的，投影到笛卡尔坐标系之后就会变成3D中的稀疏点云。

RV和BEV视角在信息上各有优劣。BEV视角下，数据是稀疏的，但是物体的尺寸会被准确的记录，而不受到距离的影响。这种一致性带来了很强的预测先验性，使得问题更容易去解决。相比之下，RV中物体大小随距离而改变，但是有dense data，并且可以保留物体的遮挡信息。

![](/assets/img/20211110/LasernetF2.png)


如上图所示，输入使用64线，垂直分辨率$0.4^\circ$，水平分辨率$0.2^\circ$的range image。每个点包含的信息有 range $r$, reflectance $e$, azimuth $\theta$, laser id $m$。

网络上，使用了deep layer aggregation来融合多层特征，如下
![](/assets/img/20211110/LasernetF3.png)

各模块内容如下

![](/assets/img/20211110/LasernetF4.png)

在预测目标上，和一般的目标检测略有不同，这里回归的是基于range image的相对坐标和旋转。预测box中心的方向的绝对坐标和绝对方向如下
$$
\begin{equation}
\begin{aligned}
\boldsymbol{b}_{c} &=[x, y]^{T}+\boldsymbol{R}_{\theta}\left[d_{x}, d_{y}\right]^{T} \\
\phi &=\theta+\operatorname{atan} 2\left(\omega_{y}, \omega_{x}\right)
\end{aligned}
\end{equation}
$$

进一步得到BEV四个角上的坐标信息
$$
\begin{equation}
\begin{array}{ll}
\boldsymbol{b}_{1}=\boldsymbol{b}_{c}+\frac{1}{2} \boldsymbol{R}_{\phi}[l, w]^{T} & \boldsymbol{b}_{2}=\boldsymbol{b}_{c}+\frac{1}{2} \boldsymbol{R}_{\phi}[l,-w]^{T} \\
\boldsymbol{b}_{3}=\boldsymbol{b}_{c}+\frac{1}{2} \boldsymbol{R}_{\phi}[-l,-w]^{T} & \boldsymbol{b}_{4}=\boldsymbol{b}_{c}+\frac{1}{2} \boldsymbol{R}_{\phi}[-l, w]^{T}
\end{array}
\end{equation}
$$

为了预测一个概率分布，这里假设x,y轴具有一致的方差，并且目标框的4个角共享这个方差。之后预测标准差的对数$s=\log \sigma$。

之后，对于点的预测进行 Mean shift聚类，来得到最后的框。由于点的坐标是连续的，这里为了加快速度，首先将空间以$\Delta x， \Delta y$，之后先将点回归的目标中心划归到bin中，再聚类，如下
$$
\begin{equation}
\begin{gathered}
\boldsymbol{m}_{i} \leftarrow \frac{\sum_{j \in i \cup N(i)} K_{i, j}\left(\boldsymbol{m}_{j} \cdot\left|S_{j}\right|\right)}{\sum_{j \in i \cup N(i)} K_{i, j}\left|S_{j}\right|} \\
K_{i, j}=\exp \left(-\frac{\left\|\boldsymbol{m}_{i}-\boldsymbol{m}_{j}\right\|^{2}}{\Delta x^{2}+\Delta y^{2}}\right)
\end{gathered}
\end{equation}
$$

之后，如果两个聚类中心跑到一个Bin中，则合并两个聚类中心

$$
\begin{equation}
\boldsymbol{m}_{j} \leftarrow \frac{\boldsymbol{m}_{i} \cdot\left|S_{i}\right|+\boldsymbol{m}_{j} \cdot\left|S_{j}\right|}{\left|S_{i}\right|+\left|S_{j}\right|} \quad S_{j} \leftarrow S_{i} \cup S_{j}
\end{equation}
$$

上述操作可以通过gpu并行实现，速度较快。最后，聚类内的点平均得到预测框的参数
$$
\begin{equation}
\hat{\boldsymbol{b}}_{i}=\frac{\sum_{j \in S_{i}} w_{j} \boldsymbol{b}_{j}}{\sum_{j \in S_{i}} w_{j}} \quad \hat{\sigma}_{i}^{2}=\left(\sum_{j \in S_{i}} \frac{1}{\sigma_{j}^{2}}\right)^{-1}
\end{equation} 
$$

其中 $w=1/\sigma^2$。实验中，作者划分的$\Delta x， \Delta y=0.5$

作者在后处理上提出了自适应的NMS，思路比较新颖。这里作者是为了解决本身就靠的很近的两个目标，由于预测的不准确而产生了交集，而BEV视图一般是不允许目标框之间有交集的，此时，两个框会被消除一个，从而丢失了一个目标。

![](/assets/img/20211110/LasernetF5.png)

这里灵活的判断是否进行消除，即通过两个焦点预测的方差来判断两个框可能的最大重叠概率,如上图所示，得到的动态iou阈值如下
$$
\begin{equation}
t=\left\{\begin{array}{cl}
\frac{\sigma_{1}+\sigma_{2}}{2 w-\sigma_{1}-\sigma_{2}} & \sigma_{1}+\sigma_{2}<w \\
1 & \text { otherwise }
\end{array}\right.
\end{equation}
$$
