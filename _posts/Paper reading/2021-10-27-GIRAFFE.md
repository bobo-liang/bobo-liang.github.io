---
layout: post
title: 'GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields'
date: 2021-10-27
author: Poley
cover: '/assets/img/20211027/GIRAFFE.png'
tags: 论文阅读
---

本文关键思想是在生成模型中利用一种复合的3D场景表示来实现更加可控的图像合成，并将这种方法与neural rendering pipeline来得到快速和真实的图像合成模型。

> 参考博客: https://blog.csdn.net/qq_43420530/article/details/119611234

目前很多生成模型都在尽力进行disentanglement，即单独控制感兴趣的熟悉感，比如物体大小，形状，姿态，而不改变其他的属性。许多方法都在2D域中进行，忽略了世界本来是3D。这通常导致了entangled representations以及控制方法并非模型的Build-in方法。这导致对于属性的控制实际上需要在后验方法中在隐空间中探索。

本方法和其他方法的对比如下
![](/assets/img/20211027/GIRAFFEF1.png)

## Related Work

GAN-based Image Synthtsis: 很多之前的工作都没有显式的模型来表达场景的复合型。 近年来的工作研究了如何在object-level控制合成过程。但是大多都在2D中建模image formation process，忽略了3D在空间的结构。本工作直接在3D空间中建模图像形成过程来更好的disentanglement和控制合成。

Implicit Functions: 使用隐函数来白哦正3D几何特征已经被应用的非常广泛。Neural Radiance Fields (NeRFs) 使用了隐神经网络以及体积渲染来生成复杂场景的新视角合成图像。但是其缺点在于不能生成新场景。本文的方法弥补了这一点。

3D-Aware Image Synthesis: 许多工作研究3D表征如何可以被作为归纳偏置呗利用到审查各行模型中，许多工作都需要使用额外的监督，但是本文中关注与如何使用raw image collection来完成。Schwarz et.al 提出了 Generative Neural Radiances Fields (GRAF)，并且实现了可控的高分辨率图像合成。只是这种表示呗限制为对于单目标场景。

# Method
## Objects as Neural Feature Field
### Neural Radiance Field

辐射场就是一个连续函数，将3D点$x \in \mathbb{R}^3$和一个视角方向$d \in \mathbb{S}^2$ 映射到一个体积密度$\sigma \in \mathbb{R}^+$以及一个RBG颜色值$c \in \mathbb{R}^3$。

并且使用一个预定义的位置编码来对$x，d$的每个元素进行编码。

$$
\begin{gathered}
\gamma(t, L)= \\
\left(\sin \left(2^{0} t \pi\right), \cos \left(2^{0} t \pi\right), \ldots, \sin \left(2^{L} t \pi\right), \cos \left(2^{L} t \pi\right)\right)
\end{gathered}
$$

使用隐模型来学习这个映射，可以通过MLP来实现：

$$
\begin{aligned}
f_{\theta}: \mathbb{R}^{L_{\mathbf{x}}} \times \mathbb{R}^{L_{\mathbf{d}}} & \rightarrow \mathbb{R}^{+} \times \mathbb{R}^{3} \\
&(\gamma(\mathbf{x}), \gamma(\mathbf{d})) & \mapsto(\sigma, \mathbf{c})
\end{aligned}
$$

其示意图如下
![](/assets/img/20211027/GIRAFFEAF1.png)

其思路主要有如下三点：
+ 空间中每个确定点，其对应的体积密度是确定的，但是RGB不是，取决于观察角度和光照；
+ 先将输入的$(x,y,z)$转换成 \sigmaσ和中间特征，之后再把中间特征和d dd结合得到点在某个视线方向下的颜色值；
+ 一个场景仅用一个模型表示。
  
体渲染公式，针对每根光线，将采样点三维的RGB与一维的体密度映射到渲染图像像素三维的RGB，需要进行N次操作，N就代表要渲染出图像的像素数
$$
\begin{aligned}
&\pi:\left(\mathbb{R}^{3} \times \mathbb{R}^{+}\right)^{N} \rightarrow \mathbb{R}^{3} \quad\left\{\left(\mathbf{c}_{r}^{i}, \sigma_{r}^{i}\right)\right\} \mapsto \mathbf{c}_{r} \\
&\mathbf{c}_{r}=\sum_{i=1}^{N} T_{r}^{i} \alpha_{r}^{i} \mathbf{c}_{r}^{i} \quad T_{r}^{i}=\prod_{j=1}^{i-1}\left(1-\alpha_{r}^{j}\right) \quad \alpha_{r}^{i}=1-\exp \left(-\sigma_{r}^{i} \delta_{r}^{i}\right)
\end{aligned}
$$
### Generative Neural Feature Fields

在NeRF学习单个场景的多视角图像时，GRAF被从无姿态的图像合集中训练，来学习NeRF的隐空间。其给NeRF加了两个编码条件$z_s,z_a\sim \mathcal{N}(0,I)$，分别控制形状和appearance。

$$
\begin{aligned}
g_{\theta}: \mathbb{R}^{L_{\mathbf{x}}} \times \mathbb{R}^{L_{\mathbf{d}}} \times \mathbb{R}^{M_{s}} \times \mathbb{R}^{M_{a}} & \rightarrow \mathbb{R}^{+} \times \mathbb{R}^{3} \\
\left(\gamma(\mathbf{x}), \gamma(\mathbf{d}), \mathbf{z}_{s}, \mathbf{z}_{a}\right) & \mapsto(\sigma, \mathbf{c})
\end{aligned}
$$

本文替换了GRAF中的三维颜色输出，改为特征输出，用来更好的与volume and neural rendering结合。

$$
\begin{aligned}
h_{\theta}: \mathbb{R}^{L_{\mathbf{x}}} \times \mathbb{R}^{L_{\mathbf{d}}} \times \mathbb{R}^{M_{s}} \times \mathbb{R}^{M_{a}} & \rightarrow \mathbb{R}^{+} \times \mathbb{R}^{M_{f}} \\
\left(\gamma(\mathbf{x}), \gamma(\mathbf{d}), \mathbf{z}_{s}, \mathbf{z}_{a}\right) & \mapsto(\sigma, \mathbf{f})
\end{aligned}
$$

### Object Representation
NeRF和GRAF的一个关键在于整个场景呗表示为单个模型。当需要disentangling场景中不同的成分时，就需要单独控制每个物体的姿态，形状和外观（这里背景也被视为一个物体）。

这里，通过结合单独的特征域以及仿射变换来表示每个物体。仿射变换如下，包括scale,rotation和tranlation。
$$
\mathbf{T}=\{\mathbf{s}, \mathbf{t}, \mathbf{R}\}
$$

通过仿射变换，可以将一个点从物体空间转换到场景空间

$$
k(\mathbf{x})=\mathbf{R} \cdot\left[\begin{array}{ccc}
s_{1} & & \\
& s_{2} & \\
& & s_{3}
\end{array}\right] \cdot \mathbf{x}+\mathbf{t}
$$

因为要表示一个物体，所以在GRAF里的射线采样点坐标系不仅可以看做局部坐标，也可以看做全局坐标；但是GIRAFFE因为牵扯到场景组合的问题，在生成模型生成一条光线打到不同物体上的时候，采样点使用的是全局坐标，而我们为了将来评估每个GRAF表示物体的时候，需要采样点的局部坐标，因此还需要进行一次仿射变换。

故我们在场景空间中（也就是全局坐标）进行体积渲染，并在其标准对象空间（也就是局部坐标）中评估特征场

$$
(\sigma, \mathbf{f})=h_{\theta}\left(\gamma\left(k^{-1}(\mathbf{x})\right), \gamma\left(k^{-1}(\mathbf{d})\right), \mathbf{z}_{s}, \mathbf{z}_{a}\right)
$$

## Scene Compositions

![](/assets/img/20211027/GIRAFFEF3.png)

这里对每个数据集内使用固定的N，即包含N-1个物体和一个背景。对于不同的数据集，N是会变动。这里背景和物体的表示没有区别，除了背景的scale 和 transation是固定的以外。

### Composition Operator

对于多个物体，每个物体都具有自己的体积密度和颜色。这里在复合时，对体积密度进行求和，并用体积密度作为权重对rgb颜色进行加权，符合常规的认知。

$$
C(\mathbf{x}, \mathbf{d})=\left(\sigma, \frac{1}{\sigma} \sum_{i=1}^{N} \sigma_{i} \mathbf{f}_{i}\right), \text { where } \quad \sigma=\sum_{i=1}^{N} \sigma_{i}
$$

## Scene Rendering

3D Volume Rendering：根据每条光纤上采样点的体密度和颜色特征，渲染出颜色特征图，颜色特征图并不是最后的RGB输出，这里只渲染场景到一个低分辨率上，用于节省时间和计算。

$$
\pi_{\mathrm{vol}}:\left(\mathbb{R}^{+} \times \mathbb{R}^{M_{f}}\right)^{N_{s}} \rightarrow \mathbb{R}^{M_{f}}, \quad\left\{\sigma_{j}, \mathbf{f}_{j}\right\}_{j=1}^{N_{s}} \mapsto \mathbf{f}
$$
$$
\mathbf{f}=\sum_{j=1}^{N_{s}} \tau_{j} \alpha_{j} \mathbf{f}_{j} \quad \tau_{j}=\prod_{k=1}^{j-1}\left(1-\alpha_{k}\right) \quad \alpha_{j}=1-e^{-\sigma_{j} \delta_{j}}
$$

2D Neural Rendering: 将上述颜色特征图渲染成高分辨率的RGB图像。

$$
\pi_{\theta}^{\text {neural }}: \mathbb{R}^{H_{V} \times W_{V} \times M_{f}} \rightarrow \mathbb{R}^{H \times W \times 3}
$$

具体结构如下
![](/assets/img/20211027/GIRAFFEF4.png)

## Training

使用一个非饱和GAN目标函数以及$R_1$梯度惩罚

$$
\begin{aligned}
&\mathcal{V}(\theta, \phi)= \\
&\mathbb{E}_{\mathbf{z}_{s}^{i}, \mathbf{z}_{a}^{i} \sim \mathcal{N}, \boldsymbol{\xi} \sim p_{\xi}, \mathbf{T}_{i} \sim p_{T}\left[f\left(D_{\phi}\left(G_{\theta}\left(\left\{\mathbf{z}_{s}^{i}, \mathbf{z}_{a}^{i}, \mathbf{T}_{i}\right\}_{i}, \boldsymbol{\xi}\right)\right)\right]\right.} \\
{+\mathbb{E}_{\mathbf{I} \sim p_{\mathcal{D}}}\left[f\left(-D_{\phi}(\mathbf{I})\right)-\lambda\left\|\nabla D_{\phi}(\mathbf{I})\right\|^{2}\right]}
\end{aligned}
$$
where $f(t)=-\log (1+\exp (-t)), \lambda=10$, and $p_{\mathcal{D}}$ indicates the data distribution.

## Experiment

![](/assets/img/20211027/GIRAFFEF7.png)