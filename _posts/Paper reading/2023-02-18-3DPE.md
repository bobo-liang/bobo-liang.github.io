---
layout: post
title: '3D Point Positional Encoding for
Multi-Camera 3D Object Detection Transformers'
date: 2023-2-18
author: Poley
cover: '/assets/img/20230218/3DPE.jpg'
tags: 论文阅读  
---

本文在网络上的创新并不显著，但是其分析部分是值得学习的。总的来说，本文分析了，分析了3D PE（来自于PETR）有用的原因，在模型结构上没啥大改动，主要就是把bev depth和PETR进行了一下结合。

PETR中对相机的编码方式如下图(a)所示
![](/assets/img/20230218/3DPEF1.jpg)

由于无法知道确切的真实深度，因此实际上PETR中是对整个camera ray做的Position embedding，从效果来看，这样取得了非常好的效果。而(b)中则是本文中使用的3D point PE(实际上就是BEVDepth)，为每个Pixel预测一个准确的深度。


对cam的3DPE中，作者认为camera ray方向是其成功的关键。但是射线防线并不能准确的对齐2D和3D特征。这里作者认为3D camera-ray PE是不如3E point PE的。因此，这里使用一个确定的深度为camera feature获得3DPE。这里的深度通过一个简单的单目深度估计head获得。


这里作者进行了消融实验，分别改变PETR中每个像素的深度数量、范围和分布。实验表明，其对PETR的Map影响在0.4%范围内，即比较小。这说明可能对于camera ray的方向的编码才是最重要的，而不是具体的3D位置。进一步提出，可以使用射线上的任意两点来代表一个camera ray。即最后一行。

![](/assets/img/20230218/3DPET1.jpg)


当只是用一个点表示camera ray的时候，其实际上相当于表达了一个lidar ray（因为3D点在3D坐标系内，也就是lidar坐标系内，其和原点的连线就是lidar ray，如下图所示）。使用单点做3DPE时，d大于15时才能得到一个相对饱和的性能（PETR code中原版用的是20m,和他的结论基本一致。）通过数学计算，验证了这一点，即当距离足够大时，lidar ray的指向和camera ray非常接近。这使得其没有基本没有性能损失。也就证明，camera ray, 是3dpe中包含的最关键信息，而具体的深度影响不大。


![](/assets/img/20230218/3DPEF2.jpg)

为了获取准确的深度，这里使用lidar ray代替cam ray（在足够的距离上），并将其投影到range view上，从Lidar获取准确的距离。具体的方式是将Lidar投影到range view上，获得稀疏的深度图，之后通过深度补全方法，获得稠密的深度图，这样可以对range view上的任一点赋予深度。这样即可以cam ray上某一点的3D深度获取，并用于cam feature的3DPE。综上，本方法的整体结构如下图所示

![](/assets/img/20230218/3DPEF4.jpg)

其中的Depth预测模块基本参考了bevdepth，创新不多

![](/assets/img/20230218/3DPEF5.jpg)

最后从结论上，也基本和Bevdepth一样，深度监督带来约3个点的性能提升，比较显著。所以笔者认为本文的主要意义是分析了3DPE有效的原因，并指出先验深度的合理取值范围。

![](/assets/img/20230218/3DPET4.jpg)