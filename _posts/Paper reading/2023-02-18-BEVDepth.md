---
layout: post
title: 'BEVDepth: Acquisition of Reliable Depth for Multi-view 3D Object Detection'
date: 2023-2-18
author: Poley
cover: '/assets/img/20230218/BEVDepth.jpg'
tags: 论文阅读  
---


本文的主要创新在于，在基于bev的camera检测中引入显示的深度预测监督，并提出一种quick view-transformer operation。

本文首先对bevdet中对camera的无监督深度预测做了定量分析，发现其实际上严重偏离了真实的深度。如下图所示
![](/assets/img/20230218/BEVDepthF1.jpg)

回顾已有的方法，可以得出结论：
1、最终的检测损失会简介的促使网络纠正对于深度的预测，建立一个隐式的深度监督。但是由于网络的表征非常复杂，因此网络准确的输出depth-aware的特征仍然非常困难。
2、depth自网络应该理论上关注相机信息（内外参）来给出更准确的深度估计，但现有方法都忽略了这一点。
3、depth-based方法一般比depth-free方法耕半，这是由于view-transformer效率太低导致的。

本文提出的结构如下所示
![](/assets/img/20230218/BEVDepthF2.jpg)

本文的改动是基于BEVDet的。在BEVDet中，来自图像的深度是隐式监督的，即对于图像每个Pixel的深度预测，并没有准确的深度gt以及对应的损失。但是，只从检测Loss来为深度估计提供监督是不够的。

本文提出使用lidar为Img的深度估计提供监督。lidar投影到图像上一般是一个sparse的dense map，为了和图像建立一一的对应关系，一般的方法是使用一个额外的深度补全网络。但是这样使得训练更加复杂，并需要额外的gt标注。这里作者选择点投影到view上，对每个grid进行min pooling，并以one hot进行深度表示。使用二值交叉熵来作为Loss。从而避免了上述进行深度补全的必要。

在深度网络的设计上，由于自车运动和时间同步等，可能导致深度gt出现误差。相比调整gt，这里选择通过增加depth net的感受野来改善这一点。方法是使用多个残差网络，以及deformable conv来增加网络的感受野，如下图所示

![](/assets/img/20230218/BEVDepthF3.jpg)

其中，使用网络来编码相机内外参数，隐式介入深度估计。这样的好处是和head脱离，不会因为head的改变而需要改变。

最后，在实验结果上，相比baseline，bevdepth具有显著的性能提升，并且并网络规模相当。唯一的区别是Bevdepth需要额外的Lidar进行监督，但是这在自动驾驶数据集中是非常容易提供的。

![](/assets/img/20230218/BEVDepthT1.jpg)

同时，本方法在引入时序上也非常简单。由于对图像上每个点都给出先显式且唯一的深度预测，因此可以形成伪点云。点云很容易进行自车补偿（通过刚性变换），因此可以简单的通过叠加来引入多帧的camera信息，这是之前的camera方法所不具备的。即，本方法对于多视图和多帧都具有很好的兼容性。

![](/assets/img/20230218/BEVDepthT3.jpg)