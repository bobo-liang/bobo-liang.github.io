---
layout: post
title: 'UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction'
date: 2021-11-24
author: Poley
cover: '/assets/img/20211122/UMAP.png'
tags: 论文阅读
---

UMAP (Uniform Manifold Approximation and Projection)是一种用于降维的流形学习方法，其理论基于黎曼几何和代数拓扑。相比t-SNE, UMAP在可视化质量以及计算速度上均有很大优势。

算法设计的初衷是设计一种可以拓展到海量数据并且可以应对数据多样性的降维方法。降维方法可以分为两类：
+ 寻找保留所有点对距离结构的方法
+ 保留局部距离而不是全局距离的方法

UMAP建立在坚实的数学基础上，尝试解决流型上数据分布一致性的问题。由于其数学基础牢固，因此其并非通过任何特定任务的目标函数得到，这也决定了它是一种更泛用的方法。

# Theoretical Foundations for UMAP
在高层上，UMAP使用局部流型估计并且拼接他们的local fuzzy simplicial set(局部模糊单纯集合，概念可以参考[Wiki](https://zh.wikipedia.org/wiki/%E5%8D%95%E7%BA%AF%E9%9B%86%E5%90%88)) representations来构建高维数据的拓扑表示。给定一些数据的低维表示，类似的过程可以用来建立等价的拓扑表示。UMAP通过优化低维空间内的数据表征的布局，来最小化两个拓扑表征之间的交叉熵。

构建模糊拓扑表征的步骤可以分解为两个问题：估计数据所在的流型，以及构建估计流型的fuzzy simplicial set。
## Uniform distribution of data on a manifold and geodesic approximation

算法的第一步是估计数据所在的流型。假设流型是未知的并且我们想要获得在其上的测地距离。出于理论原因，假设数据均匀分布在流形上是有益的，即使没有做出该假设，结果也仅在有限数据的范围内有效。实际上，真实世界的有限数据很少表现得这么好（指均匀分布）。然而，如果我们假设流形有一个不是从环境空间继承的黎曼度量，我们可以定义一个度量，使得数据相对于该度量近似均匀分布。

使用的数据为$X={X_1,...,X_N}$,流型为$\mathcal{M}$,$g$是$\mathcal{M}$上的黎曼度量，则对于每个点$p\in\mathcal{M}$，都有$g_p$，一个在切空间$\mathcal{T_pM}$上的内积。

这里直接给出引理1
![](/assets/img/20211122/UMAPL1.png)

其大概意思就是在满足度量$g$在点$p$的开邻域$U$上是一个局部的常数时，其在环境坐标中是一个常数对角矩阵。那么在$U$中一个在$g$度量下体积为...的一个以$p$为中心的球上，$p$到求内任意点$q$的测地距离为$\frac{1}{r} d_{\mathbb{R}^{n}}(p, q)$，即可以通过环境坐标中的度量得到。

此时，假设数据是均匀分布在流型$M$上的（相对于$g$），则原理任何边界的情况下，任何固定大小的空间应该包含了大约相等数量的数据点，无论流型的中心在何处。当给定有限的数据以及苏购销的局部邻域，这个估计应该足够准确，及时在靠近流型边界的位置。此时，相反的，对于一个中心在$X_i$处，包含了$k$近邻的球,其应该具有大致固定的体积，而无论$X_i \in X$的取值。因此通过上述的引理，可以估计$X_i$到其近邻的测地距离，**这个距离可以通过对环境距离使用第$k$个近邻的距离归一化而得到**。

本质上通过对于每一个$X_i$都建立一个特制的距离，我们可以保证数据在流型上均匀分布的假设成立，但是代价是现在对于每个点，我们都有一个独立的距离概念，并且这些概念可能并不一致。这样我们就有了一组离散度量空间并且希望将其融合到一致的全局结构里，**这就自然的引入了将度量空间转换到模糊单纯集中的方法**。

## Fuzzy topological representation

首先简介单纯范畴的概念如下
![](/assets/img/20211122/UMAPD1.png)
可以描述为对象为有限序数（视为全序集），态射为保序函数的范畴。

![](/assets/img/20211122/UMAPD2.png)
**单纯集是一个由单纯范畴到Sets的一个反变函子**。**Sets**代表集合范畴，即一个对象为集合

**剩余部分超出了笔者目前的数学水平，暂时没能理解。**

# A Computational View of UMAP
这部分更多的从计算的角度，而不是理论的角度来阐述UMAP。这部分对于算法中若干选择的动机描述不足，这些动机是从上述的数学分析中得出的。

从计算的角度，UMAP最终可以用加权图的构造和操作来描述。与其他基于k-近邻图的算法一样，UMAP可以分两个阶段进行描述。**在第一阶段，构造一个特殊的加权k-近邻图。在第二阶段，计算该图的低维布局。**此类中所有算法之间的差异相当于在如何构造图形和如何计算布局方面的特定细节。如第2节所述，UMAP的理论基础为这两个阶段提供了新的方法，并为所涉及的选择提供了明确的动机。

在UMAP中，我们一直假定以下条件是满足的
+ 存在一个数据均匀分布在上面的流型
+ 感兴趣的基本流型是局部连通的
+ 保持这个流型的拓扑结构是主要目标

任何试图使用类似于k-近邻图的数学结构来近似流形的算法必须遵循类似的基本结构

+ 图建立(Graph Construction)
  + 建立加权k近邻图
  + 将一些边上的变换应用于环境局部距离
  + 处理k近邻图固有的不对称性
+ 图布局(Graph Layout)
  + 定义一个目标函数，该函数保持k-近邻图的期望特征
  + 找到优化此目标函数的低维表示
  
在理论部分中（第二章），本文从理论上证明了使用k-近邻图表示流形的选择是正确的。可以找到对称化函数的核变换选择。最后，概述了我们对图形布局的选择所依据的理由。

## Graph Construction
对于数据$X={x_1,...,x_N}$，通过一个度量（或者相异度量）$d:X\times X \rightarrow \mathbb{R}_{\geq 0}$。通过给定的超参数$k$，得到$x_i$的在度量$d$下的近邻$\left\{x_{i_{1}}, \ldots, x_{i_{k}}\right\}$

对于每个$x_i$，设置$\rho_i,\sigma_i$，满足
$$
\begin{equation}
\rho_{i}=\min \left\{d\left(x_{i}, x_{i_{j}}\right) \mid 1 \leq j \leq k, d\left(x_{i}, x_{i_{j}}\right)>0\right\}
\end{equation}
$$
$$
\begin{equation}
\sum_{j=1}^{k} \exp \left(\frac{-\max \left(0, d\left(x_{i}, x_{i_{j}}\right)-\rho_{i}\right)}{\sigma_{i}}\right)=\log _{2}(k)
\end{equation}
$$

$\rho_i$用于确保$x_i$至少和一个其他数据点连接，从理论上的局部链接限制而来。其也可以防止维度灾难的出现。

$\sigma_i$相当于一只平滑因子，对应了理论上对于每个点$x_i$局部的黎曼度量。

由此得到加权的k近邻图，其边的权重如下
$$
\begin{equation}
w\left(\left(x_{i}, x_{i_{j}}\right)\right)=\exp \left(\frac{-\max \left(0, d\left(x_{i}, x_{i_{j}}\right)-\rho_{i}\right)}{\sigma_{i}}\right) .
\end{equation}
$$

通过如上，我们可以得到加权的邻接矩阵$A$(单有向图)，通过下式得到一个无向图的邻接矩阵$B$
$$
\begin{equation}
B=A+A^{\top}-A \circ A^{\top}
\end{equation}
$$
如果将A解释为两个点之间存在直接连接边的概率，那么B就可以解释为两个点之间至少存在一个直接连接边的概率（因为是双向的）。在之前的理论中（第二章），证明了为何这样构建的图可以提供数据的合适模糊拓扑表征（fuzzy topological representation of the data）。

## Graph Layout
UMAP在低维空间中使用一个力有向图布局算法。力导向图布局利用沿边施加的一组反作用力和顶点之间施加的一组斥力。任何力导向布局算法都需要描述反作用力和斥力。算法通过在每个边或顶点迭代应用反作用力和斥力来进行。这相当于一个非凸优化问题。通过以类似于模拟退火的方式缓慢减小反作用力和斥力，可以保证收敛到局部极小值。

两个节点间的吸引力确定如下
$$
\begin{equation}
\frac{-2 a b\left\|\mathbf{y}_{\mathbf{i}}-\mathbf{y}_{\mathbf{j}}\right\|_{2}^{2(b-1)}}{1+\left\|\mathbf{y}_{\mathbf{i}}-\mathbf{y}_{\mathbf{j}}\right\|_{2}^{2}} w\left(\left(x_{i}, x_{j}\right)\right)\left(\mathbf{y}_{\mathbf{i}}-\mathbf{y}_{\mathbf{j}}\right)
\end{equation}
$$
其中$a,b$是两个超参数。

两个节点之间的排斥力确定如下，通过计算限制得到，即当一个边上有吸引力的时候，其边上的其中一个点会被其他的点所排斥。
$$
\begin{equation}
\frac{2 b}{\left(\epsilon+\left\|\mathbf{y}_{\mathbf{i}}-\mathbf{y}_{\mathbf{j}}\right\|_{2}^{2}\right)\left(1+a\left\|\mathbf{y}_{\mathbf{i}}-\mathbf{y}_{\mathbf{j}}\right\|_{2}^{2 b}\right)}\left(1-w\left(\left(x_{i}, x_{j}\right)\right)\right)\left(\mathbf{y}_{\mathbf{i}}-\mathbf{y}_{\mathbf{j}}\right)
\end{equation}
$$
其中,$\epsilon$用于防止被0除。

虽然图可以使用随机初始化，但是由于对称的拉普拉斯矩阵是流型的Laplacian-Beltrami算子的离散近似，因此可以直接使用其作为初始化权重。

这个力其实就是从梯度优化得到的edge-wise交叉熵，在加权图$G$和等权重图$H$之间（等权重图构建在低维的向量$y$中，而加权图构建在原始的高维向量向量$x$上）。由于$G$可以很好的捕捉到原数据的拓扑特性，而在地位建立的$H$图去尽可能的匹配$G$的拓扑特性，今儿提供了高维数据在低维上的优秀拓扑表征。

# Implementation and Hyper-parameters
这部分主要关注于算法的实现细节，包括超参数的选择以及它们带来的实际效果。
## Algorithm description
算法流程如下：其中的输入从左到右依次代表：输入数据，近邻数，目标（降维后）维数，最小距离（代数参数，用于控制图的布局），优化的迭代次数。
![](/assets/img/20211122/UMAPA1.png)
其中，建立局部模糊单纯集的方法如下
![](/assets/img/20211122/UMAPA2.png)
计算平滑的KNN距离方法如下，这是为了将模糊1-单纯形集的基数固定为一个固定值
![](/assets/img/20211122/UMAPA3.png)
而得到谱嵌入初始化方法如下
![](/assets/img/20211122/UMAPA4.png)

模糊集交叉熵如下
$$
\begin{equation}
\begin{aligned}
C((A, \mu),(A, \nu)) &=\sum_{a \in A} \mu(a) \log \left(\frac{\mu(a)}{\nu(a)}\right)+(1-\mu(a)) \log \left(\frac{1-\mu(a)}{1-\nu(a)}\right) \\
&=\sum_{a \in A}(\mu(a) \log (\mu(a))+(1-\mu(a)) \log (1-\mu(a))) \\
& \quad-\sum_{a \in A}(\mu(a) \log (\nu(a))+(1-\mu(a)) \log (1-\nu(a))) .
\end{aligned}
\end{equation}
$$

上述$\mu$在优化过程中是固定的，因此交叉熵的最小值只取决于$v$，即
$$
\begin{equation}
-\sum_{a \in A}(\mu(a) \log (\nu(a))+(1-\mu(a)) \log (1-\nu(a)))
\end{equation}
$$

之后，需要找到一个可微的$v(a)$的近似，对于给定的1-simplex $a$，这样梯度才可以用于优化。这通过如下实现
Definition 11. Define $\Phi: \mathbb{R}^{d} \times \mathbb{R}^{d} \rightarrow[0,1]$, a smooth approximation of the membership strength of a 1-simplex between two points in $\mathbb{R}^{d}$, as
$$
\begin{equation}
\Phi(\mathbf{x}, \mathbf{y})=\left(1+a\left(\|\mathbf{x}-\mathbf{y}\|_{2}^{2}\right)^{b}\right)^{-1}
\end{equation}
$$
其中
$$
\begin{equation}
\Psi(\mathbf{x}, \mathbf{y})= \begin{cases}1 & \text { if }\|\mathbf{x}-\mathbf{y}\|_{2} \leq \text { min-dist } \\ \exp \left(-\left(\|\mathbf{x}-\mathbf{y}\|_{2}-\text { min-dist }\right)\right) & \text { otherwise }\end{cases}
\end{equation}
$$

最终的优化过程如下
![](/assets/img/20211122/UMAPA5.png)

## Implementation
算法的整体经验复杂度约为$O(N^{1.14})$。

## Hyper-parameters
如上述所说，UMAP算法一共有4个超参数，$n,d,\text{min-dist},\text{n-epochs}$，其中$d，\text{n-epochs}$的效果不言而喻。这里主要讨论另外两个参数。

对于$n$的选择，可以解释为是用来近似流型roughly flat的 local scale，因为流型的估计是通过$n$个近邻的平均获得。这可以看做一个trade-off参数，更小的$n$可以获得流型更加细节的信息，但是小的Local拼接起来并不一定可以看到大尺度的流型特征，相反，更大的$n$具有更好的“大局观”，但是会丢失一些流型上的小细节。
而min-dist是一个直接影响输出的超参数。，其控制了地位空间的模糊单纯集的建立。它代替了用于确保本地连接的到最近邻居的距离。本质上，这决定了在低维表示中，点可以紧密地组合在一起。min-dist上的低值将导致潜在的密集区域，但可能更忠实地表示流形结构。增加“最小距离”的值将迫使嵌入将更多的点分散出来，从而有助于可视化（并避免潜在的过度涂抹问题）。我们将最小距离视为一个基本的美学参数，控制嵌入的外观，因此在使用UMAP进行可视化时更为重要。

下图中提供了超参数对于算法影响的一些示例。下图中的数据点从三维有色正方体中随机采样，因此其没有局部流型结构，更大的n会有更好的效果。
![](/assets/img/20211122/UMAPF1.png)
以下数据从PenDigits数据中进行采样，这意味着更小的n具有更好效果，因为数据自然的中具有明显的聚类结构。

![](/assets/img/20211122/UMAPF2.png)
以下数据中从MNIST数据集中采样，由于这个数据集也自然的具有聚类特征，因此更倾向于用中等大小的$n$。而更大的min-dist会使得远距离的聚类被挤压到一起，使得聚类效果不明显。
![](/assets/img/20211122/UMAPF3.png)

# Practical Efficacy
进一步，作者在更多的数据集上做了实验以对比不同降维方法的效果。这些数据集有：Pen digits, COIL 20, COIL 100, Mouse scRNA-seq, Statlog(Shuttle), MNIST, F-MNIST, Flow cytometry, GoogleNews word vectors。

![](/assets/img/20211122/UMAPF4.png)

可以看到UMAP可以更好的捕捉到原数据的全局和拓扑结构。

定量分析如下
![](/assets/img/20211122/UMAPT1.png)

在聚类的稳定性上，UMAP也优于t-SNE。如下图所示（图中的聚类已经经过了普氏对齐）
![](/assets/img/20211122/UMAPF7.png)
![](/assets/img/20211122/UMAPF8.png)

在运行速度上同样具有显著优势
![](/assets/img/20211122/UMAPT3.png)

下图是UMAP方法对30000000个数据样本，1.8M维的空间聚类得到结果如下
![](/assets/img/20211122/UMAPF13.png)
# Weaknesses
和诸多非线性降维方法一样，UMAP缺乏可解释性。
UMAP中的一大假设是数据中存在流型结构，其可以在有噪声的数据中找到流型结构，就像人从星空中构建星座一样。随着采样数据的增多，噪声会减少，UMAP的结果会更加稳定。但是对于小样本的噪声数据或者只具有大尺度流型的结构，如何检测虚假嵌入是接下来要研究的问题。
同样，UMAP认为局部距离比全局距离更加重要，虽然UMAP捕捉全局距离的能力强于其他的非线性降维方法。
值得注意的是，在组合局部单纯形集结构时，高维空间中的纯最近邻结构没有显式保留。特别是在经典knn图中引入了所谓的“反向最近邻”。结合UMAP保留拓扑而非纯度量结构这一事实，这意味着UMAP在基于度量结构保留的质量度量方面的性能不如某些方法——特别是MDS等方法——这些方法明确设计用于优化度量结构保留。
UMAP试图发现数据均匀分布的流形。如果您对数据的环境距离有很强的信心，那么您应该使用一种技术来明确地保留这些距离。例如，如果您的数据由环境空间的一个区域中非常松散的结构和另一个区域中非常密集的结构组成，则UMAP将尝试将这些局部区域置于均匀的基础上。
最后，为了提高算法的计算效率，进行了大量的近似计算。对于较小（少于500个样本）的数据集大小，这可能会对UMAP的结果产生影响。特别是使用近似最近邻算法和优化中使用的负采样可能会导致次优嵌入。因此推荐使用者谨慎的在小数据集上使用UMAP。