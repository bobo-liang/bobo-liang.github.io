---
layout: post
title: 'Exploring Simple 3D Multi-Object Tracking for Autonomous Driving'
date: 2021-11-23
author: Poley
cover: '/assets/img/20211122/SimTrack.png'
tags: 论文阅读
---

本文一种SimTrack方法用于3D跟踪，集成了跟踪目标关联、新生目标检测和死目标消除等功能在一个模型中。

在2D跟踪中，最主流的方法还是tracking-by-detection，即逐帧检测，并将对应的目标关联起来。因此其主要问题就是association步骤。3D目标检测中也是一样。现在已有的方法都需要一些启发式的方法来进行，目前有很多手工设计的规则来进行这个人物。同样的情况也出现在track life management上，常见的方法是当目标连续出现在一定熟练的帧中（用于滤除false positive）之后，才将其视为一个newborned target。同样在持续消失一定的帧数后，将其删除。这些对应的超参数都需要手动调整，并且一般是data and model dependent的，使得很难进行泛化。

需要额外启发式方法的原因还是在于帧之间在进行目标检测的时候缺乏联系。本文提出SimTrack,可以建立在常见的pillar或者voxel based 3D object detection networks上。提出一种新的novel hybrid-time centerness map，可以不需要匹配而直接将当前检测与之前跟踪的目标联系到一起，并且给出关联的置信度。可以通过置信度阈值简单的判断new-born object 和 died object。

本文的主要贡献在于使用学习的方法代替了启发式的匹配步骤，大大简化了整个跟踪系统。

![](/assets/img/20211122/SimTrackF1.png)

2D跟踪方法可以大体分为两个类别：
+ Tracking-by-detection: 通过检测来得到每一帧的物体检测结果，再通过匹配建立各帧目标之间的关系。从匹配机制上又可以分为两种：
  + The motion based methods: 通过对目标进行状态估计来预测其在未来帧的位置，并与未来帧中的目标通过IOU等方式进行匹配。
  + The appearance based methods： 通过外观的相似度来判别目标之间的匹配。
+ Tracking-by-regression: 直接通过当前帧和之前帧的目标位置信息回归（预测）目标接下来的位置。但是其还需要额外的检测器来应对新生目标。

3D跟踪方法，由于点云没有纹理信息，因此以Tracking-by-detection方法为主流，主要通过motion估计实现。

## Method
![](/assets/img/20211122/SimTrackF2.png)

前置方法需要使用一个3D目标检测方法。这里使用的是基于center和centerness回归的方法。略。

本方法使用一小段点云作为输入（序列），为了简单，这里直接结合了多个幅点云（当前的和过去若干幅），通过ego-motion compensation转换到当前坐标系中，并加上相对时间戳，即点云信息为$(x,y,z,t,\Delta t)$

### Joint Detection and Tracking

为了实现这个目的，本文中主要有两个模块实现：
+ Hybrid-Time Centerness Map: 预测目标最早在场景中出现的位置，并以此作为区分目标的依据，以省去matching的需求。
+ Montion Updating Branch: 预测目标当前位置和目标初始位置的残差，也就是目标在场景中的运动。

#### Hybrid-Time Centerness Map

为了预测目标的起始位置，首先要对每一帧中的每个目标产生其centerness对应的真值。产生的机制如下

![](/assets/img/20211122/SimTrack1.png)

同时，centerness中得到的confidence scores也作为id的置信度来以及目标出现在此帧中的置信度来使用。
#### Montion Updating Branch
直接预测中心中心偏移。和Centerpoint中预测速度不同，其预测速度只是作为一个匹配目标框的中间变量，而这里的motion是直接参与产生目标框的。

#### Loss

中心预测使用Focal Loss
$$
\begin{equation}
\mathcal{L}_{\mathrm{cen}}=\frac{-1}{N} \sum_{c, d_{i}}\left\{\begin{array}{cc}
\left(1-Y_{c, d_{i}}\right)^{\alpha} \log \left(Y_{c, d_{i}}\right), & \text { if } \tilde{Y}_{c, d_{i}}=1 \\
\left(1-\tilde{Y}_{c, d_{i}}\right)^{\beta}\left(Y_{c, d_{i}}\right)^{\alpha} & \log \left(1-Y_{c, d_{i}}\right), & \text { otherwise }
\end{array}\right.
\end{equation}
$$

Motion预测使用L1
$$
\begin{equation}
\mathcal{L}_{\mathrm{mot}}=\frac{1}{N} \sum_{i=1}^{N}\left|\tilde{M}_{d_{i}}-M_{d_{i}}\right|
\end{equation}
$$
回归损失同样使用L1
$$
\begin{equation}
\mathcal{L}_{\mathrm{reg}}=\frac{1}{N} \sum_{i=1}^{N}\left|\tilde{S}_{d_{i}}-S_{d_{i}}\right|
\end{equation}
$$

#### Inference
其预测流程如下
![](/assets/img/20211122/SimTrackA1.png)

对于序列的开始，首先使用一帧来确定初始状态，之后一次输入两帧（都经过了ego-motion compensation）。对于新产生的centerness预测，会和前一帧的进行平均，并使用阈值判断。这可以使得网络自适应的添加/消除目标（比如目标消失一段时间后，经过不断的平均，centerness小于阈值之后，新增同理，相当于加入了一个滑动平均和阈值，取代了之前硬性的时间要求）。

笔者注：注意到，此网络并没有保存时间信息的部分，那么网络是如何预测物体在若干帧之前的初始位置的呢？笔者个人认为是预测不了的，但是由于每一帧预测都需要和上一帧的centerness map进行平均，并且相邻帧之间一般变化较小，因此centerness map的预测本身变动就是相对平滑的，而平均则使得这个过程更加平滑。也就是说，centerness并不需要一定真的在物体初始出现的位置，而是只要在各帧之间保证一种相对的一致性即可（这样就足以区分目标的id）。而motion的预测则是基于centerness map，因此这并不会影响motion的预测（整个网络基于一个目标检测网络，其本身就具有预测目标准确位置的能力，这里将其拆分为一个中心—+一个偏差，相当于将一个绝对的位置拆分成了两个变量进行预测，这对网络的能力并无影响），因此，及时centerness map的位置不准确，只要其一致性可以保证正确的区分出id，加上网络自身就具有的预测目标位置的能力，既可以实现准确的目标跟踪。而唯一的时间信息就保存在这个不断继承的centerness map中。笔者认为作者的这个思路非常巧妙。

## Experiments
![](/assets/img/20211122/SimTrackT12.png)
