---
layout: post
title: 'TANet: Robust 3D Object Detection from Point Clouds with Triple Attention'
date: 2021-11-05
author: Poley
cover: '/assets/img/20211101/TANet.png'
tags: 论文阅读
---

本文提出一种三元注意力模块(Triple Attention module)和一种由粗到细的回归模块(Coarse-to-Fine Regression modele)。有效增加了模型的鲁棒性，以及对于行人类别的检测能力。


Coarse-to-fine的思路是受到RefineDet工作的启发，来实现速度和性能上的平衡。

## 3D Object Detection with TANet

将点云体素化，$N,C$分别代表体素内的最大点数和特征通道数，整个体素网格可以表示为$\mathbf{V}=\{V^1,...,V^K\}$，其中$V^K \in \mathbb{R}^{N\times C}$。

![](/assets/img/20211101/TANetF3.png)
### Point-wise Attention
对一个体素$V^K$内的点，沿特征维度做maxpooling来压缩特征，得到$\mathbb{R}^{N\times 1}$的特征，根据*Excitation operation*通过两个全连接层来得到点的注意力
$$
\begin{equation}
S^{k}=W_{2} \delta\left(W_{1} E^{k}\right)
\end{equation}
$$

**这部分的作用是用来描述同一个体素内的点的空间关系。**

### Channel-wise Attention
和 Point-wise Attention一样，只不过Max pooling是在沿点维度进行的。

之后，通过上述两者的element-wise multiply，可以得到注意力矩阵$M^k \in \mathbb{R}^{N\times C}$如下
$$
\begin{equation}
M^{k}=\sigma\left(S^{k} \times T^{k}\right)
\end{equation}
$$

### Voxel-wise Attention

在上述得到 point和channel加权的体素特征之后，和体素中心concat起来，压缩point-wise和channel-wise的特征，来得到体素注意力，用于判断体素信息的重要性。

注意力可以表示为
$$
\begin{equation}
Q=\left[q^{1}, \ldots, q^{k}, \ldots, q^{K}\right] \in \mathbb{R}^{K \times 1 \times 1}
\end{equation}
$$
$$
\begin{equation}
F_{2}^{k}=q^{k} \cdot F_{1}^{k}
\end{equation}
$$

### Coarse-to-Fine Regression

![](/assets/img/20211101/TANetF4.png)

如上图所示，对于backbone的每一层都分别做下采样，之后concat起来，并与coarse feature 结合，得到更好的预测。

### Loss
很常规的loss,如下
$$
\begin{equation}
\begin{array}{r}
L_{\text {total }}=\alpha L_{\text {cls }}^{\mathcal{C}}+\beta \frac{1}{N_{\text {pos }}^{\mathcal{C}}} \sum L_{\text {reg }}^{\mathcal{C}}\left(\boldsymbol{\Delta}_{\mathbf{p}}^{\mathcal{C}}, \boldsymbol{\Delta}_{\mathrm{g}}^{\mathcal{C}}\right)+ \\
\lambda\left\{\alpha L_{\text {cls }}^{\mathcal{R}}+\beta \frac{1}{N_{\text {pos }}^{\mathcal{R}}} \sum L_{\text {reg }}^{\mathcal{R}}\left(\boldsymbol{\Delta}_{\mathbf{p}}^{\mathcal{R}}, \boldsymbol{\Delta}_{\mathbf{g}}^{\mathcal{R}}\right)\right\}
\end{array}
\end{equation}
$$