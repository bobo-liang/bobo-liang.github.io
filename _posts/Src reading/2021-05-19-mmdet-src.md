---
layout: post
title: 'mmdet源码阅读'
date: 2021-05-19
author: Poley
cover: 'https://github.com/open-mmlab/mmdetection/raw/master/resources/mmdet-logo.png'
tags: 源码阅读
---

# mmdet faster-rcnn-fpn源码流程阅读
1. 由于debug时，train.py文件不在根目录下，因此需要手动修改一下run/debug configuration 中的working directory。要不跑步起来。

2. 使用的数据集是OpenBrand,其格式和coco一样。

3. 首先看一下dataset类，bulid完的dataset的每个元素均是一个字典。其中包含四个keys，分别是
   + img_metas:记录了图像的基本信息，包括路径，文件名，大小，归一化信息，等等。这部分在训练中用不到
   + img：图像信息，torch.tensor
   + gt_bboxes: gt_bboxes信息，没有类别
   + gt_labels：上述gt_bboxes的类别

config文件中的collect中指定的项就是这四个中的一部分。

通过dataloader collect之后，同样生成一个dict，依旧有上面四个Keys，但是每个Keys内的数据变成了list，list内包含batch内的所有数据。也就是说collect的作用是将原各数据字典的对应内容拼接起来，形成一个大字典。

参数直接使用**kwargs通过字典传参，因此需要注意自己设计函数的时候，形参名要和之前输入数据字典中的keys对应上。

 4. 主干网络
   
     mmdet的two-stage detector都有一个共同的父类，**TwoStageDetector**，其又是继承了**BaseDetector**得到的。分别在 **/models/detectors/two_stage.py**和 **/models/detectors/base.py**中。浏览 **two_stage.py**的  **forward_train**方法可以看到其源码对two-stage的检测器做了共同的抽象，源码如下

   ```python
       def forward_train(self,
                         img,
                         img_metas,
                         gt_bboxes,
                         gt_labels,
                         gt_bboxes_ignore=None,
                         gt_masks=None,
                         proposals=None,
                         **kwargs):
           x = self.extract_feat(img)

           losses = dict()

           # RPN forward and loss
           if self.with_rpn:
               proposal_cfg = self.train_cfg.get('rpn_proposal',
                                                 self.test_cfg.rpn)
               rpn_losses, proposal_list = self.rpn_head.forward_train(
                   x,
                   img_metas,
                   gt_bboxes,
                   gt_labels=None,
                   gt_bboxes_ignore=gt_bboxes_ignore,
                   proposal_cfg=proposal_cfg)
               losses.update(rpn_losses)
           else:
               proposal_list = proposals

           roi_losses = self.roi_head.forward_train(x, img_metas, proposal_list,
                                                    gt_bboxes, gt_labels,
                                                    gt_bboxes_ignore, gt_masks,
                                                    **kwargs)
           losses.update(roi_losses)

           return losses
   ```

可以看到，流程很抽象。先提取特征，再送入rpn得到proposal_list以及rpn_loss（也可以使用custom的proposal，当不想使用rpn的时候，这时候自然也没有loss了）。之后将proposal送入roi_head,得到网络最后的Loss。此时就完成了网络的前向过程。

5. 特征提取
   
   随着程序，首先是**extract_feat()**方法。这个方法的内容如下
   ```python
    def extract_feat(self, img):
        """Directly extract features from the backbone+neck."""
        x = self.backbone(img)
        if self.with_neck:
            x = self.neck(x)
        return x
   ```
   同样非常简洁，就是输入经过backbone，再经过neck来进一步提取特征。但是需要具体看一下参数的传递形式。
   
   网络的backbone是resnet，这里输出的是4个stage的输出（在config文件中定义）。最终一个**tuple**的形式输出，其中包含了4个stage的特征图。

   之后进入fpn的部分。输入时直接从backbone中拿过来的tuple，fpn中也对tuple中的各个特征图依次处理。需要注意的是，fpn的inchannel是定义在config中的，需要和resnet的输出特征通道数对应，并且是需要按顺序写的。

   fpn的输入输出层数可以通过多个level变量灵活调控。这部分由于一般不做过多修改，所以没有细看。

   这部分代码也比较简单，即对所有输入先做一个lateral_conv横向连接，然后在建立一个top-down path，其中的超分辨率用插值实现。高层的通过插值扩大featuremap尺寸再和低层相加，层层累加，最后每层通过一个conv得到输出。输出同样转换为一个元组的形式。代码如下，中间还有一段add extra levels的代码，性质差不多，就是添加一个额外的maxpool或者额外的卷积。

   ```python
    def forward(self, inputs):
        """Forward function."""
        assert len(inputs) == len(self.in_channels)
        laterals = [
                lateral_conv(inputs[i + self.start_level])
                for i, lateral_conv in enumerate(self.lateral_convs)
            ]

        # build top-down path
        used_backbone_levels = len(laterals)
        for i in range(used_backbone_levels - 1, 0, -1):
            # In some cases, fixing `scale factor` (e.g. 2) is preferred, but
            #  it cannot co-exist with `size` in `F.interpolate`.
            if 'scale_factor' in self.upsample_cfg:
                laterals[i - 1] += F.interpolate(laterals[i],
                                                **self.upsample_cfg)
            else:
                prev_shape = laterals[i - 1].shape[2:]
                laterals[i - 1] += F.interpolate(
                    laterals[i], size=prev_shape, **self.upsample_cfg)

        # build outputs
        # part 1: from original levels
        outs = [
            self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels)
        ]

        ...
        return tuple(outs)

   ```

6. RPN
   
   fpn看完了，接下来就是rpn,faster-rcnn用的是RPNHEAD，这部分代码在 **/model/dense_heads/rpn_head**中，可以看到**RPNHEAD**继承自两个父类**RPNTestMixin，AnchorHead**,其中前者好像是一个测试版的RPN,而后者应该是所有AnchorHead抽象出来的一个共同父类，其中包括了anchor的大部分内容，包括**anchor_generator** , **bbox_coder** , **loss_cls** ,**loss_bbox** 等anchor方法的核心内容，anchor分类和回归目标的**assigner**也在这里被定义。上述几个关键部分都需要在config文件中定义。
   
   而RPNHEAD本身的结构就只是简单的定义了conv，relu然后再加上一个cls层，一个reg层。没啥好说的，loss的计算直接继承父类(**AnchorHead**)的**loss()**方法。

   而**AnchorHead**由继承一个测试类**BBoxTestMinxin**以及**BaseDenseHead**,关键的训练方法 **forward_train**就继承自后者。具体rpn使用的forward方法是**forward_single**，用于计算单featuremap上的rpn结果。之后将其包装为一个元组+list的形式输出。元组中两项对应score和bbox_pred，各为一个List,其中包括fpn每个特征图对应的输出。之后使用**AnchorHead**来输出Loss和得到proposal。注意，如果config文件中没有proposal的config项，这里就不会输出proposal。

   得到的proposal是一个list，长度为batch_size，每个元素都是一个[1000,5]的tensor，其中1000是config文件中预先定义的nms后的proposal数量，5是Bbox位置和置信度。

7. HEAD
   
   之后进入到了网络的最后一个环节，roihead。
   
   在ROI特征的提取中，主要内容在 **/models/roi_heads_roi_extractors_single_level_roi_extractor.py**中，其根据roi的尺度自动将roi分配到不同level的featuremap上做roipooling提取特征。
   进入类StandardRoIHead,中，使用forward_train函数来进行bbox的assigner和sample，之后输出loss。比较简单。

   至此完成了整个网络的流程。


# 自定义模型

如果要在faster-rcnn的代码上做修改，那么我初步认为应该遵从一下方法。

## 修改网络结构

1. 继承需要修改的类，这样可以直接使用现有的loss计算方法等等。

2. 对于要额外新加的通路，在需要在两个模块之间传递时，需要处理好他们的输入输出格式，以对应上。

3. loss函数在forward_train函数中就可以找到，这个函数在每一个大类（比如backbone，neck）的每个子类（比如resnet,fpn）中都有（继承或者重新定义），是训练中的必经部分。应对类别不平衡等问题时，可以直接针对其中的loss计算进行修改。

4. 如果要加入global context，即将整个feature map作为一个roi，计算特征并cat到别的roi上，那么应该可以直接在roihead模块内部做修改。



## TODO

有很多基于mmdet的project，可以参考一下他们是如何基于mmdet框架来定义自己的网络结构的。




