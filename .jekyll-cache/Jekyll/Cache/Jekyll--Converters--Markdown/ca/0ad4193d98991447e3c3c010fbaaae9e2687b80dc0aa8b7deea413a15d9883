I"£<blockquote>
  <p>è®ºæ–‡é“¾æ¥ï¼š https://openaccess.thecvf.com/content_ICCV_2019/papers/Tian_FCOS_Fully_Convolutional_One-Stage_Object_Detection_ICCV_2019_paper.pdf</p>
</blockquote>

<p>ç‰¹ç‚¹ï¼š <strong>Anchor box free, proposal free, Avoid all hyper-parameters related to anchor boxes, simple</strong>ã€‚</p>
<h1 id="introduction">Introduction</h1>

<p>ç›®å‰ä¸»æµçš„æ–¹æ³•ï¼ˆ2019å¹´ï¼‰éƒ½é€šè¿‡é¢„è®¾anchor-boxesæ¥è¾¾åˆ°å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸” <em>it has long been
believed that the use of anchor boxes is the key to detectors success</em>.</p>

<p>ä½†æ˜¯anchor-based detectorå…·æœ‰å¦‚ä¸‹çš„ç¼ºç‚¹</p>

<ul>
  <li><strong>Detection performance is sensitive to the sizes, aspect ratios and number of anchor boxes</strong>ï¼š æ¯”å¦‚RetinaNeté€šè¿‡è°ƒæ•´anchorè¶…å‚æ•°å¯ä»¥åœ¨cocoä¸Šæå‡4%çš„APã€‚</li>
  <li><strong>Because the scales and aspect ratios of anchor boxes are kept fixed, detectors encounter difficulties to deal with object candidates with large shape variations, particularly for small objects</strong> ï¼šé¢„å®šä¹‰çš„anchoré˜»ç¢äº†detectorçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
  <li><strong>In order to achieve a high recall rate, an anchor-based detector is required to densely place anchor boxes on the input image</strong>ï¼š å¤§é‡çš„anchorä¸­ç»å¤§éƒ¨åˆ†éƒ½æ˜¯è´Ÿæ ·æœ¬ï¼Œé€ æˆäº†ç±»åˆ«ä¸å¹³è¡¡ã€‚</li>
  <li><strong>Anchor boxes also involve complicated computation such as calculating the intersection-over-union (IoU) scores with ground-truth bounding boxes</strong>ï¼šå¤§é‡çš„IOUè®¡ç®—èŠ±è´¹å¾ˆå¤šæ—¶é—´ã€‚</li>
</ul>

<p>å—åˆ°FCNåœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸Šçš„å¯å‘ï¼Œè‡ªç„¶çš„å¼•å‘ä¸€ä¸ªé—®é¢˜ï¼š<em>Can we solve object detection in the neat per-pixel prediction fashion, analogue to FCN for semantic segmentation, for example?</em></p>

<p>æœ‰ä¸€äº›æ–¹æ³•é€šè¿‡FCN-based æ¶æ„æ¥è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œæ¯”å¦‚Dense-Boxã€‚è¿™äº›æ–¹æ³•éƒ½é€šè¿‡é¢„æµ‹ä¸€ä¸ª4D vectoråŠ ä¸€ä¸ªç±»åˆ«ï¼Œåœ¨æ¯ä¸ªç©ºé—´ä½ç½®ä¸Šï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚</p>

<p><img src="/assets/img/20210616/FCOSF1.png" alt="" /></p>

<p>ä½†ç°æœ‰çš„æ–¹æ³•åŸºæœ¬éƒ½ç”¨åœ¨ç‰¹æ®Šé¢†åŸŸçš„ç›®æ ‡è¯†åˆ«ï¼Œè€Œä¸æ˜¯é€šç”¨ç›®æ ‡è¯†åˆ«ã€‚å› ä¸ºé€šç”¨ç›®æ ‡è¯†åˆ«æœ‰è¾ƒå¤šçš„<strong>highly overlapped bounding boxes</strong>ï¼Œä¸€ä¸ªåƒç´ çš„å½’å±åœ¨è¿™ç§æƒ…å†µä¸‹å˜å¾—æ›´ä¸ºå¤æ‚ã€‚</p>

<p>ä½†æ˜¯ä½œè€…å‘ç°FPNå¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯åˆä¼šäº§ç”Ÿå¾ˆå¤šä½è´¨é‡çš„æ¡†ã€‚å› æ­¤åç€æå‡ºä¸€ä¸ª<strong>center-ness branch</strong>æœ‰æ•ˆçš„æŠ‘åˆ¶äº†è¿™ç§æƒ…å†µçš„å‘ç”Ÿã€‚</p>

<p>FCOSä¼˜åŠ¿å¦‚ä¸‹ï¼š</p>
<ul>
  <li>å°†ç›®æ ‡æ£€æµ‹å’Œå…¶ä»–FCNå¯ä»¥è§£å†³çš„é—®é¢˜ï¼Œæ¯”å¦‚ç›®æ ‡åˆ†å‰²è”ç³»èµ·æ¥ã€‚ä½¿å¾—è¿™äº›ä»»åŠ¡ä¸Šçš„ideaså¯ä»¥é‡å¤åˆ©ç”¨åˆ°ç›®æ ‡æ£€æµ‹ä¸Š</li>
  <li>FCOSä½¿å¾—æ£€æµ‹æ¡†æ¶æ›´åŠ ç®€å•ï¼Œå°‘äº†å¾ˆè¶…å‚æ•°çš„è®¾å®š</li>
  <li>ç”±äºæ²¡æœ‰Anchorï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—IOUçš„æ—¶é—´</li>
  <li>è¾¾åˆ°äº†å•é˜¶æ®µæ£€æµ‹å™¨çš„SOAT</li>
  <li>å¯ä»¥å¾ˆå®¹æ˜“çš„è¿ç§»åˆ°åˆ«çš„ä»»åŠ¡ä¸Š</li>
</ul>

<p><img src="/assets/img/20210616/FCOSF2.png" alt="" /></p>

<h1 id="our-approach">Our Approach</h1>

<h2 id="fully-convolutional-one-stage-object-detector">Fully Convolutional One-Stage Object Detector</h2>

<p>ä¸anchor basedæ–¹æ³•ä¸åŒçš„æ˜¯ï¼Œanchor basedæ–¹æ³•å›å½’anchor boxå’Œ target boxä¹‹é—´çš„ä¸åŒï¼Œè€Œæœ¬æ–‡ä¸­åˆ™ç›´æ¥åœ¨å½“å‰ä½ç½®å›å½’target bounding boxã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæœ¬æ–¹æ³•ç›´æ¥å°†<strong>locationçœ‹åštraining samles</strong>ã€‚</p>

<p>å½“ä¸€ä¸ªä½ç½®è½åœ¨ä»»ä½•ä¸€ä¸ªground-truth boxé‡ï¼Œåˆ™è®¤ä¸ºä»–æ˜¯æ­£æ ·æœ¬å¹¶ç»™äºˆå¯¹åº”çš„æ ‡ç­¾ã€‚å¦‚æœä¸€ä¸ªä½ç½®åŒæ—¶è½åœ¨å¤šä¸ªbounding boxé‡Œï¼Œåˆ™å°†å…¶è§†ä¸ºä¸€ä¸ª<strong>ambiguous sample</strong>ï¼Œç®€å•é€‰å–å…¶å¯¹åº”çš„æœ€å°é¢ç§¯çš„Bounding boxä½œä¸ºå›å½’ç›®æ ‡ï¼Œåœ¨multi-level predictionä¸­ï¼Œamibiguous sampleçš„æ•°é‡å¯ä»¥å¤§å¤§å‡å°‘å¹¶ä¸”å¾ˆéš¾å½±å“åˆ°æ£€æµ‹çš„æ€§èƒ½ã€‚å›å½’ç›®æ ‡å¦‚ä¸‹æ‰€ç¤ºï¼Œå³åˆ°å››æ¡è¾¹çš„è·ç¦»</p>

\[\begin{equation}
\begin{aligned}
l^{*} &amp;=x-x_{0}^{(i)}, \quad t^{*}=y-y_{0}^{(i)} \\
r^{*} &amp;=x_{1}^{(i)}-x, \quad b^{*}=y_{1}^{(i)}-y
\end{aligned}
\end{equation}\]

<p><strong>It is worth noting that FCOS can leverage as many foreground samples as possible to train the regressor.</strong></p>

<p>æœ€ç»ˆæ¯ä¸ªç‚¹è¾“å‡ºä¸€ä¸ª4D vectorsï¼ŒåŠ ä¸Šä¸€ä¸ªCä¸ªäºŒåˆ†ç±»ç»“æœã€‚ç”±äºå›å½’ç»“æœæ€»æ˜¯æ­£çš„ï¼Œå› æ­¤ç”¨æŒ‡æ•°å‡½æ•°æ¥å°†$[-\infty,\infty]$æ˜ å°„åˆ°$[0,\infty]$ã€‚</p>

<h2 id="multi-level-prediction-with-fpn-for-fcos">Multi-level Prediction with FPN for FCOS</h2>

<p>ç®€è¦çš„è¯´ï¼Œmulti-level predictionå¯ä»¥æé«˜FCOSçš„ <strong>best possible recall(BPR)</strong>,åŒæ—¶ä¹Ÿå¯ä»¥æ˜¾è‘—çš„è§£å†³<strong>ambiguity</strong>çš„é—®é¢˜ã€‚</p>

<p>å’Œanchor-basedæ–¹æ³•ç›´æ¥é€šè¿‡anchorå¤§å°æ¥åˆ’åˆ†åˆ°ä¸åŒçš„levelä¸Šä¸åŒï¼Œè¿™é‡Œé€šè¿‡é™åˆ¶å›å½’çš„å€¼æ¥åˆ†å±‚ã€‚æ¯”å¦‚</p>

\[\begin{equation}
\max \left(l^{*}, t^{*}, r^{*}, b^{*}\right)&gt;m_{i},\max \left(l^{*}, t^{*}, r^{*}, b^{*}\right)&lt;m_{i-1}
\end{equation}\]

<h2 id="center-ness-for-fcos">Center-ness for FCOS</h2>

<p>ç”¨ä¸€å±‚ç¥ç»ç½‘ç»œæ¥é¢„æµ‹centernessï¼Œå…¶å®šä¹‰å¦‚ä¸‹</p>

\[\text { centerness }^{*}=\sqrt{\frac{\min \left(l^{*}, r^{*}\right)}{\max \left(l^{*}, r^{*}\right)} \times \frac{\min \left(t^{*}, b^{*}\right)}{\max \left(t^{*}, b^{*}\right)}}\]

<p><img src="/assets/img/20210616/FCOSF3.png" alt="" /></p>

<p>å°†å…¶ä½œä¸ºå¾—åˆ†çš„æƒé‡ç”¨åœ¨NMSé‡Œï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚</p>

<h1 id="experiments">Experiments</h1>

<p><img src="/assets/img/20210616/FCOST1T2.png" alt="" />
<img src="/assets/img/20210616/FCOST3.png" alt="" />
<img src="/assets/img/20210616/FCOST4.png" alt="" /></p>
:ET