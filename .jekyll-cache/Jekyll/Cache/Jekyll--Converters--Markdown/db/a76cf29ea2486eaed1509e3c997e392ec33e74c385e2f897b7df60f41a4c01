I"Ã<blockquote>
  <p>è®ºæ–‡é“¾æ¥ï¼š https://arxiv.org/abs/1810.13125</p>
  <h1 id="introduction">Introduction</h1>
  <p>ä¼ ç»Ÿçš„non-localæ¨¡å—åªå…³ç³»äº†æ—¶é—´å’Œç©ºé—´ä¸Šçš„å…³ç³»ï¼Œè€Œæ²¡æœ‰å…³å¿ƒchannelä¹‹é—´çš„å…³ç³»ã€‚è€Œå¾€å¾€åœ¨è§†é¢‘ä¸­ï¼Œä¸åŒç›®æ ‡ä¹‹é—´çš„å…³ç³»æ˜¯ä½“ç°åœ¨ä¸åŒchannelä¸­çš„ï¼Œæ¯”å¦‚è„šå’Œè¶³çƒï¼ˆè§ä¸‹å›¾ï¼‰ã€‚å› æ­¤ï¼Œåœ¨non-localä¸­å¼•å…¥channelä¿¡æ¯æ˜¯éå¸¸å…³é”®çš„ï¼Œè¿™å°±æ˜¯æœ¬æ–‡ä¸­æå‡ºçš„ compact generalized non-local (CGNL) moduleã€‚</p>
</blockquote>

<p><img src="/assets/img/20210522/CGNLF1.png" alt="" /></p>

<h1 id="related-work">Related work</h1>
<p>ç•¥</p>

<h1 id="approch">Approch</h1>
<h2 id="review-of-non-local-operation">Review of Non-local Operation</h2>
<p>Non-local operation è®¡ç®—å“åº”Yï¼Œé€šè¿‡æ‰€æœ‰ä½ç½®çš„ç‰¹å¾åŠ æƒæ±‚å’Œ
 \(\begin{equation}
\mathbf{Y}=f(\theta(\mathbf{X}), \phi(\mathbf{X})) g(\mathbf{X})
\end{equation}\)</p>

<p>where $\theta(\cdot), \phi(\cdot), g(\cdot)$ are learnable transformations on the input. In [27], the authors suggest using $1 \times 1$ or $1 \times 1 \times 1$ convolution for simplicity, i.e., the transformations can be written as
\(\begin{align}
\theta(\mathbf{X})=\mathbf{X} \mathbf{W}_{\theta} \in \mathbb{R}^{N \times C}, \\
\quad \phi(\mathbf{X})=\mathbf{X} \mathbf{W}_{\phi} \in \mathbb{R}^{N \times C}, \\
\quad g(\mathbf{X})=\mathbf{X} \mathbf{W}_{g} \in \mathbb{R}^{N \times C}
\end{align}\)</p>

<p>è¿™ä¸ªfå…¶å®å°±æ˜¯ä¸€ä¸ªpairwiseå‡½æ•°<br />
\(\begin{equation}
f(\cdot, \cdot): \mathbb{R}^{N \times C} \times \mathbb{R}^{N \times C} \rightarrow \mathbb{R}^{N \times N}
\end{equation}\)æ¥è®¡ç®—æ‰€æœ‰ä½ç½®ä¹‹é—´çš„äº²å’Œåº¦ï¼ˆæ—¶é—´æˆ–è€…ç©ºé—´ç©ºé—´ä¸Šï¼‰ï¼Œåœ¨få‡½æ•°ä¸Šå…·æœ‰å¾ˆå¤šç§é€‰æ‹©ï¼Œæ¯”å¦‚æœ€ç®€å•çš„ç‚¹ç§¯</p>

<p>\(\begin{equation}
\mathbf{f}(\theta(\mathbf{X}), \phi(\mathbf{X}))=\theta(\mathbf{X}) \phi(\mathbf{X})^{\top}
\end{equation}\)
æ‰€ä»¥è¿™å®é™…ä¸Šç›¸å½“äºä¸€ä¸ªä¸‰çº¿æ€§æ’å€¼trilinear interpretation
\(\begin{equation}
\mathbf{Y}=\mathbf{X} \mathbf{W}_{\theta} \mathbf{W}_{\phi}^{\top} \mathbf{X}^{\top} \mathbf{X} \mathbf{W}_{g}
\end{equation}\)
æ‰€ä»¥non-localæ“ä½œçš„æ•ˆæœå¯èƒ½å’Œself-attention modelæœ‰å…³ï¼Œå› ä¸ºYçš„æœ¬è´¨æ˜¯æ‰€æœ‰ä½ç½®çš„çº¿æ€§åŠ æƒã€‚</p>

<h2 id="review-of-bilinear-pooling">Review of Bilinear pooling</h2>

<p>Bilinear poolingå°†ç‚¹å¯¹ä¹‹é—´çš„å…³ç³»å»ºç«‹æˆå¤–ç§¯(outer product)çš„å½¢å¼ï¼Œä½¿ç”¨åœ¨final classification layerä¸Šã€‚
\(\begin{equation}
\mathbf{Z}=\mathbf{X}^{\top} \mathbf{X} \in \mathbb{R}^{C \times C}
\end{equation}\)
ä¹‹åæœ€ç»ˆçš„æè¿°å­é€šè¿‡ä¸‹å¼æ±‚å’Œè·å¾—</p>

\[\begin{equation}
z_{c_{1} c_{2}}=\sum_{n} x_{n c_{1}} x_{n c_{2}}
\end{equation}\]

<p>å…¶å’Œnon-localçš„å…³ç³»ç›¸å½“äº</p>

\[\begin{equation}
\theta(\mathbf{X})=\mathbf{X}^{\top} \in \mathbb{R}^{C \times N}, \quad \phi(\mathbf{X})=\mathbf{X}^{\top} \in \mathbb{R}^{C \times N}
\end{equation}\]

<h2 id="generalized-non-local-operation">Generalized Non-local Operation</h2>

<p>åœ¨ä¸Šè¿°çš„å…³ç³»ä¸­ï¼Œchannel-wiseçš„å…³ç³»å®é™…ä¸Šæ˜¯éƒ½è¢«ä¸€èµ·aggregationäº†ã€‚å¤šä¸ªå·¥ä½œå·²ç»å°†channel-wiseçš„å…³ç³»åˆ©ç”¨èµ·æ¥ã€‚å› æ­¤å—è¿™äº›å·¥ä½œå¯å‘ï¼Œæœ¬æ–‡å¯¹Non-localåšäº†generalizeä½¿å¾—è¿™ç§æ“ä½œå¯ä»¥å»ºæ¨¡ long-range dependencies between any positions of any channelsã€‚</p>

<p>æ–¹æ³•å¦‚ä¸‹ï¼Œå…ˆå°†X reshapeæ¥å°†channelçš„ä¿¡æ¯èåˆåˆ°positioné‡Œã€‚</p>

<p>We first reshape the output of the transformations (Eq. 2$)$ on $\mathrm{X}$ by merging channel into position:
\(\begin{align}
\theta(\mathbf{X})=\operatorname{vec}\left(\mathbf{X} \mathbf{W}_{\theta}\right) \in \mathbb{R}^{N C}, \\
\phi(\mathbf{X})=\operatorname{vec}\left(\mathbf{X} \mathbf{W}_{\phi}\right) \in \mathbb{R}^{N C}, \\
g(\mathbf{X})=\operatorname{vec}\left(\mathbf{X} \mathbf{W}_{g}\right) \in \mathbb{R}^{N C}
\end{align}\)
By lifting the row space of the underlying transformations, our generalized non-local (GNL) operation pursues the same goal of Eq. 1 that computes the response $\mathbf{Y} \in \mathbb{R}^{N \times C}$ as:
\(\begin{equation}
\operatorname{vec}(\mathbf{Y})=f\left(\operatorname{vec}\left(\mathbf{X} \mathbf{W}_{\theta}\right), \operatorname{vec}\left(\mathbf{X} \mathbf{W}_{\phi}\right)\right) \operatorname{vec}\left(\mathbf{X} \mathbf{W}_{g}\right)
\end{equation}\)</p>

<p>è¿™æ ·è¿™å˜æˆäº†ä¸€ä¸ª
\(\begin{equation}
f(\cdot, \cdot): \mathbb{R}^{N C} \times \mathbb{R}^{N C} \rightarrow \mathbb{R}^{N C \times N C}
\end{equation}\)
 çš„æ˜ å°„ï¼Œè¿™æ ·å°±å¯ä»¥åŒºåˆ†ä»»æ„ä½ç½®å’Œé€šé“çš„ä¿¡æ¯ã€‚</p>

<p>ç”±äºricher similarity æå¤§çš„æå‡äº† non-localåˆ¤åˆ«fine-grained object partsæˆ–è€…action snppetsï¼ˆåŠ¨ä½œç‰‡æ®µï¼‰çš„èƒ½åŠ›ã€‚ä»–å¯ä»¥è¢«æ’å…¥åˆ°ç½‘ç»œçš„ä»»ä½•åœ°æ–¹ï¼Œè€Œä¸åªæ˜¯æœ€åä¸€å±‚ã€‚</p>

<h2 id="compact-representation">Compact Representation</h2>

<p>ä¸Šè¿°GNLçš„ä¸€ä¸ªæ˜¾è‘—é—®é¢˜æ˜¯ï¼Œå®ƒéšç€channel number Cçš„æ•°é‡å¹³æ–¹ä¸Šæ¶¨ã€‚å°½ç®¡å¯ä»¥é€šè¿‡åˆ†ç»„å‡å°å¼€é”€ï¼Œä½†æ˜¯è¿˜æ˜¯æœ‰å¾ˆå¤§çš„è´Ÿæ‹…ã€‚å› æ­¤ï¼Œæå‡ºäº†ä»–çš„compact GNL,æ¥é€šè¿‡å¯æ¥å—çš„è¿‘ä¼¼æ¥è¿‘ä¼¼GNLçš„åŠŸèƒ½ã€‚
\(\begin{align*}
\text { Let us denote } \\
\boldsymbol{\theta}=\operatorname{vec}\left(\mathbf{X} \mathbf{W}_{\theta}\right), \\
\boldsymbol{\phi}=\operatorname{vec}\left(\mathbf{X} \mathbf{W}_{\phi}\right)  \\
\boldsymbol{g}=\operatorname{vec}\left(\mathbf{X} \mathbf{W}_{g}\right) \text { , }
\end{align*}\)
ä¸å¤±ä¸€èˆ¬æ€§ï¼Œå‡è®¾fæ˜¯ä¸€ä¸ªgeneral kernel functionã€‚è€Œ</p>

<p>æ˜¯NCç»´åº¦çš„å‘é‡ã€‚é‚£ä¹ˆfå¯ä»¥ç”¨æ³°å‹’å±•å¼€è¿‘ä¼¼
\(\begin{equation}
[f(\boldsymbol{\theta}, \boldsymbol{\phi})]_{i j} \approx \sum_{p=0}^{P} \alpha_{p}^{2}\left(\theta_{i} \phi_{j}\right)^{p}
\end{equation}\)</p>

<p>ä»¥RBFï¼ˆå¾„å‘åŸºå‡½æ•°ï¼‰æ ¸ä¸ºä¾‹ï¼Œ
 \(\begin{equation}
[f(\boldsymbol{\theta}, \boldsymbol{\phi})]_{i j}=\exp \left(-\gamma\left\|\theta_{i}-\phi_{j}\right\|^{2}\right) \approx \sum_{p=0}^{P} \beta \frac{(2 \gamma)^{p}}{p !}\left(\theta_{i} \phi_{j}\right)^{p}
\end{equation}\)
 where $\alpha_{p}^{2}=\beta \frac{(2 \gamma)^{p}}{p !}$ and $\beta=\exp \left(-\gamma\left(|\boldsymbol{\theta}|^{2}+|\phi|^{2}\right)\right)$ is a constant and $\beta=\exp (-2 \gamma)$ if the input
vectors $\boldsymbol{\theta}$ and $\boldsymbol{\phi}$ are $\ell 2$ -normalized. By introducing two matrices,
\(\begin{align*}
\Theta=\left[\alpha_{0} \theta^{0},
\cdots, \alpha_{P} \boldsymbol{\theta}^{P}\right] \in \mathbb{R}^{N C \times(P+1)}, \\
\quad \boldsymbol{\Phi}=\left[\alpha_{0} \phi^{0}, \cdots, \alpha_{P} \boldsymbol{\phi}^{P}\right] \in \mathbb{R}^{N C \times(P+1)}
\end{align*}\)
our compact generalized non-local (CGNL) operation approximates Eq. 8 via a trilinear equation,
\(\operatorname{vec}(\mathbf{Y}) \approx \Theta \Phi^{\top} g\)
ä½†æ˜¯ä¸Šè¿°ä»ç„¶åŒ…æ‹¬large pairwise matrix 
\(\begin{equation}
\Theta \Phi^{\top} \in \mathbb{R}^{N C \times N C}
\end{equation}\)
ä½†æ˜¯å¹¸è¿çš„æ˜¯ä»–ä»¬çš„æ³°å‹’å±•å¼€é˜¶æ•°$PÂ«NC$ï¼Œå› æ­¤å¯ä»¥å…ˆè®¡ç®—$\boldsymbol{z}=\boldsymbol{\Phi}^{\top} \boldsymbol{g} \in \mathbb{R}^{P+1}$ ï¼Œå¾—åˆ°å°çš„è®¡ç®—å¤æ‚åº¦$\mathcal{O}(N C(P+1))$</p>

<p>å…¶æ¨¡å‹å¦‚ä¸‹</p>

<p><img src="/assets/img/20210522/CGNLF2.png" alt="" /></p>

:ET