I"S<blockquote>
  <p>论文链接：https://arxiv.org/pdf/1706.03762v5.pdf
参考博客：https://zhuanlan.zhihu.com/p/46990010
<strong>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.</strong></p>
</blockquote>
:ET