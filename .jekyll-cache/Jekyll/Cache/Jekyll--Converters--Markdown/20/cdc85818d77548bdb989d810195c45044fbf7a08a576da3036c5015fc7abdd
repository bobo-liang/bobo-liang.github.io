I"‡<blockquote>
  <p>å‚è€ƒåšå®¢ï¼šhttps://0809zheng.github.io/2021/08/09/external.html</p>
</blockquote>

<p>self-attentionå…·æœ‰$O(n^2)$çš„å¤æ‚åº¦ï¼Œå¹¶ä¸”å¿½ç•¥äº†ä¸åŒæ ·æœ¬ä¹‹é—´çš„å…³ç³»ã€‚æœ¬æ–‡æå‡ºExternal attention éšå¼çš„åŒ…å«äº†å…¨éƒ¨æ•°æ®ä¹‹é—´çš„correlationsã€‚å¹¶ä¸”å…·æœ‰çº¿æ€§å¤æ‚åº¦ã€‚åœ¨å¤šä¸ªcv tasksä¸Šå…·æœ‰ä¸é”™çš„æ•ˆæœã€‚</p>

<p>æœ¬æ–‡ä½¿ç”¨ä¸¤ä¸ªmemoryæ¥ä»£æ›¿self attentionä¸­çš„keyså’Œvaluesï¼Œè¿™ä¸¤è€…å¯¹äºå•ç‹¬çš„æ ·æœ¬æ˜¯ç‹¬ç«‹çš„ï¼Œå¹¶ä¸”åœ¨æ•´ä¸ªæ•°æ®é›†å†…å…±äº«ï¼Œå› æ­¤èµ·åˆ°äº†ä¸€ä¸ªå¾ˆå¼ºçš„<strong>æ­£åˆ™ä½œç”¨ï¼Œå¹¶ææˆäº†attentionæœºåˆ¶çš„æ³›åŒ–èƒ½åŠ›</strong>ã€‚</p>

<p>é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥å°†ç®—æ³•å¤æ‚åº¦å˜ä¸ºçº¿æ€§ã€‚åœ¨å®è·µä¸­ï¼Œä¸¤ä¸ªmemoryæ¨¡å—é€šè¿‡ä¸¤ä¸ªçº¿æ€§å±‚æ¥å®ç°ï¼Œä½¿å¾—æ•´ä¸ªç½‘ç»œå˜ä¸º<strong>all-MLP architecture named EAMLP</strong></p>

<p>å…¶å’ŒSelf-attentionçš„å¯¹æ¯”å¦‚ä¸‹å›¾æ‰€ç¤º</p>

<p><img src="/assets/img/20210824/ExternalAttF1.png" alt="" /></p>

<p>ç”±äºæœ¬æ–‡çš„memoryé€šè¿‡MLPå®ç°ï¼Œå› æ­¤å¯ä»¥é€šè¿‡BPç®—æ³•ç«¯åˆ°ç«¯å®ç°ï¼Œè€Œä¸éœ€è¦é¢å¤–çš„è¿­ä»£ç®—æ³•ã€‚</p>

<p>ä¼ ç»Ÿçš„self attention å¦‚ä¸‹æ‰€ç¤º</p>

\[\begin{equation}
\begin{aligned}
A &amp;=(\alpha)_{i, j}=\operatorname{softmax}\left(Q K^{T}\right) \\
F_{o u t} &amp;=A V
\end{aligned}
\end{equation}\]

<p>å…¶ä¸­$Q \in \mathbb{R}^{N \times d^{\prime}}$ï¼Œ$K \in \mathbb{R}^{N \times d^{\prime}}$ï¼Œ$V \in \mathbb{R}^{N \times d^{\prime}}$</p>

<p>ä¸€ç§ç®€åŒ–çš„æ–¹æ³•æ˜¯ç›´æ¥é€šè¿‡è¾“å…¥ç‰¹å¾è®¡ç®—attention mapï¼Œå¦‚ä¸‹
\(\begin{equation}
\begin{aligned}
A &amp;=\operatorname{softmax}\left(F F^{T}\right) \\
F_{\text {out }} &amp;=A F .
\end{aligned}
\end{equation}\)</p>

<p>ä½†æ˜¯å°½ç®¡ç®€åŒ–ï¼Œè¿™è¿˜æ˜¯$O(n^2)$çš„å¤æ‚åº¦ã€‚å› æ­¤å¯¹äºå›¾åƒæ¥è¯´ï¼Œé€åƒç´ çš„self-attentionæˆæœ¬å¤ªé«˜ï¼Œåªèƒ½åˆ†ä¸ºè‹¥å¹²ä¸ªpatchå¤„ç†ã€‚</p>

<p>Self-attentionå¯ä»¥æ˜¯åšäº‹ä½¿ç”¨è‡ªå·±å€¼çš„çº¿æ€§ç»„åˆæ¥å¯¹è¾“å‡ºç‰¹å¾è¿›è¡Œrefine,ä½†æ˜¯è¿™ä¸€å®šéœ€è¦$N\times N$å¤§å°çš„çŸ©é˜µä¹ˆï¼Ÿå› æ­¤ä½œè€…å¼•å…¥extenal memory unit $M \in \mathbb{R}^{S \times d}$</p>

\[\begin{equation}
\begin{aligned}
A &amp;=(\alpha)_{i, j}=\operatorname{Norm}\left(F M^{T}\right), \\
F_{\text {out }} &amp;=A M
\end{aligned}
\end{equation}\]

<p>æ—¶é—´ä¸­ï¼ŒMè¢«åˆ†ä¸ºä¸¤ä¸ªä¸åŒçš„memory unitå¦‚ä¸‹
\(\begin{equation}
\begin{aligned}
A &amp;=(\alpha)_{i, j}=\operatorname{Norm}\left(F M_k^{T}\right), \\
F_{\text {out }} &amp;=A M_v
\end{aligned}
\end{equation}\)</p>

<p>å…¶ä¼ªä»£ç å¦‚ä¸‹</p>

<p><img src="/assets/img/20210824/ExternalAttA1.png" alt="" /></p>

<p>ç”±äºattentionæ˜¯é€šè¿‡çŸ©é˜µä¹˜æ³•è€Œä¸æ˜¯ä½™å¼¦è·ç¦»å¾—åˆ°çš„ï¼Œå› æ­¤å¯¹äºè¾“å…¥ç‰¹å¾çš„scaleéå¸¸æ•æ„Ÿã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…ä½¿ç”¨äº†double-normalization,å…¶ç‹¬ç«‹çš„å¯¹è¡Œå’Œåˆ—è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¦‚ä¸‹æ‰€ç¤º</p>

\[\begin{equation}
\begin{aligned}
(\tilde{\alpha})_{i, j} &amp;=F M_{k}^{T} \\
\hat{\alpha}_{i, j} &amp;=\exp \left(\tilde{\alpha}_{i, j}\right) / \sum_{k} \exp \left(\tilde{\alpha}_{k, j}\right) \\
\alpha_{i, j} &amp;=\hat{\alpha}_{i, j} / \sum_{k} \hat{\alpha}_{i, k}
\end{aligned}
\end{equation}\]

<p>åŒæ ·ï¼Œå’Œself-attentionä¸€æ ·ï¼Œexternal attentionå¯ä»¥å¼•å…¥ multi-head æœºåˆ¶ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚</p>

<p><img src="/assets/img/20210824/ExternalAttF2.png" alt="" /></p>

<p><img src="/assets/img/20210824/ExternalAttA2.png" alt="" /></p>

<p>å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå…¶å®éªŒä¸­å±•ç°å‡ºå…¶åœ¨ç‚¹äº‘åˆ†ç±»ä¸Šä¹Ÿæœ‰ä¸é”™çš„æ•ˆæœã€‚ç¬”è€…ä¹‹åæ‰“ç®—è¿›ä¸€æ­¥å°è¯•ä¸€ä¸‹ã€‚</p>
:ET