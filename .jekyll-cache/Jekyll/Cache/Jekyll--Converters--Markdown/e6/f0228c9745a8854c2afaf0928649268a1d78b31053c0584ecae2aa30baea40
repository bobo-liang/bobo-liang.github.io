I"<blockquote>
  <p>论文链接 ： https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf</p>
</blockquote>

<p>本文多次引用了参考文献[61]，应当去学习一下。
<img src="/assets/img/20210531/MOCOF1.png" alt="" /></p>

<h2 id="constrastive-learning-as-dictionary-look-up">Constrastive Learning as Dictionary Look-up</h2>

<p>Contrastive loss function，设计的目的是当一个query 有唯一对应的key时，两者尽可能相似时，损失小，反之，则损失大（即和其他keys相似则视为负样本）。一种常用的是 <strong>InfoNCE</strong> 如下</p>

\[\begin{equation}
\mathcal{L}_{q}=-\log \frac{\exp \left(q \cdot k_{+} / \tau\right)}{\sum_{i=0}^{K} \exp \left(q \cdot k_{i} / \tau\right)}
\end{equation}\]

<p>Contrastive loss mechanisms的几种形式如下</p>

<p><img src="/assets/img/20210531/MOCOF2.png" alt="" /></p>

<h2 id="momentum-contrast">Momentum Contrast</h2>

<p>对比学习是一种对高维连续输入建立离散字典的方法,比如图像输入。这个字典是动态的，其中的keys是随机采样的，并且keys encoder 参与学习。我们的假设是一个好的feature可以被从一个包含丰富负样本的大字典中学习到，同时这个字典keys的encoder是尽可能的保持一致性（consistent），除了evolution。基于这个冬季，提出了<strong>Momentum Contrast as described</strong>。</p>

<h3 id="dictionary-as-a-queue">Dictionary as a queue</h3>

<p>本文提出的方法的核心是以<strong>一个data samples队列来维持字典</strong>，这使得我们可以重复利用不久前的Mini-batch中编码的keys，同时还可以将字典大小与mini-batch size解耦，变为一个超参数，可以人为控制，可以远大于典型的Mini-batch size。</p>

<p>字典里的Samples是逐步的呗替换的，新来的Mini-batch执行入队操作，然后最老的mini-batch对应的出队，被移除出字典。这样字典永远表示一个全部数据的采样子集，同时，维护字典所需要的额外计算量也是可以接受的。移除最老的mini-batch是合理的，因为他是最老的，所以他和最新的具有最差的一致性（所以被移除）。</p>

<h3 id="momentum-update">Momentum update</h3>
<p>使用队列可以使得字典更大，但是同时它使得key enchoder不能反向传播，因为梯度应该像队列中的所有samples传播。一个简单的解决方法是copy query的encoder，而不对key encoder进行反向传播。但是这种方法的实验结果很差，<strong>作者猜想是由于encoder的剧烈变化导致 key representations’的一致性被破坏</strong>。因此提出了一种动量更新方法来解决这个问题。</p>

<p>使用query encoder的参数+动量来更新key encoder的参数，而不是直接更新，避免了参数的剧烈变化。特征表达的一致性非常重要，因此参数需要缓慢变化，较大的动量，比如0.999，相比较小的动量，比如0.9,有更好的效果。</p>

\[\begin{equation}
\theta_{\mathrm{k}} \leftarrow m \theta_{\mathrm{k}}+(1-m) \theta_{\mathrm{q}}
\end{equation}\]

<h3 id="realtions-to-previous-mechanisms">Realtions to previous mechanisms</h3>

<p>Figure2中体现了集中对比学习的对比。</p>

<ol>
  <li>
    <p><strong>The end-to-end update</strong>，使用当前mini-batch中的数据作为dictionary。但是大小收到Mini-batch size的限制，也就是GPU显存的限制。其也受到<strong>Large mini-batch optimization</strong>的挑战。</p>
  </li>
  <li>
    <p><strong>Memory bank</strong>,存储了数据集中所有sample的特征表示。对于每个mini-batch所用的字典是从其中随机采样的，并且没有梯度，所以可以支持很大的字典大小。但是<strong>Memory bank</strong>中的特征表达不能及时更新（<strong>was updated when is was last seen</strong>）。所以sampled keys根本上是关于过去的几个epoch的编码器，因此一致性较差。之前提出了一种动量更新方法，但是更新的是同一sample的representation,而不是encoder。</p>
  </li>
</ol>

<h2 id="pretrxt-task">Pretrxt Task</h2>
<p>这里基本上使用了参考文献[61]中的设置，不再重复。</p>

<h3 id="shuffling-bn">Shuffling BN</h3>

<p>本文作者使用的Backbone是ResNet50，其中包含了BN。作者发现BN阻止了模型学习到好的特征表示。<strong>这可能是由于intra-batch communication among samples（caused by BN） leaks information</strong>。</p>

<p>因此本文通过使用shuffling BN来解决这个问题，对于每个GPUs上的数据独立的计算BN。对于key encoder，先shuffle sample order，然后在encoder之后再变回来，<strong>这样保证了batch的统计特性，用于计算query和positive key的来自于两个不同的子集</strong>。这样有效的解决了BN带来的cheatting issue并且允许训练从BN中受益。</p>

<h2 id="experiments">Experiments</h2>

<p>略</p>
:ET