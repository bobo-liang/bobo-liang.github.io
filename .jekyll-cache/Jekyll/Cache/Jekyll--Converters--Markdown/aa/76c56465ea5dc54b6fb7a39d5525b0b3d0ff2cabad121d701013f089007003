I"¹<blockquote>
  <p>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/pdf/1706.03762v5.pdf
å‚è€ƒåšå®¢ï¼šhttps://zhuanlan.zhihu.com/p/46990010
<strong>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.</strong></p>
</blockquote>

<h1 id="model-archtecture">Model Archtecture</h1>

<p><img src="/assets/img/20210624/ATTF1.png" alt="" /></p>

<h2 id="attention">Attention</h2>
<p>Attentionçš„å®šä¹‰å¦‚ä¸‹ï¼š</p>

<p><strong>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors</strong></p>

<p><strong>æ³¨æ„åŠ›å‡½æ•°å¯ä»¥æè¿°ä¸ºå°†ä¸€ä¸ªqueryå’Œä¸€ç»„key-valueå¯¹åšæ˜ å°„å¹¶è¾“å‡ºï¼Œå…¶ä¸­å®ƒä»¬éƒ½æ˜¯å‘é‡ã€‚</strong></p>

<p>æœ¬æ–‡ä½¿ç”¨çš„attentionç§°ä¸º<strong>Scaled Dot-Product Attention</strong>ï¼Œéå¸¸ç»å…¸ï¼Œå…¬å¼å¦‚ä¸‹ï¼Œå…¶ä¸­$d_k$æ˜¯Dimensionã€‚
\(\begin{equation}
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
\end{equation}\)</p>

<p>å¸¸è§çš„attention functionä¸»è¦æœ‰ä¸¤ç§ï¼Œ<strong>additive attention and dot-product</strong>ã€‚è¿™ä¸¤ä¸ªç†è®ºå¤æ‚åº¦å·®ä¸å¤šï¼Œä½†æ˜¯dotåœ¨å®è·µä¸­æ›´å¿«ï¼Œç©ºé—´å¤æ‚åº¦æ›´ä½ï¼Œå› ä¸ºä»–å¯ä»¥ä½¿ç”¨é«˜åº¦ä¼˜åŒ–çš„çŸ©é˜µä¹˜æ³•ä»£ç ã€‚</p>

<p>å¯¹äºå°çš„$d_k$æ¥è¯´ï¼Œä¸¤è€…çš„æ€§èƒ½å·®ä¸å¤šï¼Œä½†æ˜¯å¯¹äºå¤§$d_k$æ¥è¯´ï¼Œadditive attentionæ›´å¥½ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºç‚¹ç§¯çš„ç»“æœéšç€ç»´åº¦çš„å¢é•¿è€Œå¢é•¿ï¼Œå› æ­¤è¿™é‡ŒåŠ å…¥äº†ä¸€ä¸ªå½’ä¸€åŒ–å› å­$\frac{1}{\sqrt{d_k}}$ã€‚</p>

<h2 id="multi-head-attention">Multi-Head Attention</h2>
<p>ç›¸æ¯”ä½¿ç”¨ä¸€ä¸ªå•ç‹¬ç»´åº¦çš„attention functionï¼Œåç€å‘ç°ä½¿ç”¨å¤šä¸ªä¸åŒç»´åº¦çš„attention functioné‡å¤å¤šæ¬¡æ•ˆæœæ›´å¥½ã€‚å†è¾“å‡ºæ—¶å°†ä¸åŒç»´åº¦çš„è¾“å‡ºæ¥åˆ°ä¸€èµ·å³å¯ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚</p>

<p><img src="/assets/img/20210624/ATTF2.png" alt="" /></p>

<p>å¯ä»¥è¡¨ç¤ºä¸ºå¦‚ä¸‹å½¢å¼</p>

\[\begin{equation}
\begin{aligned}
\operatorname{MultiHead}(Q, K, V) &amp;=\text { Concat }\left(\text { head }_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O} \\
\text { where head }_{\mathrm{i}} &amp;=\operatorname{Attention}\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)
\end{aligned}
\end{equation}\]

<h2 id="point-wise-fee-forward-networks">Point-wise Fee-Forward Networks</h2>
<p>å³å†æ¯ä¸ªæ³¨æ„åŠ›å‡½æ•°ä¹‹ååŠ ä¸€ä¸ªå˜æ¢ï¼Œå¯¹äºæ¯ä¸ªä½ç½®çš„ä¿¡æ¯å•ç‹¬å¤„ç†ï¼Œå¦‚ä¸‹
\(\begin{equation}
\operatorname{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}
\end{equation}\)</p>

<h2 id="positional-encoding">Positional Encoding</h2>
<p>å› ä¸ºä¸Šè¿°çš„æ³¨æ„åŠ›æ¨¡å—æ˜¯æ²¡æœ‰æ•æ‰åˆ°æ—¶åºä¿¡æ¯çš„ï¼Œå› æ­¤éœ€è¦äººä¸ºåŠ å…¥æ—¶åºä¿¡æ¯ï¼Œå³<strong>Positional Encoding</strong>ã€‚</p>

\[\begin{equation}
\begin{aligned}
P E_{(p o s, 2 i)} &amp;=\sin \left(p o s / 10000^{2 i / d_{\text {model }}}\right) \\
P E_{(\text {pos }, 2 i+1)} &amp;=\cos \left(\text { pos } / 10000^{2 i / d_{\text {model }}}\right)
\end{aligned}
\end{equation}\]

<p>ä¸Šè¿°æä¾›çš„æ˜¯ç»å¯¹ä½ç½®ä¿¡æ¯ï¼Œè€Œç›¸å¯¹ä½ç½®ä¿¡æ¯ä¹Ÿå¾ˆé‡è¦ã€‚è¿™é‡Œé€‰ç”¨sin,coså‡½æ•°æ˜¯å› ä¸ºå…¶æ»¡è¶³æ€§è´¨
\(\begin{equation}
\begin{aligned}
&amp;\sin (\alpha+\beta)=\sin \alpha \cos \beta+\cos \alpha \sin \beta \\
&amp;\cos (\alpha+\beta)=\cos \alpha \cos \beta-\sin \alpha \sin \beta
\end{aligned}
\end{equation}\)</p>

<p>è¿™è¡¨æ˜p+kä½ç½®å¯ä»¥å‘é‡å¯ä»¥è¡¨ç¤ºä¸ºpä½ç½®çš„çº¿æ€§ç»„åˆï¼Œè¿™ä¸ºæ•æ‰ç›¸å¯¹ä½ç½®ä¿¡æ¯æä¾›äº†å¯èƒ½ã€‚</p>

<p>åœ¨å…¶ä»–NLPè®ºæ–‡ä¸­ï¼Œå¤§å®¶ä¹Ÿéƒ½çœ‹è¿‡position embeddingï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªè®­ç»ƒçš„å‘é‡ï¼Œä½†æ˜¯position embeddingåªæ˜¯extra featuresï¼Œæœ‰è¯¥ä¿¡æ¯ä¼šæ›´å¥½ï¼Œä½†æ˜¯æ²¡æœ‰æ€§èƒ½ä¹Ÿä¸ä¼šäº§ç”Ÿæå¤§ä¸‹é™ï¼Œå› ä¸ºRNNã€CNNæœ¬èº«å°±èƒ½å¤Ÿæ•æ‰åˆ°ä½ç½®ä¿¡æ¯ï¼Œä½†æ˜¯åœ¨Transformeræ¨¡å‹ä¸­ï¼ŒPosition Embeddingæ˜¯ä½ç½®ä¿¡æ¯çš„å”¯ä¸€æ¥æºï¼Œå› æ­¤æ˜¯è¯¥æ¨¡å‹çš„æ ¸å¿ƒæˆåˆ†ï¼Œå¹¶éæ˜¯è¾…åŠ©æ€§è´¨çš„ç‰¹å¾ã€‚</p>

<p>ä¹Ÿå¯ä»¥é‡‡ç”¨è®­ç»ƒçš„position embeddingï¼Œä½†æ˜¯è¯•éªŒç»“æœè¡¨æ˜ç›¸å·®ä¸å¤§ï¼Œå› æ­¤è®ºæ–‡é€‰æ‹©äº†sin position embeddingï¼Œå› ä¸º</p>

<p>è¿™æ ·å¯ä»¥ç›´æ¥è®¡ç®—embeddingè€Œä¸éœ€è¦è®­ç»ƒï¼Œå‡å°‘äº†è®­ç»ƒå‚æ•°
è¿™æ ·å…è®¸æ¨¡å‹å°†position embeddingæ‰©å±•åˆ°è¶…è¿‡äº†training setä¸­æœ€é•¿positionçš„positionï¼Œä¾‹å¦‚æµ‹è¯•é›†ä¸­å‡ºç°äº†æ›´å¤§çš„positionï¼Œsin position embeddingä¾ç„¶å¯ä»¥ç»™å‡ºç»“æœï¼Œä½†ä¸å­˜åœ¨è®­ç»ƒåˆ°çš„embeddingã€‚</p>

<h2 id="why-self-attention">Why Self Attention</h2>

<p>è¿™é‡Œå°†Self-Attention layerså’Œrecurrent/convolutional layersæ¥è¿›è¡Œæ¯”è¾ƒï¼Œæ¥è¯´æ˜Self-Attentionçš„å¥½å¤„ã€‚å‡è®¾å°†ä¸€ä¸ªè¾“å…¥åºåˆ—åˆ†åˆ«ç”¨</p>

<ul>
  <li>Self-Attention Layer</li>
  <li>Recurrent Layer</li>
  <li>Convolutional Layer
æ¥æ˜ å°„åˆ°ä¸€ä¸ªç›¸åŒé•¿åº¦çš„åºåˆ—</li>
</ul>

<p>æˆ‘ä»¬åˆ†æä¸‹é¢ä¸‰ä¸ªæŒ‡æ ‡ï¼š</p>

<ul>
  <li>æ¯ä¸€å±‚çš„è®¡ç®—å¤æ‚åº¦</li>
  <li>èƒ½å¤Ÿè¢«å¹¶è¡Œçš„è®¡ç®—ï¼Œç”¨éœ€è¦çš„æœ€å°‘çš„é¡ºåºæ“ä½œçš„æ•°é‡æ¥è¡¡é‡</li>
  <li>ç½‘ç»œä¸­long-range dependenciesçš„<strong>path length</strong>ï¼Œåœ¨å¤„ç†åºåˆ—ä¿¡æ¯çš„ä»»åŠ¡ä¸­å¾ˆé‡è¦çš„åœ¨äºå­¦ä¹ long-range dependenciesã€‚å½±å“å­¦ä¹ é•¿è·ç¦»ä¾èµ–çš„å…³é”®ç‚¹åœ¨äºå‰å‘/åå‘ä¿¡æ¯éœ€è¦ä¼ æ’­çš„æ­¥é•¿ï¼Œè¾“å…¥å’Œè¾“å‡ºåºåˆ—ä¸­è·¯å¾„è¶ŠçŸ­ï¼Œé‚£ä¹ˆå°±è¶Šå®¹æ˜“å­¦ä¹ long-range dependenciesã€‚å› æ­¤æˆ‘ä»¬æ¯”è¾ƒä¸‰ç§ç½‘ç»œä¸­ä»»ä½•è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„æœ€é•¿path length
ç»“æœå¦‚ä¸‹æ‰€ç¤º</li>
</ul>

<p><img src="/assets/img/20210624/ATTT1.png" alt="" /></p>

<h3 id="å¹¶è¡Œè®¡ç®—">å¹¶è¡Œè®¡ç®—</h3>
<p>Self-Attention layerç”¨ä¸€ä¸ªå¸¸é‡çº§åˆ«çš„é¡ºåºæ“ä½œï¼Œå°†æ‰€æœ‰çš„positionsè¿æ¥èµ·æ¥</p>

<p>Recurrent Layeréœ€è¦$O(b)$ä¸ªé¡ºåºæ“ä½œ</p>

<h3 id="è®¡ç®—å¤æ‚åº¦åˆ†æ">è®¡ç®—å¤æ‚åº¦åˆ†æ</h3>
<p>å¦‚æœåºåˆ—é•¿åº¦$n &lt;$è¡¨ç¤ºç»´åº¦$d$ ï¼ŒSelf-Attention Layeræ¯”recurrent layerså¿«ï¼Œè¿™å¯¹ç»å¤§éƒ¨åˆ†ç°æœ‰æ¨¡å‹å’Œä»»åŠ¡éƒ½æ˜¯æˆç«‹çš„ã€‚</p>

<p>ä¸ºäº†æé«˜åœ¨åºåˆ—é•¿åº¦å¾ˆé•¿çš„ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¯¹Self-Attentionè¿›è¡Œé™åˆ¶ï¼Œåªè€ƒè™‘è¾“å…¥åºåˆ—ä¸­çª—å£ä¸º$r$ çš„ä½ç½®ä¸Šçš„ä¿¡æ¯ï¼Œè¿™ç§°ä¸ºSelf-Attention(restricted), è¿™å›å¢åŠ maximum path lengthåˆ°$O(n/r)$ .</p>

<h3 id="length-path">length path</h3>
<p>å¦‚æœå·ç§¯å±‚kernel width$k&lt;n$ ï¼Œå¹¶ä¸ä¼šå°†æ‰€æœ‰ä½ç½®çš„è¾“å…¥å’Œè¾“å‡ºéƒ½è¿æ¥èµ·æ¥ã€‚è¿™æ ·éœ€è¦$O(k/n)$ ä¸ªå·ç§¯å±‚æˆ–è€… $O(log_k(n))$ ä¸ªdilated convolutionï¼Œå¢åŠ äº†è¾“å…¥è¾“å‡ºä¹‹é—´çš„æœ€å¤§path lengthã€‚</p>

<p>å·ç§¯å±‚æ¯”å¾ªç¯å±‚è®¡ç®—å¤æ‚åº¦æ›´é«˜ï¼Œæ˜¯kå€ã€‚ä½†æ˜¯Separable Convolutionså°†è§æ•ˆå¤æ‚åº¦ã€‚</p>

<p>åŒæ—¶self-attentionçš„æ¨¡å‹å¯è§£é‡Šæ€§æ›´å¥½(interpretable).</p>
:ET