I"<blockquote>
  <p>论文链接 : https://openaccess.thecvf.com/content_CVPR_2020/html/Zhou_Joint_3D_Instance_Segmentation_and_Object_Detection_for_Autonomous_Driving_CVPR_2020_paper</p>
</blockquote>

<blockquote>
  <p>参考博客 : https://zhuanlan.zhihu.com/p/151398153</p>
</blockquote>

<h1 id="introduction">Introduction</h1>

<p>许多方法都证明了简单的表示更适合深度学习的学习。3DBbox的表示框就很简单，但是这也有一些缺点，因为Bbox损失了物体的Shape信息，而在有遮挡的情况下就更加严重，BBox不能足够准确的描述object的精确位置。</p>

<p>因此，可以额外引入Instance mask来消除Bbox中非目标部分的一项。一种直观的实例分割方法就是先做目标检测，再做前景分割，比如MaskRCNN。</p>

<p>目前3D目标检测对于室内和室外的研究已经不错，但是大多数的3D实例分割还是针对室内的。</p>

<p>本文提出一种网络，结合了目标检测和实例分割在一个网络中，并且可以相互提升对方的性能。</p>

<h2 id="contribution">Contribution</h2>

<ol>
  <li>一种端到端训练的网络，同时得到3DBbox和实例分割；</li>
  <li>相比普通的feature embedding in a 2D image，提出的方法同时考虑了gloabl BBox和local point information together；</li>
  <li>KITTI上验证了有效性和高效性，相比其他SOTA。</li>
</ol>

<p><img src="/assets/img/20210525/J3DF2.png" alt="" /></p>

<h1 id="proposaed-approach">Proposaed Approach</h1>

<p><img src="/assets/img/20210525/J3DF3.png" alt="" /></p>

<h2 id="overview">Overview</h2>
<p>网络总体结构如上图所示，首先通过一个PointNet++来提取特征。之后分为2个branch，分别是语义分割得分以及Spatial Embeddding(SE),用于预测前景点相对于Object 中心的偏移以及尺寸等等。 基于这两个branch的信息，通过一个SE Based Clustering and Region Proposal，实现点的聚类，并给每个聚类都得到一个proposal。（这里由于每个聚类是一个目标，给出一个proposal，所以并不用NMS。）最后，经过一个refine network（比如PointNet），得到最后的BBox。</p>

<h2 id="instance-aware-se">Instance-aware SE</h2>

<p>2D和3D实例分割还是有一些区别。Scale,Spatial Laypout Ambiguity and Occlusion是2D图像领域的三个主要问题，都严重的影响了目标检测和实例分割的效果。但是这些问题在3D点云数据都不存在。相反的，由于点云数据在3D空间中系数的分布，所以直接聚类并得不到好的效果，所以为了更简单的聚类或者分割，一个well designed intermediate procedure是需要的，用于发现点的隐藏特征，比如segmantic class, instance label 或者 object’s information。</p>

<h3 id="point-cloud-feature-extraction">Point cloud feature extraction</h3>

<p>使用PointNet++。</p>

<h3 id="semantic-infomation">Semantic infomation</h3>

<p>分割分类损失，使用<strong>focal loss</strong>解决分类不平衡的问题。</p>

\[\begin{equation}
\begin{aligned}
\mathbf{L}_{\mathrm{cls}} &amp;=-\sum_{i=1}^{C}\left(y_{i} \log \left(p_{i}\right)\left(1-p_{i}\right)^{\gamma} \alpha_{i}\right.\\
&amp;\left.+\left(1-y_{i}\right) \log \left(1-p_{i}\right)\left(p_{i}\right)^{\gamma}\left(1-\alpha_{i}\right)\right)
\end{aligned}
\end{equation}\]

<h3 id="object-information">Object information</h3>

<p>这里最重要的information是目标中心，因为即影响分割（聚在一起方便聚类），也影响目标检测。这里每个点的回归目标就建模为其到所属于的目标中心的距离
\(\begin{equation}
c_{\text {offset }}^{i}=\left(p_{x}^{i}-c_{x}^{k}, p_{y}^{i}-c_{y}^{k}, p_{z}^{i}-c_{z}^{k}\right)^{T} \text { , }
\end{equation}\)</p>

<p>其余的BBox信息则直接按真值回归。</p>

<h2 id="clustering-based-proposal-generation">Clustering-based Proposal Generation</h2>

<p>上述前景点都被pull到一起，使用简单的聚类方法就可以得到他们的聚类。置信度前5的聚类每个产生一个mean BBox，送入下一阶段的ROI Pooling中。</p>

<h2 id="bbox-refinement">BBox Refinement</h2>
<p>ROI Pooling 包括两件事</p>
<ol>
  <li>将一个聚类内的所有点包含进来</li>
  <li>去除里面不属于该Bbox的前景点</li>
</ol>

<p>之后经过一个PointNet++再分两个HEAD回归就得到refine 结果。</p>

<h2 id="multi-task-loss">Multi-task Loss</h2>

<p>总体Loss
\(\begin{equation}
\mathbf{L}=\mathbf{L}_{\mathrm{sem}-\mathrm{cls}}+\mathbf{L}_{\mathrm{SE}}+\mathbf{L}_{\mathrm{reg}}
\end{equation}\)</p>

<p>SE loss使用smooth L1, BBox regression loss 用IOU loss。</p>

<h1 id="experiments">Experiments</h1>

<p><img src="/assets/img/20210525/J3DEXP.png" alt="" /></p>
:ET